{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QaFrvOWeGWoE",
    "outputId": "adc13e31-0656-4629-8a71-b7e76940fdab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from albumentations import *\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY4ffYPhGdIE"
   },
   "outputs": [],
   "source": [
    "SHAPE = (224, 224, 3)\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 100\n",
    "N_SPLITS = 5\n",
    "SEED = 1881\n",
    "TRAIN_TEST_RATIO = 0.2\n",
    "\n",
    "BASE_DIR     = \"../../data/BreaKHis_v1/histology_slides/breast/\"\n",
    "DATASET_MODE = \"200X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "futhEMcZhXDy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGT_8VyTZt2T"
   },
   "outputs": [],
   "source": [
    "class BREAKHIST_DATASET:\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape           --> TUPLE.wanted image size\n",
    "    batch_size            --> INT.yielding data size for every iteration\n",
    "    orders                --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n",
    "    base_dir              --> STR.the DIR which is include \"benign\" and \"malignant\" dirs.\n",
    "    dataset_mode          --> STR. Which type of images will be used: \"40X\", \"100X\", \"200X\", \"400X\".\n",
    "    seed                  --> INT. This allow to dataset generator to more reproduciable and it ensures that x and y are shuffled with compatible.\n",
    "    augment               --> BOOL. Augment data or not.\n",
    "    train_test_ratio      --> How much of data will be used as test set.\n",
    "    ---------\n",
    "    GENERAL_CLASSES       --> LIST.[\"benign\", \"malignant\"]\n",
    "    BENIGN_SUB_CLASSES    --> LIST.[\"adenosis\", \"fibroadenoma\", \"phyllodes_tumor\", \"tubular_adenoma\"]\n",
    "    MALIGNANT_SUB_CLASSES --> LIST.[\"ductal_carcinoma\", \"lobular_carcinoma\", \"mucinous_carcinoma\", \"papillary_carcinoma\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, batch_size, orders, base_dir, dataset_mode, seed, train_test_ratio, augment=True):\n",
    "        self.SHAPE                 = input_shape\n",
    "        self.BATCH_SIZE            = batch_size\n",
    "        self.arr                   = orders\n",
    "        self.DATASET_MODE          = dataset_mode\n",
    "        self.SEED                  = seed\n",
    "        self.TT_RATIO              = train_test_ratio\n",
    "        self.AUG                   = augment\n",
    "        \n",
    "        self.BASE_DIR              = base_dir\n",
    "        self.GENERAL_CLASSES       = [\"benign\", \"malignant\"]\n",
    "        self.BENIGN_SUB_CLASSES    = [\"adenosis\", \"fibroadenoma\", \"phyllodes_tumor\", \"tubular_adenoma\"]\n",
    "        self.MALIGNANT_SUB_CLASSES = [\"ductal_carcinoma\", \"lobular_carcinoma\", \"mucinous_carcinoma\", \"papillary_carcinoma\"]\n",
    "        \n",
    "        \n",
    "    def get_paths_n_labels(self):\n",
    "\n",
    "        x      = []\n",
    "        label = []\n",
    "\n",
    "        for ix1, a in enumerate(self.GENERAL_CLASSES):\n",
    "            if ix1 == 0:\n",
    "                for ix2, b in enumerate(self.BENIGN_SUB_CLASSES):\n",
    "                    path1 = self.BASE_DIR+a+\"/SOB/\"+b\n",
    "                    for c in os.listdir(path1):\n",
    "                        path2 = path1+\"/\"+c+\"/\"+self.DATASET_MODE\n",
    "                        for img_name in os.listdir(path2):\n",
    "                            path3 = path2+\"/\"+img_name\n",
    "\n",
    "                            # x\n",
    "                            img_path = path3 #np.array(Image.open(path3), dtype=np.float16)\n",
    "\n",
    "                            # y\n",
    "                            main_targets = np.zeros((2), dtype=np.float32) # BENIGN OR MALIGNANT\n",
    "                            main_targets[ix1] = 1.\n",
    "\n",
    "                            # Store the values\n",
    "                            x.append(img_path)\n",
    "                            label.append(main_targets)\n",
    "\n",
    "                            \n",
    "            if ix1 == 1:\n",
    "                for ix2, b in enumerate(self.MALIGNANT_SUB_CLASSES):\n",
    "                    path1 = self.BASE_DIR+a+\"/SOB/\"+b\n",
    "                    for c in os.listdir(path1):\n",
    "                        path2 = path1+\"/\"+c+\"/\"+self.DATASET_MODE\n",
    "                        for img_name in os.listdir(path2):\n",
    "                            path3 = path2+\"/\"+img_name\n",
    "\n",
    "                            # x\n",
    "                            img_path = path3  #np.array(Image.open(path3), dtype=np.float16)\n",
    "\n",
    "                            # y\n",
    "                            main_targets = np.zeros((2), dtype=np.float32) # BENIGN OR MALIGNANT\n",
    "                            main_targets[ix1] = 1.\n",
    "                     \n",
    "                            # Store the values\n",
    "                            x.append(img_path)\n",
    "                            label.append(main_targets)\n",
    "                           \n",
    "        return x, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.get_paths_n_labels()[0])\n",
    "    \n",
    "    def get_img(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        return np.array(img)\n",
    "    \n",
    "    def augmenting(self, img):\n",
    "        if self.AUG:\n",
    "            augment = Compose([VerticalFlip(p=0.5),\n",
    "                               HorizontalFlip(p=0.5),\n",
    "                               RandomBrightnessContrast(p=0.3),\n",
    "                               ShiftScaleRotate(p=0.5, shift_limit=0.2, scale_limit=0.2, rotate_limit=20)])  \n",
    "        else:\n",
    "            augment = Compose([])  \n",
    "\n",
    "        img = augment(image=img)['image']\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def resize_and_normalize(self, img):\n",
    "        img = resize(img, self.SHAPE)\n",
    "        return img\n",
    "    \n",
    "    def get_shuffled_data(self):\n",
    "        img_paths, labels = self.get_paths_n_labels()\n",
    "\n",
    "        np.random.seed(self.SEED) \n",
    "        np.random.shuffle(img_paths)\n",
    "        \n",
    "        np.random.seed(self.SEED) \n",
    "        np.random.shuffle(labels)\n",
    "        \n",
    "        return img_paths, labels\n",
    "        \n",
    "    def split_train_test(self, get):  # get=={\"train\",\"test\"}\n",
    "        img_paths, labels = self.get_shuffled_data()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(img_paths, labels, test_size=self.TT_RATIO, random_state=self.SEED)\n",
    "        \n",
    "        if get=='train':\n",
    "            return x_train, y_train\n",
    "        \n",
    "        elif get=='test':\n",
    "            return x_test, y_test\n",
    "    \n",
    "    def data_generator(self):\n",
    "        img_paths, labels = self.split_train_test(get=\"train\")\n",
    "        \n",
    "        while True:\n",
    "            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float32)\n",
    "            y = np.empty((self.BATCH_SIZE, 2), dtype=np.float32)\n",
    "\n",
    "            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n",
    "\n",
    "            for ix, id_ in enumerate(batch):\n",
    "                # x\n",
    "                img_path = img_paths[id_]\n",
    "                img = self.get_img(img_path)\n",
    "                img = self.augmenting(img)\n",
    "                img = self.resize_and_normalize(img)\n",
    "                  \n",
    "                # y \n",
    "                label = labels[id_]\n",
    "             \n",
    "                # Store the values    \n",
    "                x[ix] = img\n",
    "                y[ix] = label\n",
    "\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1223
    },
    "colab_type": "code",
    "id": "bJ-aVohsqt1F",
    "outputId": "5514493d-cfc0-47bc-c4db-7b5d3de32331",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.6836397  0.62440693 0.6465577 ]\n",
      "   [0.71869093 0.64473915 0.6871411 ]\n",
      "   [0.72813594 0.65940344 0.70481664]\n",
      "   ...\n",
      "   [0.7327534  0.5803922  0.6764312 ]\n",
      "   [0.72692794 0.57212883 0.6686997 ]\n",
      "   [0.71872157 0.58646053 0.69016105]]\n",
      "\n",
      "  [[0.6972711  0.6518951  0.6760176 ]\n",
      "   [0.72685575 0.6606158  0.6998315 ]\n",
      "   [0.7305563  0.6599681  0.71322435]\n",
      "   ...\n",
      "   [0.7310574  0.57811624 0.6722339 ]\n",
      "   [0.73467046 0.5799654  0.6871849 ]\n",
      "   [0.70196736 0.57147235 0.6754661 ]]\n",
      "\n",
      "  [[0.714115   0.6729736  0.6895111 ]\n",
      "   [0.73504025 0.67594755 0.71663386]\n",
      "   [0.7310027  0.66140145 0.7138174 ]\n",
      "   ...\n",
      "   [0.7242647  0.56168157 0.6624168 ]\n",
      "   [0.7232449  0.56544995 0.67408746]\n",
      "   [0.684119   0.5536371  0.6574339 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6432664  0.5468225  0.62231266]\n",
      "   [0.6560596  0.53694195 0.63920695]\n",
      "   [0.65966606 0.542019   0.6496652 ]\n",
      "   ...\n",
      "   [0.68441874 0.527556   0.6393076 ]\n",
      "   [0.73299634 0.58195686 0.6852022 ]\n",
      "   [0.7297619  0.6193781  0.71199447]]\n",
      "\n",
      "  [[0.6445794  0.5436712  0.62497157]\n",
      "   [0.6606027  0.541485   0.64705884]\n",
      "   [0.66191787 0.5447851  0.6513787 ]\n",
      "   ...\n",
      "   [0.6686997  0.5196801  0.62122065]\n",
      "   [0.6792717  0.535084   0.6271293 ]\n",
      "   [0.6543833  0.5530878  0.6418833 ]]\n",
      "\n",
      "  [[0.63402486 0.53718704 0.61074054]\n",
      "   [0.6629245  0.53764224 0.6478335 ]\n",
      "   [0.66382617 0.5461791  0.65206146]\n",
      "   ...\n",
      "   [0.65882355 0.52135855 0.6115546 ]\n",
      "   [0.6302171  0.5026589  0.5977197 ]\n",
      "   [0.5892419  0.4941308  0.5800442 ]]]]\n",
      "(1, 224, 224, 3)\n",
      "----------\n",
      "[[0. 1.]]\n",
      "(1, 2)\n",
      "----------\n",
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = BREAKHIST_DATASET(SHAPE, 1, range(4), BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=True)\n",
    "\n",
    "for ix, data in enumerate(dataset.data_generator()):\n",
    "    img, y = data\n",
    "    print(img)\n",
    "    print(img.shape)\n",
    "    print(\"-\"*10)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"-\"*10)\n",
    "    print(img[0,:,:,:].shape)\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.show()\n",
    "    \n",
    "    if ix==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZygcwwp0Sry"
   },
   "outputs": [],
   "source": [
    "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    \n",
    "    Only computes a batch-wise average of recall.\n",
    "    \n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    \n",
    "    Only computes a batch-wise average of precision.\n",
    "    \n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisionx = precision(y_true, y_pred)\n",
    "    recallx = recall(y_true, y_pred)\n",
    "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBbVuHAfgHnh"
   },
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvvEuAAKfkV5"
   },
   "outputs": [],
   "source": [
    "# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "    \n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature._keras_shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature._keras_shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool._keras_shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool._keras_shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat._keras_shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature._keras_shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4Ndx2vm6NZ3"
   },
   "outputs": [],
   "source": [
    "# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    shortcut = y\n",
    "\n",
    "    # down-sampling is performed with a stride of 2\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    y = add([shortcut, y])\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVUWz9lzfm6Y"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    dropRate = 0.3\n",
    "    \n",
    "    init = Input(SHAPE)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(2, activation='softmax')(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3001
    },
    "colab_type": "code",
    "id": "w2V3AUW7fm-n",
    "outputId": "d7c45fcc-739a-4166-9a30-d65352088c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 224, 224, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 224, 224, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 112, 112, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 64)     0           global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1, 8)      520         reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 1, 64)     576         dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 1, 64)     0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 1, 64)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 112, 112, 64) 0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 112, 112, 2)  0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 112, 112, 1)  98          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 112, 112, 64) 0           multiply_1[0][0]                 \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 112, 112, 64) 36928       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 112, 112, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 112, 112, 64) 36928       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 112, 112, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 112, 112, 64) 0           multiply_2[0][0]                 \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 64) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 56, 56, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 128)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 128)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 128)    0           global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 1, 16)     2064        reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1, 128)    2176        dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 1, 128)    0           dense_4[0][0]                    \n",
      "                                                                 dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1, 1, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 56, 56, 128)  0           activation_5[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 56, 56, 2)    0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 56, 56, 1)    98          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 56, 56, 128)  0           multiply_3[0][0]                 \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 56, 56, 128)  147584      multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 56, 56, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 56, 56, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 56, 56, 128)  0           multiply_4[0][0]                 \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 224, 224, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 224, 224, 64) 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 224, 224, 224 0           up_sampling2d_1[0][0]            \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 224)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          57600       global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            514         activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 605,566\n",
      "Trainable params: 603,262\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "schfFpcIfzZy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5584
    },
    "colab_type": "code",
    "id": "MSHMA1gtMQ6e",
    "outputId": "61dadec3-f4b4-4877-970a-c66700175ab5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 65s - loss: 0.4917 - precision: 0.7846 - recall: 0.7846 - f1: 0.7846 - acc: 0.7846 - val_loss: 0.2804 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28035, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 2/100\n",
      " - 45s - loss: 0.3973 - precision: 0.8396 - recall: 0.8396 - f1: 0.8396 - acc: 0.8396 - val_loss: 0.4254 - val_precision: 0.8141 - val_recall: 0.8141 - val_f1: 0.8141 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.28035\n",
      "Epoch 3/100\n",
      " - 40s - loss: 0.3624 - precision: 0.8443 - recall: 0.8443 - f1: 0.8443 - acc: 0.8443 - val_loss: 0.4662 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.28035\n",
      "Epoch 4/100\n",
      " - 39s - loss: 0.3421 - precision: 0.8671 - recall: 0.8671 - f1: 0.8671 - acc: 0.8671 - val_loss: 0.4068 - val_precision: 0.8301 - val_recall: 0.8301 - val_f1: 0.8301 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.28035\n",
      "Epoch 5/100\n",
      " - 39s - loss: 0.3473 - precision: 0.8538 - recall: 0.8538 - f1: 0.8538 - acc: 0.8538 - val_loss: 0.4792 - val_precision: 0.7372 - val_recall: 0.7372 - val_f1: 0.7372 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.28035\n",
      "Epoch 6/100\n",
      " - 38s - loss: 0.2941 - precision: 0.8907 - recall: 0.8907 - f1: 0.8907 - acc: 0.8907 - val_loss: 1.1204 - val_precision: 0.8173 - val_recall: 0.8173 - val_f1: 0.8173 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.28035\n",
      "Epoch 7/100\n",
      " - 38s - loss: 0.3070 - precision: 0.8726 - recall: 0.8726 - f1: 0.8726 - acc: 0.8726 - val_loss: 0.3642 - val_precision: 0.8462 - val_recall: 0.8462 - val_f1: 0.8462 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.28035\n",
      "Epoch 8/100\n",
      " - 38s - loss: 0.3202 - precision: 0.8671 - recall: 0.8671 - f1: 0.8671 - acc: 0.8671 - val_loss: 0.5160 - val_precision: 0.7564 - val_recall: 0.7564 - val_f1: 0.7564 - val_acc: 0.7564\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.28035\n",
      "Epoch 9/100\n",
      " - 38s - loss: 0.2880 - precision: 0.8821 - recall: 0.8821 - f1: 0.8821 - acc: 0.8821 - val_loss: 0.3238 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.28035\n",
      "Epoch 10/100\n",
      " - 38s - loss: 0.3028 - precision: 0.8656 - recall: 0.8656 - f1: 0.8656 - acc: 0.8656 - val_loss: 0.2650 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.28035 to 0.26500, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 11/100\n",
      " - 38s - loss: 0.2921 - precision: 0.8616 - recall: 0.8616 - f1: 0.8616 - acc: 0.8616 - val_loss: 1.5833 - val_precision: 0.3462 - val_recall: 0.3462 - val_f1: 0.3462 - val_acc: 0.3462\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.26500\n",
      "Epoch 12/100\n",
      " - 38s - loss: 0.2372 - precision: 0.9002 - recall: 0.9002 - f1: 0.9002 - acc: 0.9002 - val_loss: 0.6915 - val_precision: 0.7276 - val_recall: 0.7276 - val_f1: 0.7276 - val_acc: 0.7276\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.26500\n",
      "Epoch 13/100\n",
      " - 38s - loss: 0.2601 - precision: 0.8970 - recall: 0.8970 - f1: 0.8970 - acc: 0.8970 - val_loss: 0.5658 - val_precision: 0.7244 - val_recall: 0.7244 - val_f1: 0.7244 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.26500\n",
      "Epoch 14/100\n",
      " - 38s - loss: 0.2692 - precision: 0.8915 - recall: 0.8915 - f1: 0.8915 - acc: 0.8915 - val_loss: 0.3379 - val_precision: 0.8878 - val_recall: 0.8878 - val_f1: 0.8878 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.26500\n",
      "Epoch 15/100\n",
      " - 38s - loss: 0.2390 - precision: 0.8829 - recall: 0.8829 - f1: 0.8829 - acc: 0.8829 - val_loss: 0.1828 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.26500 to 0.18280, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 16/100\n",
      " - 36s - loss: 0.2018 - precision: 0.9096 - recall: 0.9096 - f1: 0.9096 - acc: 0.9096 - val_loss: 0.1348 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.18280 to 0.13483, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 17/100\n",
      " - 36s - loss: 0.1980 - precision: 0.9237 - recall: 0.9237 - f1: 0.9237 - acc: 0.9237 - val_loss: 0.2310 - val_precision: 0.9295 - val_recall: 0.9295 - val_f1: 0.9295 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13483\n",
      "Epoch 18/100\n",
      " - 36s - loss: 0.2508 - precision: 0.8970 - recall: 0.8970 - f1: 0.8970 - acc: 0.8970 - val_loss: 5.5621 - val_precision: 0.3269 - val_recall: 0.3269 - val_f1: 0.3269 - val_acc: 0.3269\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13483\n",
      "Epoch 19/100\n",
      " - 36s - loss: 0.2351 - precision: 0.9096 - recall: 0.9096 - f1: 0.9096 - acc: 0.9096 - val_loss: 0.2207 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13483\n",
      "Epoch 20/100\n",
      " - 36s - loss: 0.2330 - precision: 0.9033 - recall: 0.9033 - f1: 0.9033 - acc: 0.9033 - val_loss: 0.3549 - val_precision: 0.8109 - val_recall: 0.8109 - val_f1: 0.8109 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.13483\n",
      "Epoch 21/100\n",
      " - 36s - loss: 0.2173 - precision: 0.9119 - recall: 0.9119 - f1: 0.9119 - acc: 0.9119 - val_loss: 2.6599 - val_precision: 0.3590 - val_recall: 0.3590 - val_f1: 0.3590 - val_acc: 0.3590\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13483\n",
      "Epoch 22/100\n",
      " - 36s - loss: 0.2484 - precision: 0.8994 - recall: 0.8994 - f1: 0.8994 - acc: 0.8994 - val_loss: 0.6657 - val_precision: 0.6635 - val_recall: 0.6635 - val_f1: 0.6635 - val_acc: 0.6635\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.13483\n",
      "Epoch 23/100\n",
      " - 36s - loss: 0.2212 - precision: 0.9182 - recall: 0.9182 - f1: 0.9182 - acc: 0.9182 - val_loss: 0.1983 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.13483\n",
      "Epoch 24/100\n",
      " - 36s - loss: 0.1754 - precision: 0.9230 - recall: 0.9230 - f1: 0.9230 - acc: 0.9230 - val_loss: 0.1626 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.13483\n",
      "Epoch 25/100\n",
      " - 36s - loss: 0.2046 - precision: 0.9072 - recall: 0.9072 - f1: 0.9072 - acc: 0.9072 - val_loss: 0.6415 - val_precision: 0.8429 - val_recall: 0.8429 - val_f1: 0.8429 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.13483\n",
      "Epoch 26/100\n",
      " - 36s - loss: 0.2130 - precision: 0.9057 - recall: 0.9057 - f1: 0.9057 - acc: 0.9057 - val_loss: 1.5233 - val_precision: 0.3173 - val_recall: 0.3173 - val_f1: 0.3173 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.13483\n",
      "Epoch 27/100\n",
      " - 36s - loss: 0.1897 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.1845 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.13483\n",
      "Epoch 28/100\n",
      " - 35s - loss: 0.2236 - precision: 0.9041 - recall: 0.9041 - f1: 0.9041 - acc: 0.9041 - val_loss: 0.6796 - val_precision: 0.8205 - val_recall: 0.8205 - val_f1: 0.8205 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.13483\n",
      "Epoch 29/100\n",
      " - 35s - loss: 0.2119 - precision: 0.9080 - recall: 0.9080 - f1: 0.9080 - acc: 0.9080 - val_loss: 1.1257 - val_precision: 0.7404 - val_recall: 0.7404 - val_f1: 0.7404 - val_acc: 0.7404\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.13483\n",
      "Epoch 30/100\n",
      " - 36s - loss: 0.2100 - precision: 0.9080 - recall: 0.9080 - f1: 0.9080 - acc: 0.9080 - val_loss: 0.7064 - val_precision: 0.7724 - val_recall: 0.7724 - val_f1: 0.7724 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.13483\n",
      "Epoch 31/100\n",
      " - 35s - loss: 0.1943 - precision: 0.9222 - recall: 0.9222 - f1: 0.9222 - acc: 0.9222 - val_loss: 0.3404 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13483\n",
      "Epoch 32/100\n",
      " - 35s - loss: 0.2345 - precision: 0.9049 - recall: 0.9049 - f1: 0.9049 - acc: 0.9049 - val_loss: 0.6132 - val_precision: 0.7019 - val_recall: 0.7019 - val_f1: 0.7019 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13483\n",
      "Epoch 33/100\n",
      " - 35s - loss: 0.1825 - precision: 0.9245 - recall: 0.9245 - f1: 0.9245 - acc: 0.9245 - val_loss: 0.2184 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13483\n",
      "Epoch 34/100\n",
      " - 35s - loss: 0.1533 - precision: 0.9379 - recall: 0.9379 - f1: 0.9379 - acc: 0.9379 - val_loss: 0.2107 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.13483\n",
      "Epoch 35/100\n",
      " - 36s - loss: 0.1581 - precision: 0.9379 - recall: 0.9379 - f1: 0.9379 - acc: 0.9379 - val_loss: 0.1379 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.13483\n",
      "Epoch 36/100\n",
      " - 36s - loss: 0.1694 - precision: 0.9347 - recall: 0.9347 - f1: 0.9347 - acc: 0.9347 - val_loss: 0.2788 - val_precision: 0.8814 - val_recall: 0.8814 - val_f1: 0.8814 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.13483\n",
      "Epoch 37/100\n",
      " - 35s - loss: 0.1784 - precision: 0.9285 - recall: 0.9285 - f1: 0.9285 - acc: 0.9285 - val_loss: 0.1987 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.13483\n",
      "Epoch 38/100\n",
      " - 35s - loss: 0.1895 - precision: 0.9222 - recall: 0.9222 - f1: 0.9222 - acc: 0.9222 - val_loss: 1.1343 - val_precision: 0.6122 - val_recall: 0.6122 - val_f1: 0.6122 - val_acc: 0.6122\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.13483\n",
      "Epoch 39/100\n",
      " - 35s - loss: 0.2210 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.2914 - val_precision: 0.9006 - val_recall: 0.9006 - val_f1: 0.9006 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.13483\n",
      "Epoch 40/100\n",
      " - 36s - loss: 0.1972 - precision: 0.9363 - recall: 0.9363 - f1: 0.9363 - acc: 0.9363 - val_loss: 0.3790 - val_precision: 0.9006 - val_recall: 0.9006 - val_f1: 0.9006 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.13483\n",
      "Epoch 41/100\n",
      " - 36s - loss: 0.1455 - precision: 0.9442 - recall: 0.9442 - f1: 0.9442 - acc: 0.9442 - val_loss: 0.1974 - val_precision: 0.8910 - val_recall: 0.8910 - val_f1: 0.8910 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.13483\n",
      "Epoch 42/100\n",
      " - 35s - loss: 0.1485 - precision: 0.9355 - recall: 0.9355 - f1: 0.9355 - acc: 0.9355 - val_loss: 0.1053 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.13483 to 0.10534, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 43/100\n",
      " - 36s - loss: 0.1578 - precision: 0.9403 - recall: 0.9403 - f1: 0.9403 - acc: 0.9403 - val_loss: 0.2262 - val_precision: 0.9103 - val_recall: 0.9103 - val_f1: 0.9103 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.10534\n",
      "Epoch 44/100\n",
      " - 36s - loss: 0.1800 - precision: 0.9214 - recall: 0.9214 - f1: 0.9214 - acc: 0.9214 - val_loss: 0.1937 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10534\n",
      "Epoch 45/100\n",
      " - 36s - loss: 0.1567 - precision: 0.9426 - recall: 0.9426 - f1: 0.9426 - acc: 0.9426 - val_loss: 0.1879 - val_precision: 0.9295 - val_recall: 0.9295 - val_f1: 0.9295 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.10534\n",
      "Epoch 46/100\n",
      " - 36s - loss: 0.2024 - precision: 0.9159 - recall: 0.9159 - f1: 0.9159 - acc: 0.9159 - val_loss: 0.3115 - val_precision: 0.8622 - val_recall: 0.8622 - val_f1: 0.8622 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.10534\n",
      "Epoch 47/100\n",
      " - 36s - loss: 0.1738 - precision: 0.9285 - recall: 0.9285 - f1: 0.9285 - acc: 0.9285 - val_loss: 0.2621 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.10534\n",
      "Epoch 48/100\n",
      " - 36s - loss: 0.1514 - precision: 0.9403 - recall: 0.9403 - f1: 0.9403 - acc: 0.9403 - val_loss: 0.1384 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.10534\n",
      "Epoch 49/100\n",
      " - 36s - loss: 0.1251 - precision: 0.9536 - recall: 0.9536 - f1: 0.9536 - acc: 0.9536 - val_loss: 0.1115 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.10534\n",
      "Epoch 50/100\n",
      " - 35s - loss: 0.1634 - precision: 0.9379 - recall: 0.9379 - f1: 0.9379 - acc: 0.9379 - val_loss: 0.1100 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.10534\n",
      "Epoch 51/100\n",
      " - 35s - loss: 0.1168 - precision: 0.9458 - recall: 0.9458 - f1: 0.9458 - acc: 0.9458 - val_loss: 0.1254 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.10534\n",
      "Epoch 52/100\n",
      " - 36s - loss: 0.1613 - precision: 0.9292 - recall: 0.9292 - f1: 0.9292 - acc: 0.9292 - val_loss: 0.2926 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.10534\n",
      "Epoch 53/100\n",
      " - 35s - loss: 0.1593 - precision: 0.9363 - recall: 0.9363 - f1: 0.9363 - acc: 0.9363 - val_loss: 1.0495 - val_precision: 0.5545 - val_recall: 0.5545 - val_f1: 0.5545 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.10534\n",
      "Epoch 54/100\n",
      " - 35s - loss: 0.1906 - precision: 0.9253 - recall: 0.9253 - f1: 0.9253 - acc: 0.9253 - val_loss: 1.4449 - val_precision: 0.5064 - val_recall: 0.5064 - val_f1: 0.5064 - val_acc: 0.5064\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.10534\n",
      "Epoch 55/100\n",
      " - 35s - loss: 0.1330 - precision: 0.9465 - recall: 0.9465 - f1: 0.9465 - acc: 0.9465 - val_loss: 0.2712 - val_precision: 0.8814 - val_recall: 0.8814 - val_f1: 0.8814 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.10534\n",
      "Epoch 56/100\n",
      " - 35s - loss: 0.1230 - precision: 0.9560 - recall: 0.9560 - f1: 0.9560 - acc: 0.9560 - val_loss: 0.1050 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.10534 to 0.10500, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 57/100\n",
      " - 35s - loss: 0.1193 - precision: 0.9536 - recall: 0.9536 - f1: 0.9536 - acc: 0.9536 - val_loss: 0.0566 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.10500 to 0.05658, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 58/100\n",
      " - 35s - loss: 0.1206 - precision: 0.9623 - recall: 0.9623 - f1: 0.9623 - acc: 0.9623 - val_loss: 0.1515 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.05658\n",
      "Epoch 59/100\n",
      " - 36s - loss: 0.1384 - precision: 0.9458 - recall: 0.9458 - f1: 0.9458 - acc: 0.9458 - val_loss: 0.1806 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.05658\n",
      "Epoch 60/100\n",
      " - 35s - loss: 0.1486 - precision: 0.9395 - recall: 0.9395 - f1: 0.9395 - acc: 0.9395 - val_loss: 0.1369 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.05658\n",
      "Epoch 61/100\n",
      " - 36s - loss: 0.1468 - precision: 0.9410 - recall: 0.9410 - f1: 0.9410 - acc: 0.9410 - val_loss: 0.1384 - val_precision: 0.9359 - val_recall: 0.9359 - val_f1: 0.9359 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.05658\n",
      "Epoch 62/100\n",
      " - 35s - loss: 0.1668 - precision: 0.9395 - recall: 0.9395 - f1: 0.9395 - acc: 0.9395 - val_loss: 1.3286 - val_precision: 0.4423 - val_recall: 0.4423 - val_f1: 0.4423 - val_acc: 0.4423\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.05658\n",
      "Epoch 63/100\n",
      " - 36s - loss: 0.1405 - precision: 0.9434 - recall: 0.9434 - f1: 0.9434 - acc: 0.9434 - val_loss: 0.1206 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.05658\n",
      "Epoch 64/100\n",
      " - 36s - loss: 0.1177 - precision: 0.9544 - recall: 0.9544 - f1: 0.9544 - acc: 0.9544 - val_loss: 0.0861 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.05658\n",
      "Epoch 65/100\n",
      " - 35s - loss: 0.1060 - precision: 0.9599 - recall: 0.9599 - f1: 0.9599 - acc: 0.9599 - val_loss: 0.1175 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05658\n",
      "Epoch 66/100\n",
      " - 35s - loss: 0.1098 - precision: 0.9552 - recall: 0.9552 - f1: 0.9552 - acc: 0.9552 - val_loss: 0.2145 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: val_loss did not improve from 0.05658\n",
      "Epoch 67/100\n",
      " - 36s - loss: 0.1275 - precision: 0.9505 - recall: 0.9505 - f1: 0.9505 - acc: 0.9505 - val_loss: 0.2268 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05658\n",
      "Epoch 68/100\n",
      " - 36s - loss: 0.1936 - precision: 0.9253 - recall: 0.9253 - f1: 0.9253 - acc: 0.9253 - val_loss: 0.2981 - val_precision: 0.8429 - val_recall: 0.8429 - val_f1: 0.8429 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05658\n",
      "Epoch 69/100\n",
      " - 36s - loss: 0.1463 - precision: 0.9410 - recall: 0.9410 - f1: 0.9410 - acc: 0.9410 - val_loss: 1.1182 - val_precision: 0.6154 - val_recall: 0.6154 - val_f1: 0.6154 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05658\n",
      "Epoch 70/100\n",
      " - 36s - loss: 0.1040 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.2220 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05658\n",
      "Epoch 71/100\n",
      " - 35s - loss: 0.1413 - precision: 0.9371 - recall: 0.9371 - f1: 0.9371 - acc: 0.9371 - val_loss: 0.9685 - val_precision: 0.7853 - val_recall: 0.7853 - val_f1: 0.7853 - val_acc: 0.7853\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05658\n",
      "Epoch 72/100\n",
      " - 35s - loss: 0.1674 - precision: 0.9285 - recall: 0.9285 - f1: 0.9285 - acc: 0.9285 - val_loss: 0.1457 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05658\n",
      "Epoch 73/100\n",
      " - 35s - loss: 0.1447 - precision: 0.9434 - recall: 0.9434 - f1: 0.9434 - acc: 0.9434 - val_loss: 0.2684 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.05658\n",
      "Epoch 74/100\n",
      " - 35s - loss: 0.1086 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.1802 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.05658\n",
      "Epoch 75/100\n",
      " - 36s - loss: 0.1101 - precision: 0.9552 - recall: 0.9552 - f1: 0.9552 - acc: 0.9552 - val_loss: 0.1631 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05658\n",
      "Epoch 76/100\n",
      " - 36s - loss: 0.1075 - precision: 0.9568 - recall: 0.9568 - f1: 0.9568 - acc: 0.9568 - val_loss: 0.0636 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.05658\n",
      "Epoch 77/100\n",
      " - 35s - loss: 0.0766 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.0753 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.05658\n",
      "Epoch 78/100\n",
      " - 36s - loss: 0.0809 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.0429 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.05658 to 0.04286, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 79/100\n",
      " - 35s - loss: 0.0950 - precision: 0.9693 - recall: 0.9693 - f1: 0.9693 - acc: 0.9693 - val_loss: 0.0681 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.04286\n",
      "Epoch 80/100\n",
      " - 35s - loss: 0.0846 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.1051 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.04286\n",
      "Epoch 81/100\n",
      " - 35s - loss: 0.1145 - precision: 0.9560 - recall: 0.9560 - f1: 0.9560 - acc: 0.9560 - val_loss: 0.1285 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.04286\n",
      "Epoch 82/100\n",
      " - 36s - loss: 0.1031 - precision: 0.9654 - recall: 0.9654 - f1: 0.9654 - acc: 0.9654 - val_loss: 0.1011 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.04286\n",
      "Epoch 83/100\n",
      " - 35s - loss: 0.1118 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.1910 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.04286\n",
      "Epoch 84/100\n",
      " - 35s - loss: 0.1098 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.2280 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.04286\n",
      "Epoch 85/100\n",
      " - 35s - loss: 0.1350 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.1375 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.04286\n",
      "Epoch 86/100\n",
      " - 35s - loss: 0.1340 - precision: 0.9528 - recall: 0.9528 - f1: 0.9528 - acc: 0.9528 - val_loss: 0.1534 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.04286\n",
      "Epoch 87/100\n",
      " - 36s - loss: 0.1189 - precision: 0.9528 - recall: 0.9528 - f1: 0.9528 - acc: 0.9528 - val_loss: 0.2132 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.04286\n",
      "Epoch 88/100\n",
      " - 35s - loss: 0.1405 - precision: 0.9489 - recall: 0.9489 - f1: 0.9489 - acc: 0.9489 - val_loss: 0.9025 - val_precision: 0.6346 - val_recall: 0.6346 - val_f1: 0.6346 - val_acc: 0.6346\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.04286\n",
      "Epoch 89/100\n",
      " - 35s - loss: 0.0957 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.3046 - val_precision: 0.8686 - val_recall: 0.8686 - val_f1: 0.8686 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.04286\n",
      "Epoch 90/100\n",
      " - 36s - loss: 0.0882 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.1053 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.04286\n",
      "Epoch 91/100\n",
      " - 35s - loss: 0.0903 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.0840 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.04286\n",
      "Epoch 92/100\n",
      " - 35s - loss: 0.0566 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.0456 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.04286\n",
      "Epoch 93/100\n",
      " - 35s - loss: 0.0738 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0434 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.04286\n",
      "Epoch 94/100\n",
      " - 35s - loss: 0.0782 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.0513 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.04286\n",
      "Epoch 95/100\n",
      " - 35s - loss: 0.0630 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.0629 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.04286\n",
      "Epoch 96/100\n",
      " - 35s - loss: 0.0764 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.1463 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.04286\n",
      "Epoch 97/100\n",
      " - 36s - loss: 0.0865 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.2889 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.04286\n",
      "Epoch 98/100\n",
      " - 35s - loss: 0.0955 - precision: 0.9678 - recall: 0.9678 - f1: 0.9678 - acc: 0.9678 - val_loss: 0.2116 - val_precision: 0.8974 - val_recall: 0.8974 - val_f1: 0.8974 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.04286\n",
      "Epoch 99/100\n",
      " - 36s - loss: 0.0926 - precision: 0.9638 - recall: 0.9638 - f1: 0.9638 - acc: 0.9638 - val_loss: 0.1655 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00099: val_loss did not improve from 0.04286\n",
      "Epoch 100/100\n",
      " - 36s - loss: 0.1155 - precision: 0.9513 - recall: 0.9513 - f1: 0.9513 - acc: 0.9513 - val_loss: 0.2731 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04286\n",
      "Epoch 1/100\n",
      " - 37s - loss: 0.1576 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.4421 - val_precision: 0.7756 - val_recall: 0.7756 - val_f1: 0.7756 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44208, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 2/100\n",
      " - 35s - loss: 0.1667 - precision: 0.9347 - recall: 0.9347 - f1: 0.9347 - acc: 0.9347 - val_loss: 0.5749 - val_precision: 0.7853 - val_recall: 0.7853 - val_f1: 0.7853 - val_acc: 0.7853\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44208\n",
      "Epoch 3/100\n",
      " - 36s - loss: 0.1832 - precision: 0.9308 - recall: 0.9308 - f1: 0.9308 - acc: 0.9308 - val_loss: 0.1883 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44208 to 0.18826, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 4/100\n",
      " - 35s - loss: 0.1547 - precision: 0.9387 - recall: 0.9387 - f1: 0.9387 - acc: 0.9387 - val_loss: 0.1036 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18826 to 0.10362, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 5/100\n",
      " - 36s - loss: 0.1133 - precision: 0.9536 - recall: 0.9536 - f1: 0.9536 - acc: 0.9536 - val_loss: 0.2009 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10362\n",
      "Epoch 6/100\n",
      " - 35s - loss: 0.1448 - precision: 0.9450 - recall: 0.9450 - f1: 0.9450 - acc: 0.9450 - val_loss: 0.3684 - val_precision: 0.8686 - val_recall: 0.8686 - val_f1: 0.8686 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10362\n",
      "Epoch 7/100\n",
      " - 35s - loss: 0.1401 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.3587 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10362\n",
      "Epoch 8/100\n",
      " - 35s - loss: 0.1442 - precision: 0.9497 - recall: 0.9497 - f1: 0.9497 - acc: 0.9497 - val_loss: 0.5438 - val_precision: 0.7981 - val_recall: 0.7981 - val_f1: 0.7981 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10362\n",
      "Epoch 9/100\n",
      " - 36s - loss: 0.1509 - precision: 0.9473 - recall: 0.9473 - f1: 0.9473 - acc: 0.9473 - val_loss: 0.1594 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10362\n",
      "Epoch 10/100\n",
      " - 36s - loss: 0.1426 - precision: 0.9481 - recall: 0.9481 - f1: 0.9481 - acc: 0.9481 - val_loss: 0.2190 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10362\n",
      "Epoch 11/100\n",
      " - 37s - loss: 0.1753 - precision: 0.9292 - recall: 0.9292 - f1: 0.9292 - acc: 0.9292 - val_loss: 1.5719 - val_precision: 0.6923 - val_recall: 0.6923 - val_f1: 0.6923 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.10362\n",
      "Epoch 12/100\n",
      " - 38s - loss: 0.1271 - precision: 0.9544 - recall: 0.9544 - f1: 0.9544 - acc: 0.9544 - val_loss: 0.1181 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.10362\n",
      "Epoch 13/100\n",
      " - 36s - loss: 0.1186 - precision: 0.9568 - recall: 0.9568 - f1: 0.9568 - acc: 0.9568 - val_loss: 0.1109 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.10362\n",
      "Epoch 14/100\n",
      " - 35s - loss: 0.1325 - precision: 0.9473 - recall: 0.9473 - f1: 0.9473 - acc: 0.9473 - val_loss: 0.2909 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.10362\n",
      "Epoch 15/100\n",
      " - 36s - loss: 0.1520 - precision: 0.9347 - recall: 0.9347 - f1: 0.9347 - acc: 0.9347 - val_loss: 0.1144 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.10362\n",
      "Epoch 16/100\n",
      " - 36s - loss: 0.1095 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.0504 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.10362 to 0.05037, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 17/100\n",
      " - 36s - loss: 0.0950 - precision: 0.9607 - recall: 0.9607 - f1: 0.9607 - acc: 0.9607 - val_loss: 1.1888 - val_precision: 0.6827 - val_recall: 0.6827 - val_f1: 0.6827 - val_acc: 0.6827\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.05037\n",
      "Epoch 18/100\n",
      " - 36s - loss: 0.1480 - precision: 0.9403 - recall: 0.9403 - f1: 0.9403 - acc: 0.9403 - val_loss: 0.2286 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05037\n",
      "Epoch 19/100\n",
      " - 36s - loss: 0.1031 - precision: 0.9513 - recall: 0.9513 - f1: 0.9513 - acc: 0.9513 - val_loss: 0.0974 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.05037\n",
      "Epoch 20/100\n",
      " - 36s - loss: 0.1135 - precision: 0.9607 - recall: 0.9607 - f1: 0.9607 - acc: 0.9607 - val_loss: 0.0409 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05037 to 0.04089, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 21/100\n",
      " - 36s - loss: 0.1059 - precision: 0.9638 - recall: 0.9638 - f1: 0.9638 - acc: 0.9638 - val_loss: 0.1923 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.04089\n",
      "Epoch 22/100\n",
      " - 36s - loss: 0.1511 - precision: 0.9458 - recall: 0.9458 - f1: 0.9458 - acc: 0.9458 - val_loss: 0.2663 - val_precision: 0.8814 - val_recall: 0.8814 - val_f1: 0.8814 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.04089\n",
      "Epoch 23/100\n",
      " - 36s - loss: 0.1129 - precision: 0.9560 - recall: 0.9560 - f1: 0.9560 - acc: 0.9560 - val_loss: 0.0607 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.04089\n",
      "Epoch 24/100\n",
      " - 35s - loss: 0.1048 - precision: 0.9615 - recall: 0.9615 - f1: 0.9615 - acc: 0.9615 - val_loss: 0.2178 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.04089\n",
      "Epoch 25/100\n",
      " - 36s - loss: 0.1235 - precision: 0.9536 - recall: 0.9536 - f1: 0.9536 - acc: 0.9536 - val_loss: 0.1949 - val_precision: 0.9359 - val_recall: 0.9359 - val_f1: 0.9359 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.04089\n",
      "Epoch 26/100\n",
      " - 36s - loss: 0.1551 - precision: 0.9347 - recall: 0.9347 - f1: 0.9347 - acc: 0.9347 - val_loss: 0.1431 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.04089\n",
      "Epoch 27/100\n",
      " - 36s - loss: 0.0905 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.0766 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04089\n",
      "Epoch 28/100\n",
      " - 36s - loss: 0.0954 - precision: 0.9631 - recall: 0.9631 - f1: 0.9631 - acc: 0.9631 - val_loss: 0.0822 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.04089\n",
      "Epoch 29/100\n",
      " - 36s - loss: 0.1156 - precision: 0.9552 - recall: 0.9552 - f1: 0.9552 - acc: 0.9552 - val_loss: 2.6425 - val_precision: 0.5321 - val_recall: 0.5321 - val_f1: 0.5321 - val_acc: 0.5321\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04089\n",
      "Epoch 30/100\n",
      " - 35s - loss: 0.1305 - precision: 0.9536 - recall: 0.9536 - f1: 0.9536 - acc: 0.9536 - val_loss: 0.2253 - val_precision: 0.9359 - val_recall: 0.9359 - val_f1: 0.9359 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04089\n",
      "Epoch 31/100\n",
      " - 36s - loss: 0.0814 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.1202 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss did not improve from 0.04089\n",
      "Epoch 32/100\n",
      " - 36s - loss: 0.1225 - precision: 0.9520 - recall: 0.9520 - f1: 0.9520 - acc: 0.9520 - val_loss: 0.1365 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04089\n",
      "Epoch 33/100\n",
      " - 36s - loss: 0.1126 - precision: 0.9654 - recall: 0.9654 - f1: 0.9654 - acc: 0.9654 - val_loss: 0.0970 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04089\n",
      "Epoch 34/100\n",
      " - 36s - loss: 0.0788 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.0502 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04089\n",
      "Epoch 35/100\n",
      " - 36s - loss: 0.0859 - precision: 0.9693 - recall: 0.9693 - f1: 0.9693 - acc: 0.9693 - val_loss: 0.0610 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.04089\n",
      "Epoch 36/100\n",
      " - 36s - loss: 0.1039 - precision: 0.9607 - recall: 0.9607 - f1: 0.9607 - acc: 0.9607 - val_loss: 0.0983 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.04089\n",
      "Epoch 37/100\n",
      " - 36s - loss: 0.1254 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.7399 - val_precision: 0.6795 - val_recall: 0.6795 - val_f1: 0.6795 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.04089\n",
      "Epoch 38/100\n",
      " - 35s - loss: 0.1125 - precision: 0.9623 - recall: 0.9623 - f1: 0.9623 - acc: 0.9623 - val_loss: 2.6580 - val_precision: 0.5192 - val_recall: 0.5192 - val_f1: 0.5192 - val_acc: 0.5192\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04089\n",
      "Epoch 39/100\n",
      " - 36s - loss: 0.1033 - precision: 0.9615 - recall: 0.9615 - f1: 0.9615 - acc: 0.9615 - val_loss: 0.1382 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.04089\n",
      "Epoch 40/100\n",
      " - 36s - loss: 0.0952 - precision: 0.9638 - recall: 0.9638 - f1: 0.9638 - acc: 0.9638 - val_loss: 0.0618 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.04089\n",
      "Epoch 41/100\n",
      " - 36s - loss: 0.0630 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0474 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.04089\n",
      "Epoch 42/100\n",
      " - 36s - loss: 0.0774 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.0255 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04089 to 0.02552, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 43/100\n",
      " - 36s - loss: 0.0719 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0185 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.02552 to 0.01852, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 44/100\n",
      " - 36s - loss: 0.0695 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.1997 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01852\n",
      "Epoch 45/100\n",
      " - 35s - loss: 0.1163 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.6334 - val_precision: 0.8462 - val_recall: 0.8462 - val_f1: 0.8462 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01852\n",
      "Epoch 46/100\n",
      " - 36s - loss: 0.1293 - precision: 0.9489 - recall: 0.9489 - f1: 0.9489 - acc: 0.9489 - val_loss: 0.3106 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01852\n",
      "Epoch 47/100\n",
      " - 36s - loss: 0.1111 - precision: 0.9544 - recall: 0.9544 - f1: 0.9544 - acc: 0.9544 - val_loss: 0.2269 - val_precision: 0.9295 - val_recall: 0.9295 - val_f1: 0.9295 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01852\n",
      "Epoch 48/100\n",
      " - 36s - loss: 0.0785 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.2258 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01852\n",
      "Epoch 49/100\n",
      " - 36s - loss: 0.1013 - precision: 0.9638 - recall: 0.9638 - f1: 0.9638 - acc: 0.9638 - val_loss: 0.0449 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01852\n",
      "Epoch 50/100\n",
      " - 36s - loss: 0.0747 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.0776 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01852\n",
      "Epoch 51/100\n",
      " - 36s - loss: 0.0761 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0985 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01852\n",
      "Epoch 52/100\n",
      " - 36s - loss: 0.0822 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.1059 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01852\n",
      "Epoch 53/100\n",
      " - 36s - loss: 0.1230 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.7972 - val_precision: 0.7981 - val_recall: 0.7981 - val_f1: 0.7981 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01852\n",
      "Epoch 54/100\n",
      " - 36s - loss: 0.1366 - precision: 0.9520 - recall: 0.9520 - f1: 0.9520 - acc: 0.9520 - val_loss: 0.1555 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01852\n",
      "Epoch 55/100\n",
      " - 36s - loss: 0.0659 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.0404 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01852\n",
      "Epoch 56/100\n",
      " - 36s - loss: 0.0866 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.0338 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01852\n",
      "Epoch 57/100\n",
      " - 36s - loss: 0.0540 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0418 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01852\n",
      "Epoch 58/100\n",
      " - 36s - loss: 0.0558 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.0341 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01852\n",
      "Epoch 59/100\n",
      " - 36s - loss: 0.0925 - precision: 0.9638 - recall: 0.9638 - f1: 0.9638 - acc: 0.9638 - val_loss: 0.4173 - val_precision: 0.8429 - val_recall: 0.8429 - val_f1: 0.8429 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01852\n",
      "Epoch 60/100\n",
      " - 36s - loss: 0.1247 - precision: 0.9568 - recall: 0.9568 - f1: 0.9568 - acc: 0.9568 - val_loss: 0.2115 - val_precision: 0.8846 - val_recall: 0.8846 - val_f1: 0.8846 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01852\n",
      "Epoch 61/100\n",
      " - 36s - loss: 0.1092 - precision: 0.9591 - recall: 0.9591 - f1: 0.9591 - acc: 0.9591 - val_loss: 0.2698 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01852\n",
      "Epoch 62/100\n",
      " - 36s - loss: 0.0955 - precision: 0.9623 - recall: 0.9623 - f1: 0.9623 - acc: 0.9623 - val_loss: 0.2647 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01852\n",
      "Epoch 63/100\n",
      " - 36s - loss: 0.0815 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.0820 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01852\n",
      "Epoch 64/100\n",
      " - 36s - loss: 0.0711 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0587 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01852\n",
      "Epoch 65/100\n",
      " - 36s - loss: 0.0587 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0352 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01852\n",
      "Epoch 66/100\n",
      " - 35s - loss: 0.0714 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0749 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01852\n",
      "Epoch 67/100\n",
      " - 36s - loss: 0.1063 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.2235 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01852\n",
      "Epoch 68/100\n",
      " - 36s - loss: 0.0788 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.4581 - val_precision: 0.8558 - val_recall: 0.8558 - val_f1: 0.8558 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01852\n",
      "Epoch 69/100\n",
      " - 36s - loss: 0.1094 - precision: 0.9599 - recall: 0.9599 - f1: 0.9599 - acc: 0.9599 - val_loss: 0.1593 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01852\n",
      "Epoch 70/100\n",
      " - 36s - loss: 0.1047 - precision: 0.9631 - recall: 0.9631 - f1: 0.9631 - acc: 0.9631 - val_loss: 0.0955 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01852\n",
      "Epoch 71/100\n",
      " - 36s - loss: 0.0728 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.5833 - val_precision: 0.7949 - val_recall: 0.7949 - val_f1: 0.7949 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01852\n",
      "Epoch 72/100\n",
      " - 36s - loss: 0.0958 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.7836 - val_precision: 0.7276 - val_recall: 0.7276 - val_f1: 0.7276 - val_acc: 0.7276\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01852\n",
      "Epoch 73/100\n",
      " - 36s - loss: 0.1182 - precision: 0.9623 - recall: 0.9623 - f1: 0.9623 - acc: 0.9623 - val_loss: 0.5607 - val_precision: 0.7853 - val_recall: 0.7853 - val_f1: 0.7853 - val_acc: 0.7853\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01852\n",
      "Epoch 74/100\n",
      " - 36s - loss: 0.0850 - precision: 0.9623 - recall: 0.9623 - f1: 0.9623 - acc: 0.9623 - val_loss: 0.0676 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01852\n",
      "Epoch 75/100\n",
      " - 36s - loss: 0.0616 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0851 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01852\n",
      "Epoch 76/100\n",
      " - 36s - loss: 0.0645 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.0336 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01852\n",
      "Epoch 77/100\n",
      " - 36s - loss: 0.0581 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0349 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01852\n",
      "Epoch 78/100\n",
      " - 36s - loss: 0.0568 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0437 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01852\n",
      "Epoch 79/100\n",
      " - 35s - loss: 0.0533 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0463 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01852\n",
      "Epoch 80/100\n",
      " - 35s - loss: 0.0846 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0446 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01852\n",
      "Epoch 81/100\n",
      " - 35s - loss: 0.0507 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.1730 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01852\n",
      "Epoch 82/100\n",
      " - 35s - loss: 0.0768 - precision: 0.9678 - recall: 0.9678 - f1: 0.9678 - acc: 0.9678 - val_loss: 0.1395 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01852\n",
      "Epoch 83/100\n",
      " - 36s - loss: 0.0756 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.1040 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01852\n",
      "Epoch 84/100\n",
      " - 36s - loss: 0.0950 - precision: 0.9654 - recall: 0.9654 - f1: 0.9654 - acc: 0.9654 - val_loss: 0.1866 - val_precision: 0.9295 - val_recall: 0.9295 - val_f1: 0.9295 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01852\n",
      "Epoch 85/100\n",
      " - 35s - loss: 0.0654 - precision: 0.9756 - recall: 0.9756 - f1: 0.9756 - acc: 0.9756 - val_loss: 0.3742 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01852\n",
      "Epoch 86/100\n",
      " - 35s - loss: 0.1069 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.0825 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01852\n",
      "Epoch 87/100\n",
      " - 36s - loss: 0.0783 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.0904 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01852\n",
      "Epoch 88/100\n",
      " - 36s - loss: 0.0912 - precision: 0.9678 - recall: 0.9678 - f1: 0.9678 - acc: 0.9678 - val_loss: 0.2050 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01852\n",
      "Epoch 89/100\n",
      " - 36s - loss: 0.0568 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.2718 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01852\n",
      "Epoch 90/100\n",
      " - 36s - loss: 0.0472 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0921 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01852\n",
      "Epoch 91/100\n",
      " - 36s - loss: 0.0490 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0368 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01852\n",
      "Epoch 92/100\n",
      " - 36s - loss: 0.0553 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0362 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01852\n",
      "Epoch 93/100\n",
      " - 36s - loss: 0.0498 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0257 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01852\n",
      "Epoch 94/100\n",
      " - 36s - loss: 0.0451 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0165 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.01852 to 0.01647, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 95/100\n",
      " - 36s - loss: 0.0495 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0248 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01647\n",
      "Epoch 96/100\n",
      " - 36s - loss: 0.0414 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0358 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01647\n",
      "Epoch 97/100\n",
      " - 36s - loss: 0.0593 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.3700 - val_precision: 0.8654 - val_recall: 0.8654 - val_f1: 0.8654 - val_acc: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01647\n",
      "Epoch 98/100\n",
      " - 36s - loss: 0.0577 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.4156 - val_precision: 0.8846 - val_recall: 0.8846 - val_f1: 0.8846 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01647\n",
      "Epoch 99/100\n",
      " - 36s - loss: 0.0782 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.1566 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01647\n",
      "Epoch 100/100\n",
      " - 37s - loss: 0.0981 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.2533 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01647\n",
      "Epoch 1/100\n",
      " - 38s - loss: 0.1222 - precision: 0.9591 - recall: 0.9591 - f1: 0.9591 - acc: 0.9591 - val_loss: 0.7589 - val_precision: 0.7340 - val_recall: 0.7340 - val_f1: 0.7340 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75886, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 2/100\n",
      " - 36s - loss: 0.0975 - precision: 0.9615 - recall: 0.9615 - f1: 0.9615 - acc: 0.9615 - val_loss: 0.3029 - val_precision: 0.8974 - val_recall: 0.8974 - val_f1: 0.8974 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.75886 to 0.30285, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 3/100\n",
      " - 36s - loss: 0.1102 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.1059 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30285 to 0.10586, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 4/100\n",
      " - 36s - loss: 0.0978 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.1487 - val_precision: 0.9359 - val_recall: 0.9359 - val_f1: 0.9359 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10586\n",
      "Epoch 5/100\n",
      " - 36s - loss: 0.0898 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.0874 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10586 to 0.08742, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 6/100\n",
      " - 36s - loss: 0.0903 - precision: 0.9654 - recall: 0.9654 - f1: 0.9654 - acc: 0.9654 - val_loss: 0.6516 - val_precision: 0.8045 - val_recall: 0.8045 - val_f1: 0.8045 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.08742\n",
      "Epoch 7/100\n",
      " - 36s - loss: 0.1009 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.0670 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08742 to 0.06703, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 8/100\n",
      " - 36s - loss: 0.1067 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.0820 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06703\n",
      "Epoch 9/100\n",
      " - 36s - loss: 0.0737 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0458 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06703 to 0.04583, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 10/100\n",
      " - 36s - loss: 0.0904 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.1414 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04583\n",
      "Epoch 11/100\n",
      " - 36s - loss: 0.0715 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.1423 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04583\n",
      "Epoch 12/100\n",
      " - 36s - loss: 0.0707 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.0477 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04583\n",
      "Epoch 13/100\n",
      " - 36s - loss: 0.0796 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.5547 - val_precision: 0.8462 - val_recall: 0.8462 - val_f1: 0.8462 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04583\n",
      "Epoch 14/100\n",
      " - 36s - loss: 0.1042 - precision: 0.9599 - recall: 0.9599 - f1: 0.9599 - acc: 0.9599 - val_loss: 1.1243 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04583\n",
      "Epoch 15/100\n",
      " - 36s - loss: 0.1082 - precision: 0.9591 - recall: 0.9591 - f1: 0.9591 - acc: 0.9591 - val_loss: 0.0465 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04583\n",
      "Epoch 16/100\n",
      " - 36s - loss: 0.0525 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0772 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04583\n",
      "Epoch 17/100\n",
      " - 36s - loss: 0.0696 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.1296 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04583\n",
      "Epoch 18/100\n",
      " - 36s - loss: 0.0975 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.3020 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04583\n",
      "Epoch 19/100\n",
      " - 36s - loss: 0.1080 - precision: 0.9623 - recall: 0.9623 - f1: 0.9623 - acc: 0.9623 - val_loss: 0.1584 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04583\n",
      "Epoch 20/100\n",
      " - 36s - loss: 0.0822 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.2171 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04583\n",
      "Epoch 21/100\n",
      " - 36s - loss: 0.0787 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.1022 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.04583\n",
      "Epoch 22/100\n",
      " - 35s - loss: 0.0874 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 1.5797 - val_precision: 0.7244 - val_recall: 0.7244 - val_f1: 0.7244 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.04583\n",
      "Epoch 23/100\n",
      " - 35s - loss: 0.0748 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.0346 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.04583 to 0.03465, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 24/100\n",
      " - 36s - loss: 0.0578 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0242 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.03465 to 0.02418, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 25/100\n",
      " - 35s - loss: 0.0808 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.1018 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02418\n",
      "Epoch 26/100\n",
      " - 36s - loss: 0.0973 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 1.1236 - val_precision: 0.6955 - val_recall: 0.6955 - val_f1: 0.6955 - val_acc: 0.6955\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02418\n",
      "Epoch 27/100\n",
      " - 35s - loss: 0.0716 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.0963 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02418\n",
      "Epoch 28/100\n",
      " - 36s - loss: 0.0558 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0423 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02418\n",
      "Epoch 29/100\n",
      " - 36s - loss: 0.1038 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.4823 - val_precision: 0.7853 - val_recall: 0.7853 - val_f1: 0.7853 - val_acc: 0.7853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02418\n",
      "Epoch 30/100\n",
      " - 35s - loss: 0.0998 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.2648 - val_precision: 0.8526 - val_recall: 0.8526 - val_f1: 0.8526 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02418\n",
      "Epoch 31/100\n",
      " - 36s - loss: 0.0769 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0787 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02418\n",
      "Epoch 32/100\n",
      " - 36s - loss: 0.0826 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.2055 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02418\n",
      "Epoch 33/100\n",
      " - 35s - loss: 0.0698 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0750 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02418\n",
      "Epoch 34/100\n",
      " - 36s - loss: 0.0526 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.0362 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02418\n",
      "Epoch 35/100\n",
      " - 36s - loss: 0.0521 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0628 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02418\n",
      "Epoch 36/100\n",
      " - 36s - loss: 0.0447 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0988 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02418\n",
      "Epoch 37/100\n",
      " - 36s - loss: 0.0890 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.1674 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02418\n",
      "Epoch 38/100\n",
      " - 36s - loss: 0.1011 - precision: 0.9678 - recall: 0.9678 - f1: 0.9678 - acc: 0.9678 - val_loss: 0.2791 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02418\n",
      "Epoch 39/100\n",
      " - 36s - loss: 0.0682 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.3528 - val_precision: 0.8526 - val_recall: 0.8526 - val_f1: 0.8526 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02418\n",
      "Epoch 40/100\n",
      " - 36s - loss: 0.0976 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.0316 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02418\n",
      "Epoch 41/100\n",
      " - 36s - loss: 0.0618 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0474 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02418\n",
      "Epoch 42/100\n",
      " - 36s - loss: 0.0543 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.0280 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02418\n",
      "Epoch 43/100\n",
      " - 36s - loss: 0.0630 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0527 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02418\n",
      "Epoch 44/100\n",
      " - 35s - loss: 0.0755 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.1279 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02418\n",
      "Epoch 45/100\n",
      " - 36s - loss: 0.0833 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.7295 - val_precision: 0.8301 - val_recall: 0.8301 - val_f1: 0.8301 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02418\n",
      "Epoch 46/100\n",
      " - 36s - loss: 0.1042 - precision: 0.9599 - recall: 0.9599 - f1: 0.9599 - acc: 0.9599 - val_loss: 0.5991 - val_precision: 0.7788 - val_recall: 0.7788 - val_f1: 0.7788 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02418\n",
      "Epoch 47/100\n",
      " - 35s - loss: 0.0632 - precision: 0.9827 - recall: 0.9827 - f1: 0.9827 - acc: 0.9827 - val_loss: 0.0860 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02418\n",
      "Epoch 48/100\n",
      " - 36s - loss: 0.0657 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0558 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02418\n",
      "Epoch 49/100\n",
      " - 35s - loss: 0.0569 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0450 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02418\n",
      "Epoch 50/100\n",
      " - 36s - loss: 0.0415 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0253 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02418\n",
      "Epoch 51/100\n",
      " - 36s - loss: 0.0431 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.0525 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02418\n",
      "Epoch 52/100\n",
      " - 36s - loss: 0.0521 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.1128 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02418\n",
      "Epoch 53/100\n",
      " - 36s - loss: 0.1047 - precision: 0.9591 - recall: 0.9591 - f1: 0.9591 - acc: 0.9591 - val_loss: 0.1368 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02418\n",
      "Epoch 54/100\n",
      " - 36s - loss: 0.0886 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.1670 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02418\n",
      "Epoch 55/100\n",
      " - 36s - loss: 0.0603 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.1479 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02418\n",
      "Epoch 56/100\n",
      " - 35s - loss: 0.0436 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.1381 - val_precision: 0.9519 - val_recall: 0.9519 - val_f1: 0.9519 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02418\n",
      "Epoch 57/100\n",
      " - 35s - loss: 0.0682 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.0797 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02418\n",
      "Epoch 58/100\n",
      " - 35s - loss: 0.0488 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0347 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02418\n",
      "Epoch 59/100\n",
      " - 35s - loss: 0.0670 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.1212 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02418\n",
      "Epoch 60/100\n",
      " - 36s - loss: 0.0934 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.9603 - val_precision: 0.7468 - val_recall: 0.7468 - val_f1: 0.7468 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02418\n",
      "Epoch 61/100\n",
      " - 36s - loss: 0.0872 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.1794 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02418\n",
      "Epoch 62/100\n",
      " - 35s - loss: 0.0844 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.0725 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02418\n",
      "Epoch 63/100\n",
      " - 36s - loss: 0.0585 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.0465 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02418\n",
      "Epoch 64/100\n",
      " - 35s - loss: 0.0685 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.0417 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02418\n",
      "Epoch 65/100\n",
      " - 36s - loss: 0.0509 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0417 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02418\n",
      "Epoch 66/100\n",
      " - 36s - loss: 0.0484 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0837 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02418\n",
      "Epoch 67/100\n",
      " - 35s - loss: 0.0688 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.7891 - val_precision: 0.7404 - val_recall: 0.7404 - val_f1: 0.7404 - val_acc: 0.7404\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02418\n",
      "Epoch 68/100\n",
      " - 35s - loss: 0.0726 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.0952 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02418\n",
      "Epoch 69/100\n",
      " - 36s - loss: 0.0684 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 1.2212 - val_precision: 0.7019 - val_recall: 0.7019 - val_f1: 0.7019 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02418\n",
      "Epoch 70/100\n",
      " - 35s - loss: 0.0667 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.1945 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02418\n",
      "Epoch 71/100\n",
      " - 35s - loss: 0.0596 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.6926 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02418\n",
      "Epoch 72/100\n",
      " - 35s - loss: 0.0653 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.2095 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02418\n",
      "Epoch 73/100\n",
      " - 36s - loss: 0.0859 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.1793 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02418\n",
      "Epoch 74/100\n",
      " - 35s - loss: 0.0498 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.6921 - val_precision: 0.8205 - val_recall: 0.8205 - val_f1: 0.8205 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02418\n",
      "Epoch 75/100\n",
      " - 36s - loss: 0.0671 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0687 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02418\n",
      "Epoch 76/100\n",
      " - 35s - loss: 0.0740 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0913 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02418\n",
      "Epoch 77/100\n",
      " - 35s - loss: 0.0503 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0577 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02418\n",
      "Epoch 78/100\n",
      " - 35s - loss: 0.0507 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0345 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02418\n",
      "Epoch 79/100\n",
      " - 36s - loss: 0.0330 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0481 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02418\n",
      "Epoch 80/100\n",
      " - 36s - loss: 0.0372 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0400 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02418\n",
      "Epoch 81/100\n",
      " - 36s - loss: 0.0214 - precision: 0.9945 - recall: 0.9945 - f1: 0.9945 - acc: 0.9945 - val_loss: 0.0905 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02418\n",
      "Epoch 82/100\n",
      " - 35s - loss: 0.0604 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.1776 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02418\n",
      "Epoch 83/100\n",
      " - 35s - loss: 0.0584 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.2633 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02418\n",
      "Epoch 84/100\n",
      " - 35s - loss: 0.0508 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.1419 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02418\n",
      "Epoch 85/100\n",
      " - 35s - loss: 0.0793 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.1980 - val_precision: 0.9327 - val_recall: 0.9327 - val_f1: 0.9327 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02418\n",
      "Epoch 86/100\n",
      " - 35s - loss: 0.0752 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.1800 - val_precision: 0.9006 - val_recall: 0.9006 - val_f1: 0.9006 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02418\n",
      "Epoch 87/100\n",
      " - 35s - loss: 0.0751 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.3645 - val_precision: 0.8974 - val_recall: 0.8974 - val_f1: 0.8974 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02418\n",
      "Epoch 88/100\n",
      " - 36s - loss: 0.0652 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.4449 - val_precision: 0.8686 - val_recall: 0.8686 - val_f1: 0.8686 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02418\n",
      "Epoch 89/100\n",
      " - 35s - loss: 0.0519 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.5076 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02418\n",
      "Epoch 90/100\n",
      " - 36s - loss: 0.0430 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0285 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02418\n",
      "Epoch 91/100\n",
      " - 36s - loss: 0.0356 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0425 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02418\n",
      "Epoch 92/100\n",
      " - 36s - loss: 0.0302 - precision: 0.9914 - recall: 0.9914 - f1: 0.9914 - acc: 0.9914 - val_loss: 0.0392 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02418\n",
      "Epoch 93/100\n",
      " - 36s - loss: 0.0225 - precision: 0.9937 - recall: 0.9937 - f1: 0.9937 - acc: 0.9937 - val_loss: 0.0167 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.02418 to 0.01665, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 94/100\n",
      " - 35s - loss: 0.0267 - precision: 0.9929 - recall: 0.9929 - f1: 0.9929 - acc: 0.9929 - val_loss: 0.0167 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01665\n",
      "Epoch 95/100\n",
      " - 36s - loss: 0.0403 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.1076 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01665\n",
      "Epoch 96/100\n",
      " - 36s - loss: 0.0341 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.1033 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01665\n",
      "Epoch 97/100\n",
      " - 35s - loss: 0.0587 - precision: 0.9827 - recall: 0.9827 - f1: 0.9827 - acc: 0.9827 - val_loss: 0.2526 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01665\n",
      "Epoch 98/100\n",
      " - 35s - loss: 0.0663 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.4112 - val_precision: 0.8590 - val_recall: 0.8590 - val_f1: 0.8590 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01665\n",
      "Epoch 99/100\n",
      " - 36s - loss: 0.0459 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.3297 - val_precision: 0.9038 - val_recall: 0.9038 - val_f1: 0.9038 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01665\n",
      "Epoch 100/100\n",
      " - 36s - loss: 0.0555 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.1123 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01665\n",
      "Epoch 1/100\n",
      " - 37s - loss: 0.0978 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.5417 - val_precision: 0.8269 - val_recall: 0.8269 - val_f1: 0.8269 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54169, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 2/100\n",
      " - 35s - loss: 0.0906 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.0939 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54169 to 0.09387, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 3/100\n",
      " - 36s - loss: 0.0908 - precision: 0.9654 - recall: 0.9654 - f1: 0.9654 - acc: 0.9654 - val_loss: 0.0694 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09387 to 0.06941, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 4/100\n",
      " - 36s - loss: 0.0648 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.2966 - val_precision: 0.9103 - val_recall: 0.9103 - val_f1: 0.9103 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06941\n",
      "Epoch 5/100\n",
      " - 35s - loss: 0.0981 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.0837 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06941\n",
      "Epoch 6/100\n",
      " - 36s - loss: 0.0688 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 1.6573 - val_precision: 0.7115 - val_recall: 0.7115 - val_f1: 0.7115 - val_acc: 0.7115\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06941\n",
      "Epoch 7/100\n",
      " - 36s - loss: 0.0693 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0594 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06941 to 0.05944, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 8/100\n",
      " - 36s - loss: 0.0997 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.7817 - val_precision: 0.7949 - val_recall: 0.7949 - val_f1: 0.7949 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05944\n",
      "Epoch 9/100\n",
      " - 36s - loss: 0.0722 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.2367 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05944\n",
      "Epoch 10/100\n",
      " - 35s - loss: 0.0603 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.0674 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.05944\n",
      "Epoch 11/100\n",
      " - 36s - loss: 0.0702 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 1.3836 - val_precision: 0.7179 - val_recall: 0.7179 - val_f1: 0.7179 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05944\n",
      "Epoch 12/100\n",
      " - 36s - loss: 0.0617 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.0429 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05944 to 0.04291, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 13/100\n",
      " - 36s - loss: 0.0460 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.0863 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04291\n",
      "Epoch 14/100\n",
      " - 35s - loss: 0.0589 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 2.1857 - val_precision: 0.4872 - val_recall: 0.4872 - val_f1: 0.4872 - val_acc: 0.4872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04291\n",
      "Epoch 15/100\n",
      " - 35s - loss: 0.0993 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.0785 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04291\n",
      "Epoch 16/100\n",
      " - 36s - loss: 0.0544 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0543 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04291\n",
      "Epoch 17/100\n",
      " - 36s - loss: 0.0418 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.8968 - val_precision: 0.7404 - val_recall: 0.7404 - val_f1: 0.7404 - val_acc: 0.7404\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04291\n",
      "Epoch 18/100\n",
      " - 36s - loss: 0.0740 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 1.4266 - val_precision: 0.5128 - val_recall: 0.5128 - val_f1: 0.5128 - val_acc: 0.5128\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04291\n",
      "Epoch 19/100\n",
      " - 36s - loss: 0.0610 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0866 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04291\n",
      "Epoch 20/100\n",
      " - 36s - loss: 0.0540 - precision: 0.9827 - recall: 0.9827 - f1: 0.9827 - acc: 0.9827 - val_loss: 0.0151 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.04291 to 0.01508, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 21/100\n",
      " - 36s - loss: 0.0759 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 1.4231 - val_precision: 0.7115 - val_recall: 0.7115 - val_f1: 0.7115 - val_acc: 0.7115\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01508\n",
      "Epoch 22/100\n",
      " - 36s - loss: 0.0650 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.2349 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01508\n",
      "Epoch 23/100\n",
      " - 36s - loss: 0.0637 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0875 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01508\n",
      "Epoch 24/100\n",
      " - 36s - loss: 0.0575 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0806 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01508\n",
      "Epoch 25/100\n",
      " - 35s - loss: 0.0429 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.1158 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01508\n",
      "Epoch 26/100\n",
      " - 35s - loss: 0.1077 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.1010 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01508\n",
      "Epoch 27/100\n",
      " - 35s - loss: 0.0605 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0342 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01508\n",
      "Epoch 28/100\n",
      " - 35s - loss: 0.0605 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0285 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01508\n",
      "Epoch 29/100\n",
      " - 35s - loss: 0.0840 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.3073 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01508\n",
      "Epoch 30/100\n",
      " - 35s - loss: 0.1161 - precision: 0.9599 - recall: 0.9599 - f1: 0.9599 - acc: 0.9599 - val_loss: 0.9715 - val_precision: 0.6538 - val_recall: 0.6538 - val_f1: 0.6538 - val_acc: 0.6538\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01508\n",
      "Epoch 31/100\n",
      " - 35s - loss: 0.0675 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.0643 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01508\n",
      "Epoch 32/100\n",
      " - 36s - loss: 0.0720 - precision: 0.9693 - recall: 0.9693 - f1: 0.9693 - acc: 0.9693 - val_loss: 0.5554 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01508\n",
      "Epoch 33/100\n",
      " - 36s - loss: 0.0595 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.1565 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01508\n",
      "Epoch 34/100\n",
      " - 35s - loss: 0.0486 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0484 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01508\n",
      "Epoch 35/100\n",
      " - 35s - loss: 0.0440 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0118 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01508 to 0.01178, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 36/100\n",
      " - 35s - loss: 0.0428 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.1113 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01178\n",
      "Epoch 37/100\n",
      " - 36s - loss: 0.0831 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.1846 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01178\n",
      "Epoch 38/100\n",
      " - 36s - loss: 0.0931 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.2308 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01178\n",
      "Epoch 39/100\n",
      " - 35s - loss: 0.0956 - precision: 0.9607 - recall: 0.9607 - f1: 0.9607 - acc: 0.9607 - val_loss: 0.1215 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01178\n",
      "Epoch 40/100\n",
      " - 36s - loss: 0.0552 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0198 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01178\n",
      "Epoch 41/100\n",
      " - 36s - loss: 0.0500 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.0663 - val_precision: 0.9712 - val_recall: 0.9712 - val_f1: 0.9712 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01178\n",
      "Epoch 42/100\n",
      " - 36s - loss: 0.0386 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0169 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01178\n",
      "Epoch 43/100\n",
      " - 36s - loss: 0.0433 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0917 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01178\n",
      "Epoch 44/100\n",
      " - 35s - loss: 0.0462 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.3080 - val_precision: 0.8910 - val_recall: 0.8910 - val_f1: 0.8910 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01178\n",
      "Epoch 45/100\n",
      " - 36s - loss: 0.0879 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.2226 - val_precision: 0.9199 - val_recall: 0.9199 - val_f1: 0.9199 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01178\n",
      "Epoch 46/100\n",
      " - 36s - loss: 0.0575 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.2606 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01178\n",
      "Epoch 47/100\n",
      " - 36s - loss: 0.0713 - precision: 0.9709 - recall: 0.9709 - f1: 0.9709 - acc: 0.9709 - val_loss: 0.2208 - val_precision: 0.9359 - val_recall: 0.9359 - val_f1: 0.9359 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01178\n",
      "Epoch 48/100\n",
      " - 36s - loss: 0.0540 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.2470 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01178\n",
      "Epoch 49/100\n",
      " - 36s - loss: 0.0391 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.1148 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01178\n",
      "Epoch 50/100\n",
      " - 36s - loss: 0.0378 - precision: 0.9882 - recall: 0.9882 - f1: 0.9882 - acc: 0.9882 - val_loss: 0.0235 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01178\n",
      "Epoch 51/100\n",
      " - 36s - loss: 0.0242 - precision: 0.9914 - recall: 0.9914 - f1: 0.9914 - acc: 0.9914 - val_loss: 0.0105 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.01178 to 0.01046, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 52/100\n",
      " - 36s - loss: 0.0509 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0417 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01046\n",
      "Epoch 53/100\n",
      " - 36s - loss: 0.0785 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.3388 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01046\n",
      "Epoch 54/100\n",
      " - 36s - loss: 0.0602 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.0599 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01046\n",
      "Epoch 55/100\n",
      " - 36s - loss: 0.0594 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.1056 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01046\n",
      "Epoch 56/100\n",
      " - 36s - loss: 0.0403 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0456 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01046\n",
      "Epoch 57/100\n",
      " - 35s - loss: 0.0405 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0171 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01046\n",
      "Epoch 58/100\n",
      " - 36s - loss: 0.0428 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.1665 - val_precision: 0.9295 - val_recall: 0.9295 - val_f1: 0.9295 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01046\n",
      "Epoch 59/100\n",
      " - 36s - loss: 0.0282 - precision: 0.9921 - recall: 0.9921 - f1: 0.9921 - acc: 0.9921 - val_loss: 0.1072 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01046\n",
      "Epoch 60/100\n",
      " - 36s - loss: 0.1000 - precision: 0.9686 - recall: 0.9686 - f1: 0.9686 - acc: 0.9686 - val_loss: 0.5543 - val_precision: 0.8077 - val_recall: 0.8077 - val_f1: 0.8077 - val_acc: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01046\n",
      "Epoch 61/100\n",
      " - 36s - loss: 0.0974 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.1657 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01046\n",
      "Epoch 62/100\n",
      " - 35s - loss: 0.0519 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.2317 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01046\n",
      "Epoch 63/100\n",
      " - 36s - loss: 0.0481 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0803 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01046\n",
      "Epoch 64/100\n",
      " - 36s - loss: 0.0687 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.0207 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01046\n",
      "Epoch 65/100\n",
      " - 36s - loss: 0.0344 - precision: 0.9882 - recall: 0.9882 - f1: 0.9882 - acc: 0.9882 - val_loss: 0.0195 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01046\n",
      "Epoch 66/100\n",
      " - 36s - loss: 0.0457 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0250 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01046\n",
      "Epoch 67/100\n",
      " - 35s - loss: 0.0445 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.1593 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01046\n",
      "Epoch 68/100\n",
      " - 36s - loss: 0.0783 - precision: 0.9701 - recall: 0.9701 - f1: 0.9701 - acc: 0.9701 - val_loss: 0.1802 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01046\n",
      "Epoch 69/100\n",
      " - 35s - loss: 0.0423 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.2730 - val_precision: 0.9038 - val_recall: 0.9038 - val_f1: 0.9038 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01046\n",
      "Epoch 70/100\n",
      " - 36s - loss: 0.0781 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.3166 - val_precision: 0.8974 - val_recall: 0.8974 - val_f1: 0.8974 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01046\n",
      "Epoch 71/100\n",
      " - 36s - loss: 0.0629 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.2022 - val_precision: 0.9263 - val_recall: 0.9263 - val_f1: 0.9263 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01046\n",
      "Epoch 72/100\n",
      " - 36s - loss: 0.0626 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0948 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01046\n",
      "Epoch 73/100\n",
      " - 35s - loss: 0.0453 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.2760 - val_precision: 0.9103 - val_recall: 0.9103 - val_f1: 0.9103 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01046\n",
      "Epoch 74/100\n",
      " - 36s - loss: 0.0466 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0685 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01046\n",
      "Epoch 75/100\n",
      " - 36s - loss: 0.0334 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0235 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01046\n",
      "Epoch 76/100\n",
      " - 36s - loss: 0.0480 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.0146 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01046\n",
      "Epoch 77/100\n",
      " - 35s - loss: 0.0399 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0074 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.01046 to 0.00739, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 78/100\n",
      " - 36s - loss: 0.0323 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0242 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00739\n",
      "Epoch 79/100\n",
      " - 35s - loss: 0.0315 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0374 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00739\n",
      "Epoch 80/100\n",
      " - 35s - loss: 0.0275 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0574 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00739\n",
      "Epoch 81/100\n",
      " - 35s - loss: 0.0358 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.0310 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00739\n",
      "Epoch 82/100\n",
      " - 35s - loss: 0.0347 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.7658 - val_precision: 0.8269 - val_recall: 0.8269 - val_f1: 0.8269 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00739\n",
      "Epoch 83/100\n",
      " - 35s - loss: 0.0481 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.1889 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00739\n",
      "Epoch 84/100\n",
      " - 35s - loss: 0.0532 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.3476 - val_precision: 0.9006 - val_recall: 0.9006 - val_f1: 0.9006 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00739\n",
      "Epoch 85/100\n",
      " - 35s - loss: 0.0684 - precision: 0.9764 - recall: 0.9764 - f1: 0.9764 - acc: 0.9764 - val_loss: 0.1215 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00739\n",
      "Epoch 86/100\n",
      " - 35s - loss: 0.0699 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.7765 - val_precision: 0.7244 - val_recall: 0.7244 - val_f1: 0.7244 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00739\n",
      "Epoch 87/100\n",
      " - 35s - loss: 0.0696 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 1.1185 - val_precision: 0.7724 - val_recall: 0.7724 - val_f1: 0.7724 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00739\n",
      "Epoch 88/100\n",
      " - 35s - loss: 0.0544 - precision: 0.9827 - recall: 0.9827 - f1: 0.9827 - acc: 0.9827 - val_loss: 0.0846 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00739\n",
      "Epoch 89/100\n",
      " - 36s - loss: 0.0417 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0220 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00739\n",
      "Epoch 90/100\n",
      " - 35s - loss: 0.0229 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0412 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00739\n",
      "Epoch 91/100\n",
      " - 35s - loss: 0.0401 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0139 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00739\n",
      "Epoch 92/100\n",
      " - 36s - loss: 0.0408 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0123 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00739\n",
      "Epoch 93/100\n",
      " - 35s - loss: 0.0388 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0120 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00739\n",
      "Epoch 94/100\n",
      " - 36s - loss: 0.0270 - precision: 0.9906 - recall: 0.9906 - f1: 0.9906 - acc: 0.9906 - val_loss: 0.0082 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00739\n",
      "Epoch 95/100\n",
      " - 35s - loss: 0.0336 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0133 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00739\n",
      "Epoch 96/100\n",
      " - 36s - loss: 0.0321 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0105 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00739\n",
      "Epoch 97/100\n",
      " - 36s - loss: 0.0315 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.1539 - val_precision: 0.9551 - val_recall: 0.9551 - val_f1: 0.9551 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00739\n",
      "Epoch 98/100\n",
      " - 36s - loss: 0.0525 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0942 - val_precision: 0.9455 - val_recall: 0.9455 - val_f1: 0.9455 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00739\n",
      "Epoch 99/100\n",
      " - 35s - loss: 0.0755 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.5148 - val_precision: 0.8686 - val_recall: 0.8686 - val_f1: 0.8686 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00739\n",
      "Epoch 100/100\n",
      " - 35s - loss: 0.0437 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0787 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00739\n",
      "Epoch 1/100\n",
      " - 38s - loss: 0.0891 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.1747 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17466, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 2/100\n",
      " - 35s - loss: 0.0790 - precision: 0.9678 - recall: 0.9678 - f1: 0.9678 - acc: 0.9678 - val_loss: 0.0564 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17466 to 0.05636, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 3/100\n",
      " - 36s - loss: 0.0695 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 0.0357 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05636 to 0.03574, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 4/100\n",
      " - 36s - loss: 0.0557 - precision: 0.9827 - recall: 0.9827 - f1: 0.9827 - acc: 0.9827 - val_loss: 0.7782 - val_precision: 0.7788 - val_recall: 0.7788 - val_f1: 0.7788 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03574\n",
      "Epoch 5/100\n",
      " - 36s - loss: 0.0589 - precision: 0.9741 - recall: 0.9741 - f1: 0.9741 - acc: 0.9741 - val_loss: 0.0498 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03574\n",
      "Epoch 6/100\n",
      " - 35s - loss: 0.0799 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 0.3881 - val_precision: 0.9487 - val_recall: 0.9487 - val_f1: 0.9487 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03574\n",
      "Epoch 7/100\n",
      " - 36s - loss: 0.0637 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.0791 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03574\n",
      "Epoch 8/100\n",
      " - 36s - loss: 0.0850 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.3802 - val_precision: 0.8942 - val_recall: 0.8942 - val_f1: 0.8942 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03574\n",
      "Epoch 9/100\n",
      " - 35s - loss: 0.0528 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0959 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03574\n",
      "Epoch 10/100\n",
      " - 35s - loss: 0.0672 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0637 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03574\n",
      "Epoch 11/100\n",
      " - 36s - loss: 0.0507 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.2703 - val_precision: 0.9071 - val_recall: 0.9071 - val_f1: 0.9071 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03574\n",
      "Epoch 12/100\n",
      " - 35s - loss: 0.0588 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.0103 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03574 to 0.01028, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 13/100\n",
      " - 36s - loss: 0.0372 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0435 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01028\n",
      "Epoch 14/100\n",
      " - 35s - loss: 0.0543 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.3439 - val_precision: 0.9038 - val_recall: 0.9038 - val_f1: 0.9038 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01028\n",
      "Epoch 15/100\n",
      " - 36s - loss: 0.1046 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.3245 - val_precision: 0.9006 - val_recall: 0.9006 - val_f1: 0.9006 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01028\n",
      "Epoch 16/100\n",
      " - 36s - loss: 0.0550 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0160 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01028\n",
      "Epoch 17/100\n",
      " - 36s - loss: 0.0550 - precision: 0.9756 - recall: 0.9756 - f1: 0.9756 - acc: 0.9756 - val_loss: 0.1738 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01028\n",
      "Epoch 18/100\n",
      " - 36s - loss: 0.0682 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0297 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01028\n",
      "Epoch 19/100\n",
      " - 36s - loss: 0.0658 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0477 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01028\n",
      "Epoch 20/100\n",
      " - 36s - loss: 0.0518 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0406 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01028\n",
      "Epoch 21/100\n",
      " - 35s - loss: 0.0629 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.1563 - val_precision: 0.9135 - val_recall: 0.9135 - val_f1: 0.9135 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01028\n",
      "Epoch 22/100\n",
      " - 36s - loss: 0.0863 - precision: 0.9756 - recall: 0.9756 - f1: 0.9756 - acc: 0.9756 - val_loss: 0.0373 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01028\n",
      "Epoch 23/100\n",
      " - 35s - loss: 0.0576 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0248 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01028\n",
      "Epoch 24/100\n",
      " - 36s - loss: 0.0523 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.2278 - val_precision: 0.9103 - val_recall: 0.9103 - val_f1: 0.9103 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01028\n",
      "Epoch 25/100\n",
      " - 36s - loss: 0.0695 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.0868 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01028\n",
      "Epoch 26/100\n",
      " - 36s - loss: 0.0931 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.5516 - val_precision: 0.8718 - val_recall: 0.8718 - val_f1: 0.8718 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01028\n",
      "Epoch 27/100\n",
      " - 36s - loss: 0.0535 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0278 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01028\n",
      "Epoch 28/100\n",
      " - 35s - loss: 0.0297 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0188 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01028\n",
      "Epoch 29/100\n",
      " - 35s - loss: 0.0574 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 1.3462 - val_precision: 0.5962 - val_recall: 0.5962 - val_f1: 0.5962 - val_acc: 0.5962\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01028\n",
      "Epoch 30/100\n",
      " - 36s - loss: 0.0679 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0484 - val_precision: 0.9808 - val_recall: 0.9808 - val_f1: 0.9808 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01028\n",
      "Epoch 31/100\n",
      " - 35s - loss: 0.0527 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0583 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01028\n",
      "Epoch 32/100\n",
      " - 35s - loss: 0.0874 - precision: 0.9646 - recall: 0.9646 - f1: 0.9646 - acc: 0.9646 - val_loss: 0.0901 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01028\n",
      "Epoch 33/100\n",
      " - 35s - loss: 0.0418 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0278 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01028\n",
      "Epoch 34/100\n",
      " - 35s - loss: 0.0512 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0159 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01028\n",
      "Epoch 35/100\n",
      " - 35s - loss: 0.0354 - precision: 0.9906 - recall: 0.9906 - f1: 0.9906 - acc: 0.9906 - val_loss: 0.0117 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01028\n",
      "Epoch 36/100\n",
      " - 35s - loss: 0.0462 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0118 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01028\n",
      "Epoch 37/100\n",
      " - 35s - loss: 0.0441 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0234 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01028\n",
      "Epoch 38/100\n",
      " - 36s - loss: 0.0599 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 0.1527 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01028\n",
      "Epoch 39/100\n",
      " - 36s - loss: 0.0640 - precision: 0.9733 - recall: 0.9733 - f1: 0.9733 - acc: 0.9733 - val_loss: 1.5770 - val_precision: 0.6474 - val_recall: 0.6474 - val_f1: 0.6474 - val_acc: 0.6474\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01028\n",
      "Epoch 40/100\n",
      " - 35s - loss: 0.0426 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0186 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01028\n",
      "Epoch 41/100\n",
      " - 35s - loss: 0.0356 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0118 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01028\n",
      "Epoch 42/100\n",
      " - 36s - loss: 0.0318 - precision: 0.9882 - recall: 0.9882 - f1: 0.9882 - acc: 0.9882 - val_loss: 0.0075 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01028 to 0.00746, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 43/100\n",
      " - 35s - loss: 0.0344 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0485 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00746\n",
      "Epoch 44/100\n",
      " - 35s - loss: 0.0417 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.7335 - val_precision: 0.7821 - val_recall: 0.7821 - val_f1: 0.7821 - val_acc: 0.7821\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00746\n",
      "Epoch 45/100\n",
      " - 35s - loss: 0.0541 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.3106 - val_precision: 0.8814 - val_recall: 0.8814 - val_f1: 0.8814 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00746\n",
      "Epoch 46/100\n",
      " - 35s - loss: 0.0707 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.7080 - val_precision: 0.7692 - val_recall: 0.7692 - val_f1: 0.7692 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00746\n",
      "Epoch 47/100\n",
      " - 35s - loss: 0.0547 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0883 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00746\n",
      "Epoch 48/100\n",
      " - 35s - loss: 0.0380 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.0382 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00746\n",
      "Epoch 49/100\n",
      " - 35s - loss: 0.0265 - precision: 0.9914 - recall: 0.9914 - f1: 0.9914 - acc: 0.9914 - val_loss: 0.0388 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00746\n",
      "Epoch 50/100\n",
      " - 35s - loss: 0.0301 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0103 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00746\n",
      "Epoch 51/100\n",
      " - 35s - loss: 0.0545 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.1695 - val_precision: 0.9231 - val_recall: 0.9231 - val_f1: 0.9231 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00746\n",
      "Epoch 52/100\n",
      " - 36s - loss: 0.0488 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0941 - val_precision: 0.9679 - val_recall: 0.9679 - val_f1: 0.9679 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00746\n",
      "Epoch 53/100\n",
      " - 35s - loss: 0.0626 - precision: 0.9717 - recall: 0.9717 - f1: 0.9717 - acc: 0.9717 - val_loss: 0.0402 - val_precision: 0.9872 - val_recall: 0.9872 - val_f1: 0.9872 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00746\n",
      "Epoch 54/100\n",
      " - 36s - loss: 0.0693 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 1.0448 - val_precision: 0.7372 - val_recall: 0.7372 - val_f1: 0.7372 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00746\n",
      "Epoch 55/100\n",
      " - 35s - loss: 0.0464 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0678 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00746\n",
      "Epoch 56/100\n",
      " - 36s - loss: 0.0295 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0206 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00746\n",
      "Epoch 57/100\n",
      " - 35s - loss: 0.0345 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0149 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00746\n",
      "Epoch 58/100\n",
      " - 36s - loss: 0.0242 - precision: 0.9953 - recall: 0.9953 - f1: 0.9953 - acc: 0.9953 - val_loss: 0.0092 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00746\n",
      "Epoch 59/100\n",
      " - 36s - loss: 0.0332 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0149 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00746\n",
      "Epoch 60/100\n",
      " - 36s - loss: 0.0472 - precision: 0.9811 - recall: 0.9811 - f1: 0.9811 - acc: 0.9811 - val_loss: 3.3593 - val_precision: 0.7051 - val_recall: 0.7051 - val_f1: 0.7051 - val_acc: 0.7051\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00746\n",
      "Epoch 61/100\n",
      " - 36s - loss: 0.0938 - precision: 0.9662 - recall: 0.9662 - f1: 0.9662 - acc: 0.9662 - val_loss: 0.0650 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00746\n",
      "Epoch 62/100\n",
      " - 35s - loss: 0.0614 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0153 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00746\n",
      "Epoch 63/100\n",
      " - 36s - loss: 0.0385 - precision: 0.9882 - recall: 0.9882 - f1: 0.9882 - acc: 0.9882 - val_loss: 0.0287 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00746\n",
      "Epoch 64/100\n",
      " - 36s - loss: 0.0295 - precision: 0.9906 - recall: 0.9906 - f1: 0.9906 - acc: 0.9906 - val_loss: 0.0292 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00746\n",
      "Epoch 65/100\n",
      " - 36s - loss: 0.0370 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0135 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00746\n",
      "Epoch 66/100\n",
      " - 36s - loss: 0.0232 - precision: 0.9914 - recall: 0.9914 - f1: 0.9914 - acc: 0.9914 - val_loss: 0.0548 - val_precision: 0.9776 - val_recall: 0.9776 - val_f1: 0.9776 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00746\n",
      "Epoch 67/100\n",
      " - 35s - loss: 0.0470 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 1.8429 - val_precision: 0.7115 - val_recall: 0.7115 - val_f1: 0.7115 - val_acc: 0.7115\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00746\n",
      "Epoch 68/100\n",
      " - 35s - loss: 0.0517 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.3460 - val_precision: 0.8814 - val_recall: 0.8814 - val_f1: 0.8814 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00746\n",
      "Epoch 69/100\n",
      " - 35s - loss: 0.0588 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.0307 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00746\n",
      "Epoch 70/100\n",
      " - 36s - loss: 0.0449 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0298 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00746\n",
      "Epoch 71/100\n",
      " - 36s - loss: 0.0393 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.1342 - val_precision: 0.9391 - val_recall: 0.9391 - val_f1: 0.9391 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00746\n",
      "Epoch 72/100\n",
      " - 36s - loss: 0.0561 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0596 - val_precision: 0.9840 - val_recall: 0.9840 - val_f1: 0.9840 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00746\n",
      "Epoch 73/100\n",
      " - 35s - loss: 0.0399 - precision: 0.9858 - recall: 0.9858 - f1: 0.9858 - acc: 0.9858 - val_loss: 0.0091 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00746\n",
      "Epoch 74/100\n",
      " - 36s - loss: 0.0265 - precision: 0.9937 - recall: 0.9937 - f1: 0.9937 - acc: 0.9937 - val_loss: 0.0096 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00746\n",
      "Epoch 75/100\n",
      " - 36s - loss: 0.0305 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0093 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00746\n",
      "Epoch 76/100\n",
      " - 36s - loss: 0.0291 - precision: 0.9866 - recall: 0.9866 - f1: 0.9866 - acc: 0.9866 - val_loss: 0.0067 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00746 to 0.00670, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 77/100\n",
      " - 36s - loss: 0.0265 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0043 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00670 to 0.00426, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 78/100\n",
      " - 36s - loss: 0.0264 - precision: 0.9914 - recall: 0.9914 - f1: 0.9914 - acc: 0.9914 - val_loss: 0.0077 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00426\n",
      "Epoch 79/100\n",
      " - 36s - loss: 0.0268 - precision: 0.9929 - recall: 0.9929 - f1: 0.9929 - acc: 0.9929 - val_loss: 0.0065 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00426\n",
      "Epoch 80/100\n",
      " - 35s - loss: 0.0332 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0047 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00426\n",
      "Epoch 81/100\n",
      " - 36s - loss: 0.0255 - precision: 0.9937 - recall: 0.9937 - f1: 0.9937 - acc: 0.9937 - val_loss: 0.0027 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00426 to 0.00270, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 82/100\n",
      " - 36s - loss: 0.0476 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0790 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00270\n",
      "Epoch 83/100\n",
      " - 35s - loss: 0.0583 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.2541 - val_precision: 0.9423 - val_recall: 0.9423 - val_f1: 0.9423 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00270\n",
      "Epoch 84/100\n",
      " - 36s - loss: 0.0480 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.3751 - val_precision: 0.8814 - val_recall: 0.8814 - val_f1: 0.8814 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00270\n",
      "Epoch 85/100\n",
      " - 35s - loss: 0.1105 - precision: 0.9560 - recall: 0.9560 - f1: 0.9560 - acc: 0.9560 - val_loss: 0.1065 - val_precision: 0.9615 - val_recall: 0.9615 - val_f1: 0.9615 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00270\n",
      "Epoch 86/100\n",
      " - 35s - loss: 0.0531 - precision: 0.9780 - recall: 0.9780 - f1: 0.9780 - acc: 0.9780 - val_loss: 0.0977 - val_precision: 0.9647 - val_recall: 0.9647 - val_f1: 0.9647 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00270\n",
      "Epoch 87/100\n",
      " - 35s - loss: 0.0558 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788 - acc: 0.9788 - val_loss: 0.2396 - val_precision: 0.9103 - val_recall: 0.9103 - val_f1: 0.9103 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00270\n",
      "Epoch 88/100\n",
      " - 35s - loss: 0.0528 - precision: 0.9819 - recall: 0.9819 - f1: 0.9819 - acc: 0.9819 - val_loss: 0.0565 - val_precision: 0.9744 - val_recall: 0.9744 - val_f1: 0.9744 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00270\n",
      "Epoch 89/100\n",
      " - 35s - loss: 0.0383 - precision: 0.9843 - recall: 0.9843 - f1: 0.9843 - acc: 0.9843 - val_loss: 0.0345 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00270\n",
      "Epoch 90/100\n",
      " - 35s - loss: 0.0283 - precision: 0.9906 - recall: 0.9906 - f1: 0.9906 - acc: 0.9906 - val_loss: 0.0274 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00270\n",
      "Epoch 91/100\n",
      " - 35s - loss: 0.0295 - precision: 0.9921 - recall: 0.9921 - f1: 0.9921 - acc: 0.9921 - val_loss: 0.0302 - val_precision: 0.9936 - val_recall: 0.9936 - val_f1: 0.9936 - val_acc: 0.9936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00270\n",
      "Epoch 92/100\n",
      " - 35s - loss: 0.0216 - precision: 0.9921 - recall: 0.9921 - f1: 0.9921 - acc: 0.9921 - val_loss: 0.0038 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00270\n",
      "Epoch 93/100\n",
      " - 35s - loss: 0.0210 - precision: 0.9921 - recall: 0.9921 - f1: 0.9921 - acc: 0.9921 - val_loss: 0.0077 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00270\n",
      "Epoch 94/100\n",
      " - 35s - loss: 0.0320 - precision: 0.9890 - recall: 0.9890 - f1: 0.9890 - acc: 0.9890 - val_loss: 0.0039 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00270\n",
      "Epoch 95/100\n",
      " - 35s - loss: 0.0161 - precision: 0.9929 - recall: 0.9929 - f1: 0.9929 - acc: 0.9929 - val_loss: 0.0057 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00270\n",
      "Epoch 96/100\n",
      " - 35s - loss: 0.0584 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803 - acc: 0.9803 - val_loss: 0.0084 - val_precision: 0.9968 - val_recall: 0.9968 - val_f1: 0.9968 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00270\n",
      "Epoch 97/100\n",
      " - 35s - loss: 0.0409 - precision: 0.9851 - recall: 0.9851 - f1: 0.9851 - acc: 0.9851 - val_loss: 0.0063 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00270\n",
      "Epoch 98/100\n",
      " - 35s - loss: 0.0325 - precision: 0.9882 - recall: 0.9882 - f1: 0.9882 - acc: 0.9882 - val_loss: 0.2928 - val_precision: 0.9295 - val_recall: 0.9295 - val_f1: 0.9295 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00270\n",
      "Epoch 99/100\n",
      " - 35s - loss: 0.0668 - precision: 0.9725 - recall: 0.9725 - f1: 0.9725 - acc: 0.9725 - val_loss: 3.9206 - val_precision: 0.4359 - val_recall: 0.4359 - val_f1: 0.4359 - val_acc: 0.4359\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00270\n",
      "Epoch 100/100\n",
      " - 35s - loss: 0.0553 - precision: 0.9772 - recall: 0.9772 - f1: 0.9772 - acc: 0.9772 - val_loss: 0.0276 - val_precision: 0.9904 - val_recall: 0.9904 - val_f1: 0.9904 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00270\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset.split_train_test(\"train\")[0])))):\n",
    "                                               \n",
    "    tg = BREAKHIST_DATASET(SHAPE, BATCH_SIZE, train_index, BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=True)\n",
    "    vg = BREAKHIST_DATASET(SHAPE, BATCH_SIZE, test_index , BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=False)\n",
    "        \n",
    "    schedule = SGDRScheduler(min_lr=1e-6,\n",
    "                             max_lr=1e-3,\n",
    "                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n",
    "                             lr_decay=0.9,\n",
    "                             cycle_length=10,\n",
    "                             mult_factor=2.)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[precision, recall, f1, 'acc'])\n",
    "\n",
    "    model_ckpt = \"BREAKHIST_FOLD_\"+str(ix)+\".h5\"\n",
    "    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n",
    "                 TensorBoard(log_dir='./log_'+str(ix), update_freq='batch'), \n",
    "                 schedule] \n",
    "                                               \n",
    "    model.fit_generator(tg.data_generator(),\n",
    "                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        validation_data=vg.data_generator(),\n",
    "                        validation_steps=len(test_index)//BATCH_SIZE,\n",
    "                        callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    gen = BREAKHIST_DATASET(SHAPE, BATCH_SIZE, range(1), BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=False).split_train_test(\"test\")\n",
    "                       \n",
    "    x = np.empty((len(gen[0]),)+SHAPE, dtype=np.float32)\n",
    "    y = np.empty((len(gen[1]), 2), dtype=np.float32)\n",
    "    \n",
    "    for ix, path in tqdm(enumerate(gen[0])):\n",
    "        img = np.array(Image.open(gen[0][ix]))\n",
    "        img = resize(img, SHAPE)\n",
    "\n",
    "        label = gen[1][ix]\n",
    "\n",
    "        x[ix] = img\n",
    "        y[ix] = label\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "403it [00:18, 21.75it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions with THRESH_VAL\n",
    "def threshold_arr(array):\n",
    "    # Get all value from array\n",
    "    # Compare calue with THRESH_VAL \n",
    "    # IF value >= THRESH_VAL. round to 1\n",
    "    # ELSE. round to 0\n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val)), dtype=np.float32))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09582688891913548, 0.9702233250620348, 0.9702233250620348, 0.9702232871990346, 0.9702233250620348]\n",
      "[0.10515641817642797, 0.9776674937965261, 0.9776674937965261, 0.9776674606664009, 0.9776674937965261]\n",
      "[0.06278485274658753, 0.9851116625310173, 0.9851116625310173, 0.9851116435995173, 0.9851116625310173]\n",
      "[0.0691400856385123, 0.9727047146401985, 0.9727047146401985, 0.9727046767771984, 0.9727047146401985]\n",
      "[0.086323945439899, 0.9727047146401985, 0.9727047146401985, 0.9727046815100734, 0.9727047146401985]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    model = load_model(\"BREAKHIST_FOLD_{}.h5\".format(i), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
    "    print(model.evaluate(x, y, verbose=0))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig(\"200X - confusion matrix - 3. FOLD.jpg\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9851116625310173, F1_Score: 0.9828267045454546, Precision: 0.978937728937729, Recall: 0.9870064752736233\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       126\n",
      "           1       1.00      0.98      0.99       277\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       403\n",
      "   macro avg       0.98      0.99      0.98       403\n",
      "weighted avg       0.99      0.99      0.99       403\n",
      " samples avg       0.99      0.99      0.99       403\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds = threshold_arr(models[2].predict(x, verbose=0))\n",
    "\n",
    "results = precision_recall_fscore_support(y, y_preds ,average='macro')\n",
    "acc = accuracy_score(y, y_preds)\n",
    "\n",
    "print(\"Accuracy: {}, F1_Score: {}, Precision: {}, Recall: {}\".format(acc, results[2], results[0], results[1]))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y, y_preds))\n",
    "print(\"\\n\")\n",
    "cnf_matrix = confusion_matrix(y.argmax(axis=1), y_preds.argmax(axis=1))\n",
    "\n",
    "plot_confusion_matrix(cm           = cnf_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['BENIGN', 'MALIGNANT'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]]\n",
      "777.6315212249756 ms\n",
      "***\n",
      "[[1. 0.]]\n",
      "9.024381637573242 ms\n",
      "***\n",
      "[[1. 0.]]\n",
      "8.522510528564453 ms\n",
      "***\n",
      "[[1. 0.]]\n",
      "8.522748947143555 ms\n",
      "***\n",
      "[[1. 0.]]\n",
      "8.051633834838867 ms\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(5): \n",
    "    img = np.array(Image.open(BASE_DIR+\"benign/SOB/adenosis/SOB_B_A_14-22549AB/40X/SOB_B_A-14-22549AB-40-009.png\"))\n",
    "    x = resize(img, SHAPE)\n",
    "    x = x.reshape((1,) + x.shape) \n",
    "    start = time.time()\n",
    "    prediction = models[3].predict(x, batch_size=1)\n",
    "    finish = time.time()\n",
    "    print(threshold_arr(prediction))\n",
    "    print((finish-start)*1000,\"ms\")\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BREAK_HIST_GANGSTERS_v1.0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
