{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QaFrvOWeGWoE",
    "outputId": "adc13e31-0656-4629-8a71-b7e76940fdab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from albumentations import *\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY4ffYPhGdIE"
   },
   "outputs": [],
   "source": [
    "SHAPE = (224, 224, 3)\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 100\n",
    "N_SPLITS = 5\n",
    "SEED = 1881\n",
    "TRAIN_TEST_RATIO = 0.2\n",
    "\n",
    "BASE_DIR     = \"../data/BreaKHis_v1/histology_slides/breast/\"\n",
    "DATASET_MODE = \"400X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "futhEMcZhXDy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGT_8VyTZt2T"
   },
   "outputs": [],
   "source": [
    "class BREAKHIST_DATASET:\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape           --> TUPLE.wanted image size\n",
    "    batch_size            --> INT.yielding data size for every iteration\n",
    "    orders                --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n",
    "    base_dir              --> STR.the DIR which is include \"benign\" and \"malignant\" dirs.\n",
    "    dataset_mode          --> STR. Which type of images will be used: \"40X\", \"100X\", \"200X\", \"400X\".\n",
    "    seed                  --> INT. This allow to dataset generator to more reproduciable and it ensures that x and y are shuffled with compatible.\n",
    "    augment               --> BOOL. Augment data or not.\n",
    "    train_test_ratio      --> How much of data will be used as test set.\n",
    "    ---------\n",
    "    GENERAL_CLASSES       --> LIST.[\"benign\", \"malignant\"]\n",
    "    BENIGN_SUB_CLASSES    --> LIST.[\"adenosis\", \"fibroadenoma\", \"phyllodes_tumor\", \"tubular_adenoma\"]\n",
    "    MALIGNANT_SUB_CLASSES --> LIST.[\"ductal_carcinoma\", \"lobular_carcinoma\", \"mucinous_carcinoma\", \"papillary_carcinoma\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, batch_size, orders, base_dir, dataset_mode, seed, train_test_ratio, augment=True):\n",
    "        self.SHAPE                 = input_shape\n",
    "        self.BATCH_SIZE            = batch_size\n",
    "        self.arr                   = orders\n",
    "        self.DATASET_MODE          = dataset_mode\n",
    "        self.SEED                  = seed\n",
    "        self.TT_RATIO              = train_test_ratio\n",
    "        self.AUG                   = augment\n",
    "        \n",
    "        self.BASE_DIR              = base_dir\n",
    "        self.GENERAL_CLASSES       = [\"benign\", \"malignant\"]\n",
    "        self.BENIGN_SUB_CLASSES    = [\"adenosis\", \"fibroadenoma\", \"phyllodes_tumor\", \"tubular_adenoma\"]\n",
    "        self.MALIGNANT_SUB_CLASSES = [\"ductal_carcinoma\", \"lobular_carcinoma\", \"mucinous_carcinoma\", \"papillary_carcinoma\"]\n",
    "        \n",
    "        \n",
    "    def get_paths_n_labels(self):\n",
    "\n",
    "        x      = []\n",
    "        label = []\n",
    "\n",
    "        for ix1, a in enumerate(self.GENERAL_CLASSES):\n",
    "            if ix1 == 0:\n",
    "                for ix2, b in enumerate(self.BENIGN_SUB_CLASSES):\n",
    "                    path1 = self.BASE_DIR+a+\"/SOB/\"+b\n",
    "                    for c in os.listdir(path1):\n",
    "                        path2 = path1+\"/\"+c+\"/\"+self.DATASET_MODE\n",
    "                        for img_name in os.listdir(path2):\n",
    "                            path3 = path2+\"/\"+img_name\n",
    "\n",
    "                            # x\n",
    "                            img_path = path3 #np.array(Image.open(path3), dtype=np.float16)\n",
    "\n",
    "                            # y\n",
    "                            main_targets = np.zeros((2), dtype=np.float32) # BENIGN OR MALIGNANT\n",
    "                            main_targets[ix1] = 1.\n",
    "\n",
    "                            # Store the values\n",
    "                            x.append(img_path)\n",
    "                            label.append(main_targets)\n",
    "\n",
    "                            \n",
    "            if ix1 == 1:\n",
    "                for ix2, b in enumerate(self.MALIGNANT_SUB_CLASSES):\n",
    "                    path1 = self.BASE_DIR+a+\"/SOB/\"+b\n",
    "                    for c in os.listdir(path1):\n",
    "                        path2 = path1+\"/\"+c+\"/\"+self.DATASET_MODE\n",
    "                        for img_name in os.listdir(path2):\n",
    "                            path3 = path2+\"/\"+img_name\n",
    "\n",
    "                            # x\n",
    "                            img_path = path3  #np.array(Image.open(path3), dtype=np.float16)\n",
    "\n",
    "                            # y\n",
    "                            main_targets = np.zeros((2), dtype=np.float32) # BENIGN OR MALIGNANT\n",
    "                            main_targets[ix1] = 1.\n",
    "                     \n",
    "                            # Store the values\n",
    "                            x.append(img_path)\n",
    "                            label.append(main_targets)\n",
    "                           \n",
    "        return x, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.get_paths_n_labels()[0])\n",
    "    \n",
    "    def get_img(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        return np.array(img)\n",
    "    \n",
    "    def augmenting(self, img):\n",
    "        if self.AUG:\n",
    "            augment = Compose([VerticalFlip(p=0.5),\n",
    "                               HorizontalFlip(p=0.5),\n",
    "                               RandomBrightnessContrast(p=0.3),\n",
    "                               ShiftScaleRotate(p=0.5, shift_limit=0.2, scale_limit=0.2, rotate_limit=20)])  \n",
    "        else:\n",
    "            augment = Compose([])  \n",
    "\n",
    "        img = augment(image=img)['image']\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def resize_and_normalize(self, img):\n",
    "        img = resize(img, self.SHAPE)\n",
    "        return img\n",
    "    \n",
    "    def get_shuffled_data(self):\n",
    "        img_paths, labels = self.get_paths_n_labels()\n",
    "\n",
    "        np.random.seed(self.SEED) \n",
    "        np.random.shuffle(img_paths)\n",
    "        \n",
    "        np.random.seed(self.SEED) \n",
    "        np.random.shuffle(labels)\n",
    "        \n",
    "        return img_paths, labels\n",
    "        \n",
    "    def split_train_test(self, get):  # get=={\"train\",\"test\"}\n",
    "        img_paths, labels = self.get_shuffled_data()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(img_paths, labels, test_size=self.TT_RATIO, random_state=self.SEED)\n",
    "        \n",
    "        if get=='train':\n",
    "            return x_train, y_train\n",
    "        \n",
    "        elif get=='test':\n",
    "            return x_test, y_test\n",
    "    \n",
    "    def data_generator(self):\n",
    "        img_paths, labels = self.split_train_test(get=\"train\")\n",
    "        \n",
    "        while True:\n",
    "            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float32)\n",
    "            y = np.empty((self.BATCH_SIZE, 2), dtype=np.float32)\n",
    "\n",
    "            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n",
    "\n",
    "            for ix, id_ in enumerate(batch):\n",
    "                # x\n",
    "                img_path = img_paths[id_]\n",
    "                img = self.get_img(img_path)\n",
    "                img = self.augmenting(img)\n",
    "                img = self.resize_and_normalize(img)\n",
    "                  \n",
    "                # y \n",
    "                label = labels[id_]\n",
    "             \n",
    "                # Store the values    \n",
    "                x[ix] = img\n",
    "                y[ix] = label\n",
    "\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1223
    },
    "colab_type": "code",
    "id": "bJ-aVohsqt1F",
    "outputId": "5514493d-cfc0-47bc-c4db-7b5d3de32331",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.53690475 0.43801427 0.5917936 ]\n",
      "   [0.5133469  0.3871214  0.5706605 ]\n",
      "   [0.5065848  0.37717307 0.56032693]\n",
      "   ...\n",
      "   [0.8250657  0.48781076 0.67996764]\n",
      "   [0.82244617 0.4788187  0.6677893 ]\n",
      "   [0.7935793  0.4938244  0.63647145]]\n",
      "\n",
      "  [[0.5842503  0.48915222 0.6315542 ]\n",
      "   [0.5559458  0.42920825 0.6028711 ]\n",
      "   [0.53566617 0.39603248 0.5837535 ]\n",
      "   ...\n",
      "   [0.82877934 0.48760286 0.6876029 ]\n",
      "   [0.82990193 0.48480392 0.6745098 ]\n",
      "   [0.7906578  0.49090293 0.63355   ]]\n",
      "\n",
      "  [[0.6065323  0.5063069  0.6511949 ]\n",
      "   [0.5699908  0.44057906 0.61938244]\n",
      "   [0.54656863 0.40539217 0.59362745]\n",
      "   ...\n",
      "   [0.83203125 0.4869332  0.68301165]\n",
      "   [0.8241115  0.47901347 0.6727591 ]\n",
      "   [0.7797772  0.48786545 0.63425463]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.4242647  0.3257353  0.47916666]\n",
      "   [0.46049106 0.3360535  0.50980395]\n",
      "   [0.47923452 0.33805802 0.52629334]\n",
      "   ...\n",
      "   [0.6247155  0.42334998 0.60003066]\n",
      "   [0.6467546  0.520481   0.6744507 ]\n",
      "   [0.80249476 0.76175815 0.84661674]]\n",
      "\n",
      "  [[0.4276589  0.32892376 0.48634017]\n",
      "   [0.46132267 0.33828345 0.5115678 ]\n",
      "   [0.49184173 0.35066527 0.53890055]\n",
      "   ...\n",
      "   [0.6645177  0.42887342 0.6222426 ]\n",
      "   [0.6302346  0.490439   0.650291  ]\n",
      "   [0.7991706  0.74938285 0.82907915]]\n",
      "\n",
      "  [[0.43957895 0.33297443 0.49449405]\n",
      "   [0.48956802 0.353427   0.5399838 ]\n",
      "   [0.50223655 0.35321692 0.5503764 ]\n",
      "   ...\n",
      "   [0.6797269  0.43869486 0.63972557]\n",
      "   [0.61272323 0.4615262  0.61689204]\n",
      "   [0.78967303 0.7257966  0.80576634]]]]\n",
      "(1, 224, 224, 3)\n",
      "----------\n",
      "[[1. 0.]]\n",
      "(1, 2)\n",
      "----------\n",
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = BREAKHIST_DATASET(SHAPE, 1, range(4), BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=True)\n",
    "\n",
    "for ix, data in enumerate(dataset.data_generator()):\n",
    "    img, y = data\n",
    "    print(img)\n",
    "    print(img.shape)\n",
    "    print(\"-\"*10)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"-\"*10)\n",
    "    print(img[0,:,:,:].shape)\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.show()\n",
    "    \n",
    "    if ix==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZygcwwp0Sry"
   },
   "outputs": [],
   "source": [
    "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    \n",
    "    Only computes a batch-wise average of recall.\n",
    "    \n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    \n",
    "    Only computes a batch-wise average of precision.\n",
    "    \n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisionx = precision(y_true, y_pred)\n",
    "    recallx = recall(y_true, y_pred)\n",
    "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBbVuHAfgHnh"
   },
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvvEuAAKfkV5"
   },
   "outputs": [],
   "source": [
    "# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "    \n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature._keras_shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature._keras_shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool._keras_shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool._keras_shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat._keras_shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature._keras_shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4Ndx2vm6NZ3"
   },
   "outputs": [],
   "source": [
    "# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    shortcut = y\n",
    "\n",
    "    # down-sampling is performed with a stride of 2\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 11 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    y = add([shortcut, y])\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVUWz9lzfm6Y"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    dropRate = 0.3\n",
    "    \n",
    "    init = Input(SHAPE)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(2, activation='softmax')(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3001
    },
    "colab_type": "code",
    "id": "w2V3AUW7fm-n",
    "outputId": "d7c45fcc-739a-4166-9a30-d65352088c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 224, 224, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 224, 224, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 112, 112, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 64)     0           global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1, 8)      520         reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 1, 64)     576         dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 1, 64)     0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 1, 64)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 112, 112, 64) 0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 112, 112, 2)  0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 112, 112, 1)  98          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 112, 112, 64) 0           multiply_1[0][0]                 \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 112, 112, 64) 36928       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 112, 112, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 112, 112, 64) 36928       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 112, 112, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 112, 112, 64) 0           multiply_2[0][0]                 \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 64) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 56, 56, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_average_pooling2d_2 (Glo (None, 128)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 128)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 128)    0           global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 1, 16)     2064        reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1, 128)    2176        dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 1, 128)    0           dense_4[0][0]                    \n",
      "                                                                 dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1, 1, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 56, 56, 128)  0           activation_5[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 56, 56, 2)    0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 56, 56, 1)    98          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 56, 56, 128)  0           multiply_3[0][0]                 \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 56, 56, 128)  147584      multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 56, 56, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 56, 56, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 56, 56, 128)  0           multiply_4[0][0]                 \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 224, 224, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 224, 224, 64) 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 224, 224, 224 0           up_sampling2d_1[0][0]            \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 224)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          57600       global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            514         activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 605,566\n",
      "Trainable params: 603,262\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "schfFpcIfzZy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5584
    },
    "colab_type": "code",
    "id": "MSHMA1gtMQ6e",
    "outputId": "61dadec3-f4b4-4877-970a-c66700175ab5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 64s - loss: 0.4791 - precision: 0.7951 - recall: 0.7951 - f1: 0.7951 - acc: 0.7951 - val_loss: 0.7578 - val_precision: 0.7569 - val_recall: 0.7569 - val_f1: 0.7569 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75776, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 2/100\n",
      " - 42s - loss: 0.4176 - precision: 0.8281 - recall: 0.8281 - f1: 0.8281 - acc: 0.8281 - val_loss: 0.3840 - val_precision: 0.8472 - val_recall: 0.8472 - val_f1: 0.8472 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.75776 to 0.38398, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 3/100\n",
      " - 37s - loss: 0.4081 - precision: 0.8316 - recall: 0.8316 - f1: 0.8316 - acc: 0.8316 - val_loss: 0.8492 - val_precision: 0.5972 - val_recall: 0.5972 - val_f1: 0.5972 - val_acc: 0.5972\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38398\n",
      "Epoch 4/100\n",
      " - 36s - loss: 0.3771 - precision: 0.8490 - recall: 0.8490 - f1: 0.8490 - acc: 0.8490 - val_loss: 0.9464 - val_precision: 0.6597 - val_recall: 0.6597 - val_f1: 0.6597 - val_acc: 0.6597\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38398\n",
      "Epoch 5/100\n",
      " - 35s - loss: 0.3862 - precision: 0.8385 - recall: 0.8385 - f1: 0.8385 - acc: 0.8385 - val_loss: 1.2614 - val_precision: 0.6771 - val_recall: 0.6771 - val_f1: 0.6771 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38398\n",
      "Epoch 6/100\n",
      " - 35s - loss: 0.3522 - precision: 0.8568 - recall: 0.8568 - f1: 0.8568 - acc: 0.8568 - val_loss: 0.6750 - val_precision: 0.7118 - val_recall: 0.7118 - val_f1: 0.7118 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38398\n",
      "Epoch 7/100\n",
      " - 34s - loss: 0.4069 - precision: 0.8411 - recall: 0.8411 - f1: 0.8411 - acc: 0.8411 - val_loss: 0.5172 - val_precision: 0.7465 - val_recall: 0.7465 - val_f1: 0.7465 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38398\n",
      "Epoch 8/100\n",
      " - 35s - loss: 0.3444 - precision: 0.8559 - recall: 0.8559 - f1: 0.8559 - acc: 0.8559 - val_loss: 0.3920 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38398\n",
      "Epoch 9/100\n",
      " - 35s - loss: 0.3988 - precision: 0.8359 - recall: 0.8359 - f1: 0.8359 - acc: 0.8359 - val_loss: 1.7200 - val_precision: 0.5486 - val_recall: 0.5486 - val_f1: 0.5486 - val_acc: 0.5486\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38398\n",
      "Epoch 10/100\n",
      " - 34s - loss: 0.3368 - precision: 0.8672 - recall: 0.8672 - f1: 0.8672 - acc: 0.8672 - val_loss: 1.6364 - val_precision: 0.5382 - val_recall: 0.5382 - val_f1: 0.5382 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38398\n",
      "Epoch 11/100\n",
      " - 35s - loss: 0.3444 - precision: 0.8620 - recall: 0.8620 - f1: 0.8620 - acc: 0.8620 - val_loss: 0.3511 - val_precision: 0.8646 - val_recall: 0.8646 - val_f1: 0.8646 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.38398 to 0.35105, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 12/100\n",
      " - 33s - loss: 0.3103 - precision: 0.8568 - recall: 0.8568 - f1: 0.8568 - acc: 0.8568 - val_loss: 0.3047 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35105 to 0.30467, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 13/100\n",
      " - 33s - loss: 0.3014 - precision: 0.8819 - recall: 0.8819 - f1: 0.8819 - acc: 0.8819 - val_loss: 0.4440 - val_precision: 0.8299 - val_recall: 0.8299 - val_f1: 0.8299 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.30467\n",
      "Epoch 14/100\n",
      " - 32s - loss: 0.3126 - precision: 0.8724 - recall: 0.8724 - f1: 0.8724 - acc: 0.8724 - val_loss: 0.7988 - val_precision: 0.6285 - val_recall: 0.6285 - val_f1: 0.6285 - val_acc: 0.6285\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.30467\n",
      "Epoch 15/100\n",
      " - 32s - loss: 0.3157 - precision: 0.8628 - recall: 0.8628 - f1: 0.8628 - acc: 0.8628 - val_loss: 1.5778 - val_precision: 0.6979 - val_recall: 0.6979 - val_f1: 0.6979 - val_acc: 0.6979\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.30467\n",
      "Epoch 16/100\n",
      " - 32s - loss: 0.2915 - precision: 0.8837 - recall: 0.8837 - f1: 0.8837 - acc: 0.8837 - val_loss: 0.4897 - val_precision: 0.8229 - val_recall: 0.8229 - val_f1: 0.8229 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.30467\n",
      "Epoch 17/100\n",
      " - 32s - loss: 0.2875 - precision: 0.8898 - recall: 0.8898 - f1: 0.8898 - acc: 0.8898 - val_loss: 0.5070 - val_precision: 0.7882 - val_recall: 0.7882 - val_f1: 0.7882 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.30467\n",
      "Epoch 18/100\n",
      " - 32s - loss: 0.2904 - precision: 0.8767 - recall: 0.8767 - f1: 0.8767 - acc: 0.8767 - val_loss: 1.6837 - val_precision: 0.6424 - val_recall: 0.6424 - val_f1: 0.6424 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30467\n",
      "Epoch 19/100\n",
      " - 32s - loss: 0.3073 - precision: 0.8715 - recall: 0.8715 - f1: 0.8715 - acc: 0.8715 - val_loss: 0.5457 - val_precision: 0.6944 - val_recall: 0.6944 - val_f1: 0.6944 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30467\n",
      "Epoch 20/100\n",
      " - 33s - loss: 0.3028 - precision: 0.8715 - recall: 0.8715 - f1: 0.8715 - acc: 0.8715 - val_loss: 0.3017 - val_precision: 0.8611 - val_recall: 0.8611 - val_f1: 0.8611 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.30467 to 0.30172, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 21/100\n",
      " - 33s - loss: 0.2908 - precision: 0.8854 - recall: 0.8854 - f1: 0.8854 - acc: 0.8854 - val_loss: 0.2972 - val_precision: 0.8889 - val_recall: 0.8889 - val_f1: 0.8889 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30172 to 0.29722, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 22/100\n",
      " - 32s - loss: 0.2499 - precision: 0.9002 - recall: 0.9002 - f1: 0.9002 - acc: 0.9002 - val_loss: 0.7698 - val_precision: 0.6424 - val_recall: 0.6424 - val_f1: 0.6424 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29722\n",
      "Epoch 23/100\n",
      " - 32s - loss: 0.3020 - precision: 0.8802 - recall: 0.8802 - f1: 0.8802 - acc: 0.8802 - val_loss: 1.1875 - val_precision: 0.6319 - val_recall: 0.6319 - val_f1: 0.6319 - val_acc: 0.6319\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.29722\n",
      "Epoch 24/100\n",
      " - 32s - loss: 0.2634 - precision: 0.9002 - recall: 0.9002 - f1: 0.9002 - acc: 0.9002 - val_loss: 0.3974 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.29722\n",
      "Epoch 25/100\n",
      " - 32s - loss: 0.2770 - precision: 0.8880 - recall: 0.8880 - f1: 0.8880 - acc: 0.8880 - val_loss: 0.3006 - val_precision: 0.8681 - val_recall: 0.8681 - val_f1: 0.8681 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.29722\n",
      "Epoch 26/100\n",
      " - 32s - loss: 0.2444 - precision: 0.9036 - recall: 0.9036 - f1: 0.9036 - acc: 0.9036 - val_loss: 0.9848 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.29722\n",
      "Epoch 27/100\n",
      " - 32s - loss: 0.2686 - precision: 0.8872 - recall: 0.8872 - f1: 0.8872 - acc: 0.8872 - val_loss: 0.9865 - val_precision: 0.4896 - val_recall: 0.4896 - val_f1: 0.4896 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.29722\n",
      "Epoch 28/100\n",
      " - 32s - loss: 0.2571 - precision: 0.9002 - recall: 0.9002 - f1: 0.9002 - acc: 0.9002 - val_loss: 0.2948 - val_precision: 0.8889 - val_recall: 0.8889 - val_f1: 0.8889 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.29722 to 0.29475, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 29/100\n",
      " - 32s - loss: 0.2511 - precision: 0.8976 - recall: 0.8976 - f1: 0.8976 - acc: 0.8976 - val_loss: 0.2201 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.29475 to 0.22009, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 30/100\n",
      " - 32s - loss: 0.2470 - precision: 0.8993 - recall: 0.8993 - f1: 0.8993 - acc: 0.8993 - val_loss: 0.4208 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.22009\n",
      "Epoch 31/100\n",
      " - 32s - loss: 0.2307 - precision: 0.9097 - recall: 0.9097 - f1: 0.9097 - acc: 0.9097 - val_loss: 0.3385 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.22009\n",
      "Epoch 32/100\n",
      " - 32s - loss: 0.2294 - precision: 0.9010 - recall: 0.9010 - f1: 0.9010 - acc: 0.9010 - val_loss: 0.3958 - val_precision: 0.8681 - val_recall: 0.8681 - val_f1: 0.8681 - val_acc: 0.8681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_loss did not improve from 0.22009\n",
      "Epoch 33/100\n",
      " - 32s - loss: 0.2130 - precision: 0.9106 - recall: 0.9106 - f1: 0.9106 - acc: 0.9106 - val_loss: 0.7283 - val_precision: 0.7882 - val_recall: 0.7882 - val_f1: 0.7882 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.22009\n",
      "Epoch 34/100\n",
      " - 32s - loss: 0.2046 - precision: 0.9175 - recall: 0.9175 - f1: 0.9175 - acc: 0.9175 - val_loss: 0.3404 - val_precision: 0.8403 - val_recall: 0.8403 - val_f1: 0.8403 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.22009\n",
      "Epoch 35/100\n",
      " - 32s - loss: 0.1941 - precision: 0.9245 - recall: 0.9245 - f1: 0.9245 - acc: 0.9245 - val_loss: 0.2696 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.22009\n",
      "Epoch 36/100\n",
      " - 32s - loss: 0.2179 - precision: 0.9054 - recall: 0.9054 - f1: 0.9054 - acc: 0.9054 - val_loss: 0.3723 - val_precision: 0.8472 - val_recall: 0.8472 - val_f1: 0.8472 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.22009\n",
      "Epoch 37/100\n",
      " - 32s - loss: 0.2188 - precision: 0.9115 - recall: 0.9115 - f1: 0.9115 - acc: 0.9115 - val_loss: 0.2236 - val_precision: 0.9097 - val_recall: 0.9097 - val_f1: 0.9097 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.22009\n",
      "Epoch 38/100\n",
      " - 32s - loss: 0.2522 - precision: 0.8993 - recall: 0.8993 - f1: 0.8993 - acc: 0.8993 - val_loss: 0.8501 - val_precision: 0.6944 - val_recall: 0.6944 - val_f1: 0.6944 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.22009\n",
      "Epoch 39/100\n",
      " - 32s - loss: 0.2333 - precision: 0.9080 - recall: 0.9080 - f1: 0.9080 - acc: 0.9080 - val_loss: 3.7968 - val_precision: 0.6944 - val_recall: 0.6944 - val_f1: 0.6944 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.22009\n",
      "Epoch 40/100\n",
      " - 32s - loss: 0.2914 - precision: 0.8741 - recall: 0.8741 - f1: 0.8741 - acc: 0.8741 - val_loss: 0.6236 - val_precision: 0.8368 - val_recall: 0.8368 - val_f1: 0.8368 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22009\n",
      "Epoch 41/100\n",
      " - 32s - loss: 0.2503 - precision: 0.9071 - recall: 0.9071 - f1: 0.9071 - acc: 0.9071 - val_loss: 0.4072 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22009\n",
      "Epoch 42/100\n",
      " - 32s - loss: 0.2073 - precision: 0.9210 - recall: 0.9210 - f1: 0.9210 - acc: 0.9210 - val_loss: 0.3618 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22009\n",
      "Epoch 43/100\n",
      " - 32s - loss: 0.1833 - precision: 0.9349 - recall: 0.9349 - f1: 0.9349 - acc: 0.9349 - val_loss: 0.2171 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.22009 to 0.21715, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 44/100\n",
      " - 32s - loss: 0.1975 - precision: 0.9253 - recall: 0.9253 - f1: 0.9253 - acc: 0.9253 - val_loss: 0.2941 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.21715\n",
      "Epoch 45/100\n",
      " - 32s - loss: 0.2226 - precision: 0.9123 - recall: 0.9123 - f1: 0.9123 - acc: 0.9123 - val_loss: 0.3525 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.21715\n",
      "Epoch 46/100\n",
      " - 32s - loss: 0.2527 - precision: 0.8976 - recall: 0.8976 - f1: 0.8976 - acc: 0.8976 - val_loss: 0.3449 - val_precision: 0.8403 - val_recall: 0.8403 - val_f1: 0.8403 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21715\n",
      "Epoch 47/100\n",
      " - 32s - loss: 0.2358 - precision: 0.9106 - recall: 0.9106 - f1: 0.9106 - acc: 0.9106 - val_loss: 0.6487 - val_precision: 0.6111 - val_recall: 0.6111 - val_f1: 0.6111 - val_acc: 0.6111\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21715\n",
      "Epoch 48/100\n",
      " - 32s - loss: 0.2554 - precision: 0.8950 - recall: 0.8950 - f1: 0.8950 - acc: 0.8950 - val_loss: 0.3490 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21715\n",
      "Epoch 49/100\n",
      " - 32s - loss: 0.1947 - precision: 0.9236 - recall: 0.9236 - f1: 0.9236 - acc: 0.9236 - val_loss: 0.2317 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21715\n",
      "Epoch 50/100\n",
      " - 32s - loss: 0.1608 - precision: 0.9340 - recall: 0.9340 - f1: 0.9340 - acc: 0.9340 - val_loss: 0.2996 - val_precision: 0.9028 - val_recall: 0.9028 - val_f1: 0.9028 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21715\n",
      "Epoch 51/100\n",
      " - 32s - loss: 0.1818 - precision: 0.9262 - recall: 0.9262 - f1: 0.9262 - acc: 0.9262 - val_loss: 0.1733 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.21715 to 0.17332, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 52/100\n",
      " - 32s - loss: 0.1699 - precision: 0.9280 - recall: 0.9280 - f1: 0.9280 - acc: 0.9280 - val_loss: 0.1794 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.17332\n",
      "Epoch 53/100\n",
      " - 32s - loss: 0.1534 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.2555 - val_precision: 0.9028 - val_recall: 0.9028 - val_f1: 0.9028 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.17332\n",
      "Epoch 54/100\n",
      " - 32s - loss: 0.2235 - precision: 0.9106 - recall: 0.9106 - f1: 0.9106 - acc: 0.9106 - val_loss: 0.6713 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.17332\n",
      "Epoch 55/100\n",
      " - 32s - loss: 0.2355 - precision: 0.9019 - recall: 0.9019 - f1: 0.9019 - acc: 0.9019 - val_loss: 1.6858 - val_precision: 0.6042 - val_recall: 0.6042 - val_f1: 0.6042 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.17332\n",
      "Epoch 56/100\n",
      " - 32s - loss: 0.2359 - precision: 0.9019 - recall: 0.9019 - f1: 0.9019 - acc: 0.9019 - val_loss: 1.0777 - val_precision: 0.6632 - val_recall: 0.6632 - val_f1: 0.6632 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.17332\n",
      "Epoch 57/100\n",
      " - 32s - loss: 0.2106 - precision: 0.9132 - recall: 0.9132 - f1: 0.9132 - acc: 0.9132 - val_loss: 0.3107 - val_precision: 0.8681 - val_recall: 0.8681 - val_f1: 0.8681 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.17332\n",
      "Epoch 58/100\n",
      " - 32s - loss: 0.1968 - precision: 0.9253 - recall: 0.9253 - f1: 0.9253 - acc: 0.9253 - val_loss: 0.2933 - val_precision: 0.8611 - val_recall: 0.8611 - val_f1: 0.8611 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.17332\n",
      "Epoch 59/100\n",
      " - 32s - loss: 0.1655 - precision: 0.9349 - recall: 0.9349 - f1: 0.9349 - acc: 0.9349 - val_loss: 0.1855 - val_precision: 0.9097 - val_recall: 0.9097 - val_f1: 0.9097 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.17332\n",
      "Epoch 60/100\n",
      " - 32s - loss: 0.1600 - precision: 0.9427 - recall: 0.9427 - f1: 0.9427 - acc: 0.9427 - val_loss: 0.1489 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.17332 to 0.14890, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 61/100\n",
      " - 33s - loss: 0.2244 - precision: 0.9132 - recall: 0.9132 - f1: 0.9132 - acc: 0.9132 - val_loss: 0.2799 - val_precision: 0.8785 - val_recall: 0.8785 - val_f1: 0.8785 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.14890\n",
      "Epoch 62/100\n",
      " - 33s - loss: 0.2039 - precision: 0.9245 - recall: 0.9245 - f1: 0.9245 - acc: 0.9245 - val_loss: 0.3031 - val_precision: 0.8403 - val_recall: 0.8403 - val_f1: 0.8403 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.14890\n",
      "Epoch 63/100\n",
      " - 32s - loss: 0.1806 - precision: 0.9280 - recall: 0.9280 - f1: 0.9280 - acc: 0.9280 - val_loss: 1.0006 - val_precision: 0.5694 - val_recall: 0.5694 - val_f1: 0.5694 - val_acc: 0.5694\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.14890\n",
      "Epoch 64/100\n",
      " - 32s - loss: 0.1997 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.6309 - val_precision: 0.7674 - val_recall: 0.7674 - val_f1: 0.7674 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.14890\n",
      "Epoch 65/100\n",
      " - 32s - loss: 0.2052 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.4617 - val_precision: 0.7847 - val_recall: 0.7847 - val_f1: 0.7847 - val_acc: 0.7847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_loss did not improve from 0.14890\n",
      "Epoch 66/100\n",
      " - 32s - loss: 0.1741 - precision: 0.9323 - recall: 0.9323 - f1: 0.9323 - acc: 0.9323 - val_loss: 0.1904 - val_precision: 0.9097 - val_recall: 0.9097 - val_f1: 0.9097 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.14890\n",
      "Epoch 67/100\n",
      " - 32s - loss: 0.1491 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.1715 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.14890\n",
      "Epoch 68/100\n",
      " - 33s - loss: 0.1236 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.1614 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.14890\n",
      "Epoch 69/100\n",
      " - 32s - loss: 0.1245 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.2308 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.14890\n",
      "Epoch 70/100\n",
      " - 32s - loss: 0.1773 - precision: 0.9306 - recall: 0.9306 - f1: 0.9306 - acc: 0.9306 - val_loss: 0.4474 - val_precision: 0.8576 - val_recall: 0.8576 - val_f1: 0.8576 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.14890\n",
      "Epoch 71/100\n",
      " - 32s - loss: 0.1796 - precision: 0.9271 - recall: 0.9271 - f1: 0.9271 - acc: 0.9271 - val_loss: 0.8323 - val_precision: 0.8299 - val_recall: 0.8299 - val_f1: 0.8299 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.14890\n",
      "Epoch 72/100\n",
      " - 32s - loss: 0.1633 - precision: 0.9410 - recall: 0.9410 - f1: 0.9410 - acc: 0.9410 - val_loss: 0.5040 - val_precision: 0.7882 - val_recall: 0.7882 - val_f1: 0.7882 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.14890\n",
      "Epoch 73/100\n",
      " - 32s - loss: 0.1835 - precision: 0.9262 - recall: 0.9262 - f1: 0.9262 - acc: 0.9262 - val_loss: 0.5222 - val_precision: 0.7674 - val_recall: 0.7674 - val_f1: 0.7674 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.14890\n",
      "Epoch 74/100\n",
      " - 32s - loss: 0.1665 - precision: 0.9375 - recall: 0.9375 - f1: 0.9375 - acc: 0.9375 - val_loss: 0.4801 - val_precision: 0.7812 - val_recall: 0.7812 - val_f1: 0.7812 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14890\n",
      "Epoch 75/100\n",
      " - 32s - loss: 0.1755 - precision: 0.9323 - recall: 0.9323 - f1: 0.9323 - acc: 0.9323 - val_loss: 0.5895 - val_precision: 0.7535 - val_recall: 0.7535 - val_f1: 0.7535 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.14890\n",
      "Epoch 76/100\n",
      " - 32s - loss: 0.1391 - precision: 0.9549 - recall: 0.9549 - f1: 0.9549 - acc: 0.9549 - val_loss: 0.2023 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.14890\n",
      "Epoch 77/100\n",
      " - 32s - loss: 0.1272 - precision: 0.9444 - recall: 0.9444 - f1: 0.9444 - acc: 0.9444 - val_loss: 0.2361 - val_precision: 0.9097 - val_recall: 0.9097 - val_f1: 0.9097 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.14890\n",
      "Epoch 78/100\n",
      " - 33s - loss: 0.1164 - precision: 0.9523 - recall: 0.9523 - f1: 0.9523 - acc: 0.9523 - val_loss: 0.1922 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.14890\n",
      "Epoch 79/100\n",
      " - 32s - loss: 0.1071 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.2197 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.14890\n",
      "Epoch 80/100\n",
      " - 32s - loss: 0.1069 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.1868 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.14890\n",
      "Epoch 81/100\n",
      " - 32s - loss: 0.1350 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 0.1784 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.14890\n",
      "Epoch 82/100\n",
      " - 32s - loss: 0.1424 - precision: 0.9470 - recall: 0.9470 - f1: 0.9470 - acc: 0.9470 - val_loss: 0.2345 - val_precision: 0.9063 - val_recall: 0.9063 - val_f1: 0.9062 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.14890\n",
      "Epoch 83/100\n",
      " - 32s - loss: 0.1272 - precision: 0.9479 - recall: 0.9479 - f1: 0.9479 - acc: 0.9479 - val_loss: 0.5564 - val_precision: 0.7396 - val_recall: 0.7396 - val_f1: 0.7396 - val_acc: 0.7396\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.14890\n",
      "Epoch 84/100\n",
      " - 33s - loss: 0.1765 - precision: 0.9280 - recall: 0.9280 - f1: 0.9280 - acc: 0.9280 - val_loss: 0.4454 - val_precision: 0.8715 - val_recall: 0.8715 - val_f1: 0.8715 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.14890\n",
      "Epoch 85/100\n",
      " - 32s - loss: 0.1792 - precision: 0.9227 - recall: 0.9227 - f1: 0.9227 - acc: 0.9227 - val_loss: 0.3598 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14890\n",
      "Epoch 86/100\n",
      " - 32s - loss: 0.1734 - precision: 0.9358 - recall: 0.9358 - f1: 0.9358 - acc: 0.9358 - val_loss: 0.2348 - val_precision: 0.8854 - val_recall: 0.8854 - val_f1: 0.8854 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14890\n",
      "Epoch 87/100\n",
      " - 32s - loss: 0.1592 - precision: 0.9384 - recall: 0.9384 - f1: 0.9384 - acc: 0.9384 - val_loss: 2.1698 - val_precision: 0.6806 - val_recall: 0.6806 - val_f1: 0.6806 - val_acc: 0.6806\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14890\n",
      "Epoch 88/100\n",
      " - 32s - loss: 0.1868 - precision: 0.9149 - recall: 0.9149 - f1: 0.9149 - acc: 0.9149 - val_loss: 0.7167 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14890\n",
      "Epoch 89/100\n",
      " - 32s - loss: 0.1838 - precision: 0.9314 - recall: 0.9314 - f1: 0.9314 - acc: 0.9314 - val_loss: 0.5849 - val_precision: 0.7153 - val_recall: 0.7153 - val_f1: 0.7153 - val_acc: 0.7153\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14890\n",
      "Epoch 90/100\n",
      " - 32s - loss: 0.1751 - precision: 0.9314 - recall: 0.9314 - f1: 0.9314 - acc: 0.9314 - val_loss: 0.8294 - val_precision: 0.5972 - val_recall: 0.5972 - val_f1: 0.5972 - val_acc: 0.5972\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14890\n",
      "Epoch 91/100\n",
      " - 32s - loss: 0.1160 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.4833 - val_precision: 0.8160 - val_recall: 0.8160 - val_f1: 0.8160 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.14890\n",
      "Epoch 92/100\n",
      " - 32s - loss: 0.1442 - precision: 0.9470 - recall: 0.9470 - f1: 0.9470 - acc: 0.9470 - val_loss: 0.3283 - val_precision: 0.8472 - val_recall: 0.8472 - val_f1: 0.8472 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14890\n",
      "Epoch 93/100\n",
      " - 32s - loss: 0.1474 - precision: 0.9462 - recall: 0.9462 - f1: 0.9462 - acc: 0.9462 - val_loss: 0.2961 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14890\n",
      "Epoch 94/100\n",
      " - 32s - loss: 0.1107 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.2105 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14890\n",
      "Epoch 95/100\n",
      " - 33s - loss: 0.1001 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.1641 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14890\n",
      "Epoch 96/100\n",
      " - 32s - loss: 0.1068 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 0.1640 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.14890\n",
      "Epoch 97/100\n",
      " - 32s - loss: 0.1265 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.1383 - val_precision: 0.9444 - val_recall: 0.9444 - val_f1: 0.9444 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.14890 to 0.13827, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 98/100\n",
      " - 32s - loss: 0.1216 - precision: 0.9505 - recall: 0.9505 - f1: 0.9505 - acc: 0.9505 - val_loss: 0.1157 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00098: val_loss improved from 0.13827 to 0.11574, saving model to BREAKHIST_FOLD_0.h5\n",
      "Epoch 99/100\n",
      " - 32s - loss: 0.1374 - precision: 0.9462 - recall: 0.9462 - f1: 0.9462 - acc: 0.9462 - val_loss: 0.5439 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.11574\n",
      "Epoch 100/100\n",
      " - 32s - loss: 0.1435 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.6865 - val_precision: 0.7569 - val_recall: 0.7569 - val_f1: 0.7569 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.11574\n",
      "Epoch 1/100\n",
      " - 34s - loss: 0.1998 - precision: 0.9262 - recall: 0.9262 - f1: 0.9262 - acc: 0.9262 - val_loss: 0.2040 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20399, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 2/100\n",
      " - 32s - loss: 0.2088 - precision: 0.9158 - recall: 0.9158 - f1: 0.9158 - acc: 0.9158 - val_loss: 0.2191 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.20399\n",
      "Epoch 3/100\n",
      " - 32s - loss: 0.2109 - precision: 0.9123 - recall: 0.9123 - f1: 0.9123 - acc: 0.9123 - val_loss: 0.4895 - val_precision: 0.7882 - val_recall: 0.7882 - val_f1: 0.7882 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.20399\n",
      "Epoch 4/100\n",
      " - 32s - loss: 0.1987 - precision: 0.9149 - recall: 0.9149 - f1: 0.9149 - acc: 0.9149 - val_loss: 0.3163 - val_precision: 0.8438 - val_recall: 0.8438 - val_f1: 0.8437 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.20399\n",
      "Epoch 5/100\n",
      " - 32s - loss: 0.2323 - precision: 0.9028 - recall: 0.9028 - f1: 0.9028 - acc: 0.9028 - val_loss: 0.6368 - val_precision: 0.7604 - val_recall: 0.7604 - val_f1: 0.7604 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.20399\n",
      "Epoch 6/100\n",
      " - 32s - loss: 0.2009 - precision: 0.9210 - recall: 0.9210 - f1: 0.9210 - acc: 0.9210 - val_loss: 0.3892 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.20399\n",
      "Epoch 7/100\n",
      " - 32s - loss: 0.1961 - precision: 0.9271 - recall: 0.9271 - f1: 0.9271 - acc: 0.9271 - val_loss: 0.3483 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.20399\n",
      "Epoch 8/100\n",
      " - 32s - loss: 0.1609 - precision: 0.9366 - recall: 0.9366 - f1: 0.9366 - acc: 0.9366 - val_loss: 0.5755 - val_precision: 0.8056 - val_recall: 0.8056 - val_f1: 0.8056 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20399\n",
      "Epoch 9/100\n",
      " - 32s - loss: 0.1945 - precision: 0.9288 - recall: 0.9288 - f1: 0.9288 - acc: 0.9288 - val_loss: 0.7102 - val_precision: 0.8056 - val_recall: 0.8056 - val_f1: 0.8056 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20399\n",
      "Epoch 10/100\n",
      " - 32s - loss: 0.1842 - precision: 0.9323 - recall: 0.9323 - f1: 0.9323 - acc: 0.9323 - val_loss: 0.7457 - val_precision: 0.7708 - val_recall: 0.7708 - val_f1: 0.7708 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20399\n",
      "Epoch 11/100\n",
      " - 32s - loss: 0.1638 - precision: 0.9306 - recall: 0.9306 - f1: 0.9306 - acc: 0.9306 - val_loss: 1.3476 - val_precision: 0.7951 - val_recall: 0.7951 - val_f1: 0.7951 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20399\n",
      "Epoch 12/100\n",
      " - 32s - loss: 0.1439 - precision: 0.9410 - recall: 0.9410 - f1: 0.9410 - acc: 0.9410 - val_loss: 0.1829 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20399 to 0.18288, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 13/100\n",
      " - 32s - loss: 0.1504 - precision: 0.9427 - recall: 0.9427 - f1: 0.9427 - acc: 0.9427 - val_loss: 0.3923 - val_precision: 0.8681 - val_recall: 0.8681 - val_f1: 0.8681 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.18288\n",
      "Epoch 14/100\n",
      " - 32s - loss: 0.1767 - precision: 0.9314 - recall: 0.9314 - f1: 0.9314 - acc: 0.9314 - val_loss: 2.6617 - val_precision: 0.3785 - val_recall: 0.3785 - val_f1: 0.3785 - val_acc: 0.3785\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18288\n",
      "Epoch 15/100\n",
      " - 32s - loss: 0.2688 - precision: 0.8906 - recall: 0.8906 - f1: 0.8906 - acc: 0.8906 - val_loss: 0.4079 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18288\n",
      "Epoch 16/100\n",
      " - 32s - loss: 0.1827 - precision: 0.9245 - recall: 0.9245 - f1: 0.9245 - acc: 0.9245 - val_loss: 0.2020 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18288\n",
      "Epoch 17/100\n",
      " - 32s - loss: 0.1605 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 0.2550 - val_precision: 0.8854 - val_recall: 0.8854 - val_f1: 0.8854 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18288\n",
      "Epoch 18/100\n",
      " - 32s - loss: 0.1654 - precision: 0.9349 - recall: 0.9349 - f1: 0.9349 - acc: 0.9349 - val_loss: 0.4578 - val_precision: 0.8299 - val_recall: 0.8299 - val_f1: 0.8299 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18288\n",
      "Epoch 19/100\n",
      " - 32s - loss: 0.2071 - precision: 0.9210 - recall: 0.9210 - f1: 0.9210 - acc: 0.9210 - val_loss: 1.2475 - val_precision: 0.7188 - val_recall: 0.7188 - val_f1: 0.7187 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18288\n",
      "Epoch 20/100\n",
      " - 32s - loss: 0.1983 - precision: 0.9219 - recall: 0.9219 - f1: 0.9219 - acc: 0.9219 - val_loss: 0.2050 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18288\n",
      "Epoch 21/100\n",
      " - 32s - loss: 0.1526 - precision: 0.9453 - recall: 0.9453 - f1: 0.9453 - acc: 0.9453 - val_loss: 0.1632 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.18288 to 0.16321, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 22/100\n",
      " - 31s - loss: 0.1403 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.8008 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.16321\n",
      "Epoch 23/100\n",
      " - 32s - loss: 0.1908 - precision: 0.9193 - recall: 0.9193 - f1: 0.9193 - acc: 0.9193 - val_loss: 0.4607 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.16321\n",
      "Epoch 24/100\n",
      " - 31s - loss: 0.1650 - precision: 0.9392 - recall: 0.9392 - f1: 0.9392 - acc: 0.9392 - val_loss: 0.2925 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.16321\n",
      "Epoch 25/100\n",
      " - 32s - loss: 0.1150 - precision: 0.9635 - recall: 0.9635 - f1: 0.9635 - acc: 0.9635 - val_loss: 0.1530 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.16321 to 0.15299, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 26/100\n",
      " - 31s - loss: 0.1494 - precision: 0.9470 - recall: 0.9470 - f1: 0.9470 - acc: 0.9470 - val_loss: 0.1836 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15299\n",
      "Epoch 27/100\n",
      " - 32s - loss: 0.1663 - precision: 0.9314 - recall: 0.9314 - f1: 0.9314 - acc: 0.9314 - val_loss: 0.3464 - val_precision: 0.8681 - val_recall: 0.8681 - val_f1: 0.8681 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15299\n",
      "Epoch 28/100\n",
      " - 32s - loss: 0.1402 - precision: 0.9453 - recall: 0.9453 - f1: 0.9453 - acc: 0.9453 - val_loss: 0.2897 - val_precision: 0.8715 - val_recall: 0.8715 - val_f1: 0.8715 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15299\n",
      "Epoch 29/100\n",
      " - 32s - loss: 0.1515 - precision: 0.9453 - recall: 0.9453 - f1: 0.9453 - acc: 0.9453 - val_loss: 0.1505 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.15299 to 0.15051, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 30/100\n",
      " - 32s - loss: 0.1109 - precision: 0.9540 - recall: 0.9540 - f1: 0.9540 - acc: 0.9540 - val_loss: 0.6368 - val_precision: 0.7951 - val_recall: 0.7951 - val_f1: 0.7951 - val_acc: 0.7951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15051\n",
      "Epoch 31/100\n",
      " - 32s - loss: 0.1616 - precision: 0.9288 - recall: 0.9288 - f1: 0.9288 - acc: 0.9288 - val_loss: 1.8526 - val_precision: 0.7639 - val_recall: 0.7639 - val_f1: 0.7639 - val_acc: 0.7639\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15051\n",
      "Epoch 32/100\n",
      " - 32s - loss: 0.1809 - precision: 0.9245 - recall: 0.9245 - f1: 0.9245 - acc: 0.9245 - val_loss: 0.1927 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15051\n",
      "Epoch 33/100\n",
      " - 32s - loss: 0.1320 - precision: 0.9462 - recall: 0.9462 - f1: 0.9462 - acc: 0.9462 - val_loss: 0.7257 - val_precision: 0.8368 - val_recall: 0.8368 - val_f1: 0.8368 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.15051\n",
      "Epoch 34/100\n",
      " - 32s - loss: 0.1197 - precision: 0.9627 - recall: 0.9627 - f1: 0.9627 - acc: 0.9627 - val_loss: 0.1599 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15051\n",
      "Epoch 35/100\n",
      " - 32s - loss: 0.1236 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 0.1027 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.15051 to 0.10271, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 36/100\n",
      " - 32s - loss: 0.1046 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.1331 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10271\n",
      "Epoch 37/100\n",
      " - 32s - loss: 0.1138 - precision: 0.9488 - recall: 0.9488 - f1: 0.9488 - acc: 0.9488 - val_loss: 0.2718 - val_precision: 0.8715 - val_recall: 0.8715 - val_f1: 0.8715 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10271\n",
      "Epoch 38/100\n",
      " - 32s - loss: 0.1539 - precision: 0.9323 - recall: 0.9323 - f1: 0.9323 - acc: 0.9323 - val_loss: 0.9333 - val_precision: 0.7396 - val_recall: 0.7396 - val_f1: 0.7396 - val_acc: 0.7396\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.10271\n",
      "Epoch 39/100\n",
      " - 32s - loss: 0.1984 - precision: 0.9184 - recall: 0.9184 - f1: 0.9184 - acc: 0.9184 - val_loss: 1.7009 - val_precision: 0.6736 - val_recall: 0.6736 - val_f1: 0.6736 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10271\n",
      "Epoch 40/100\n",
      " - 32s - loss: 0.1695 - precision: 0.9306 - recall: 0.9306 - f1: 0.9306 - acc: 0.9306 - val_loss: 0.3442 - val_precision: 0.8507 - val_recall: 0.8507 - val_f1: 0.8507 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10271\n",
      "Epoch 41/100\n",
      " - 32s - loss: 0.1461 - precision: 0.9470 - recall: 0.9470 - f1: 0.9470 - acc: 0.9470 - val_loss: 0.1698 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10271\n",
      "Epoch 42/100\n",
      " - 32s - loss: 0.1414 - precision: 0.9497 - recall: 0.9497 - f1: 0.9497 - acc: 0.9497 - val_loss: 0.1558 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.10271\n",
      "Epoch 43/100\n",
      " - 32s - loss: 0.1135 - precision: 0.9627 - recall: 0.9627 - f1: 0.9627 - acc: 0.9627 - val_loss: 0.1573 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.10271\n",
      "Epoch 44/100\n",
      " - 32s - loss: 0.1218 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.0970 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.10271 to 0.09705, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 45/100\n",
      " - 32s - loss: 0.1184 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.2300 - val_precision: 0.8993 - val_recall: 0.8993 - val_f1: 0.8993 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09705\n",
      "Epoch 46/100\n",
      " - 31s - loss: 0.1772 - precision: 0.9201 - recall: 0.9201 - f1: 0.9201 - acc: 0.9201 - val_loss: 0.7951 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09705\n",
      "Epoch 47/100\n",
      " - 32s - loss: 0.1371 - precision: 0.9418 - recall: 0.9418 - f1: 0.9418 - acc: 0.9418 - val_loss: 1.2643 - val_precision: 0.5104 - val_recall: 0.5104 - val_f1: 0.5104 - val_acc: 0.5104\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09705\n",
      "Epoch 48/100\n",
      " - 32s - loss: 0.1536 - precision: 0.9401 - recall: 0.9401 - f1: 0.9401 - acc: 0.9401 - val_loss: 0.4223 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09705\n",
      "Epoch 49/100\n",
      " - 32s - loss: 0.1495 - precision: 0.9401 - recall: 0.9401 - f1: 0.9401 - acc: 0.9401 - val_loss: 0.1980 - val_precision: 0.9097 - val_recall: 0.9097 - val_f1: 0.9097 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09705\n",
      "Epoch 50/100\n",
      " - 32s - loss: 0.1192 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1012 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09705\n",
      "Epoch 51/100\n",
      " - 32s - loss: 0.1063 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.0751 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09705 to 0.07508, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 52/100\n",
      " - 32s - loss: 0.1217 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.0961 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07508\n",
      "Epoch 53/100\n",
      " - 32s - loss: 0.1411 - precision: 0.9453 - recall: 0.9453 - f1: 0.9453 - acc: 0.9453 - val_loss: 0.2378 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07508\n",
      "Epoch 54/100\n",
      " - 32s - loss: 0.1102 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 1.3645 - val_precision: 0.6528 - val_recall: 0.6528 - val_f1: 0.6528 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07508\n",
      "Epoch 55/100\n",
      " - 32s - loss: 0.1501 - precision: 0.9427 - recall: 0.9427 - f1: 0.9427 - acc: 0.9427 - val_loss: 0.6707 - val_precision: 0.7569 - val_recall: 0.7569 - val_f1: 0.7569 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07508\n",
      "Epoch 56/100\n",
      " - 32s - loss: 0.1806 - precision: 0.9410 - recall: 0.9410 - f1: 0.9410 - acc: 0.9410 - val_loss: 0.4600 - val_precision: 0.8021 - val_recall: 0.8021 - val_f1: 0.8021 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07508\n",
      "Epoch 57/100\n",
      " - 32s - loss: 0.1652 - precision: 0.9366 - recall: 0.9366 - f1: 0.9366 - acc: 0.9366 - val_loss: 0.2400 - val_precision: 0.8611 - val_recall: 0.8611 - val_f1: 0.8611 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07508\n",
      "Epoch 58/100\n",
      " - 32s - loss: 0.0974 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1963 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07508\n",
      "Epoch 59/100\n",
      " - 32s - loss: 0.1125 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.0979 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07508\n",
      "Epoch 60/100\n",
      " - 32s - loss: 0.1105 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.0884 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07508\n",
      "Epoch 61/100\n",
      " - 32s - loss: 0.1042 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1576 - val_precision: 0.9444 - val_recall: 0.9444 - val_f1: 0.9444 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07508\n",
      "Epoch 62/100\n",
      " - 32s - loss: 0.1018 - precision: 0.9609 - recall: 0.9609 - f1: 0.9609 - acc: 0.9609 - val_loss: 0.1427 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07508\n",
      "Epoch 63/100\n",
      " - 32s - loss: 0.1695 - precision: 0.9349 - recall: 0.9349 - f1: 0.9349 - acc: 0.9349 - val_loss: 0.3832 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07508\n",
      "Epoch 64/100\n",
      " - 32s - loss: 0.1514 - precision: 0.9427 - recall: 0.9427 - f1: 0.9427 - acc: 0.9427 - val_loss: 0.2659 - val_precision: 0.8993 - val_recall: 0.8993 - val_f1: 0.8993 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07508\n",
      "Epoch 65/100\n",
      " - 32s - loss: 0.1391 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.1643 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07508\n",
      "Epoch 66/100\n",
      " - 32s - loss: 0.1219 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.1561 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07508\n",
      "Epoch 67/100\n",
      " - 32s - loss: 0.0905 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1292 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07508\n",
      "Epoch 68/100\n",
      " - 32s - loss: 0.0893 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.0888 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07508\n",
      "Epoch 69/100\n",
      " - 32s - loss: 0.1118 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.1017 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07508\n",
      "Epoch 70/100\n",
      " - 32s - loss: 0.1164 - precision: 0.9488 - recall: 0.9488 - f1: 0.9488 - acc: 0.9488 - val_loss: 0.6128 - val_precision: 0.7882 - val_recall: 0.7882 - val_f1: 0.7882 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07508\n",
      "Epoch 71/100\n",
      " - 32s - loss: 0.0913 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.2276 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07508\n",
      "Epoch 72/100\n",
      " - 32s - loss: 0.1350 - precision: 0.9540 - recall: 0.9540 - f1: 0.9540 - acc: 0.9540 - val_loss: 0.1770 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07508\n",
      "Epoch 73/100\n",
      " - 32s - loss: 0.1427 - precision: 0.9453 - recall: 0.9453 - f1: 0.9453 - acc: 0.9453 - val_loss: 0.2923 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07508\n",
      "Epoch 74/100\n",
      " - 32s - loss: 0.1193 - precision: 0.9523 - recall: 0.9523 - f1: 0.9523 - acc: 0.9523 - val_loss: 0.9220 - val_precision: 0.7674 - val_recall: 0.7674 - val_f1: 0.7674 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07508\n",
      "Epoch 75/100\n",
      " - 32s - loss: 0.0800 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.2217 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07508\n",
      "Epoch 76/100\n",
      " - 32s - loss: 0.0990 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.1022 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07508\n",
      "Epoch 77/100\n",
      " - 32s - loss: 0.0836 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.0614 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.07508 to 0.06136, saving model to BREAKHIST_FOLD_1.h5\n",
      "Epoch 78/100\n",
      " - 32s - loss: 0.0801 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.0757 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.06136\n",
      "Epoch 79/100\n",
      " - 32s - loss: 0.0671 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.0924 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.06136\n",
      "Epoch 80/100\n",
      " - 32s - loss: 0.0986 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.0832 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.06136\n",
      "Epoch 81/100\n",
      " - 32s - loss: 0.0905 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.1774 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.06136\n",
      "Epoch 82/100\n",
      " - 32s - loss: 0.0790 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.1342 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.06136\n",
      "Epoch 83/100\n",
      " - 32s - loss: 0.0945 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.2585 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.06136\n",
      "Epoch 84/100\n",
      " - 32s - loss: 0.0914 - precision: 0.9653 - recall: 0.9653 - f1: 0.9653 - acc: 0.9653 - val_loss: 0.4945 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.06136\n",
      "Epoch 85/100\n",
      " - 32s - loss: 0.1278 - precision: 0.9497 - recall: 0.9497 - f1: 0.9497 - acc: 0.9497 - val_loss: 0.7866 - val_precision: 0.7847 - val_recall: 0.7847 - val_f1: 0.7847 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.06136\n",
      "Epoch 86/100\n",
      " - 32s - loss: 0.1278 - precision: 0.9514 - recall: 0.9514 - f1: 0.9514 - acc: 0.9514 - val_loss: 0.4233 - val_precision: 0.8090 - val_recall: 0.8090 - val_f1: 0.8090 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.06136\n",
      "Epoch 87/100\n",
      " - 32s - loss: 0.1206 - precision: 0.9523 - recall: 0.9523 - f1: 0.9523 - acc: 0.9523 - val_loss: 0.3714 - val_precision: 0.8576 - val_recall: 0.8576 - val_f1: 0.8576 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.06136\n",
      "Epoch 88/100\n",
      " - 32s - loss: 0.1416 - precision: 0.9514 - recall: 0.9514 - f1: 0.9514 - acc: 0.9514 - val_loss: 0.2025 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.06136\n",
      "Epoch 89/100\n",
      " - 32s - loss: 0.1022 - precision: 0.9609 - recall: 0.9609 - f1: 0.9609 - acc: 0.9609 - val_loss: 0.2089 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.06136\n",
      "Epoch 90/100\n",
      " - 32s - loss: 0.1000 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.1444 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.06136\n",
      "Epoch 91/100\n",
      " - 32s - loss: 0.0853 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.5051 - val_precision: 0.8472 - val_recall: 0.8472 - val_f1: 0.8472 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.06136\n",
      "Epoch 92/100\n",
      " - 32s - loss: 0.0824 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.3049 - val_precision: 0.8854 - val_recall: 0.8854 - val_f1: 0.8854 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.06136\n",
      "Epoch 93/100\n",
      " - 32s - loss: 0.0835 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.2148 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.06136\n",
      "Epoch 94/100\n",
      " - 32s - loss: 0.0638 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.0995 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.06136\n",
      "Epoch 95/100\n",
      " - 32s - loss: 0.0709 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.1184 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.06136\n",
      "Epoch 96/100\n",
      " - 32s - loss: 0.0706 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.1075 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00096: val_loss did not improve from 0.06136\n",
      "Epoch 97/100\n",
      " - 32s - loss: 0.0864 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.1004 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.06136\n",
      "Epoch 98/100\n",
      " - 32s - loss: 0.0628 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.1157 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.06136\n",
      "Epoch 99/100\n",
      " - 32s - loss: 0.0750 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.3271 - val_precision: 0.8785 - val_recall: 0.8785 - val_f1: 0.8785 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.06136\n",
      "Epoch 100/100\n",
      " - 32s - loss: 0.1202 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 0.4351 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.06136\n",
      "Epoch 1/100\n",
      " - 34s - loss: 0.1622 - precision: 0.9384 - recall: 0.9384 - f1: 0.9384 - acc: 0.9384 - val_loss: 0.3587 - val_precision: 0.8507 - val_recall: 0.8507 - val_f1: 0.8507 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35869, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 2/100\n",
      " - 31s - loss: 0.1457 - precision: 0.9375 - recall: 0.9375 - f1: 0.9375 - acc: 0.9375 - val_loss: 0.4891 - val_precision: 0.8368 - val_recall: 0.8368 - val_f1: 0.8368 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35869\n",
      "Epoch 3/100\n",
      " - 32s - loss: 0.1391 - precision: 0.9488 - recall: 0.9488 - f1: 0.9488 - acc: 0.9488 - val_loss: 0.1066 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35869 to 0.10660, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 4/100\n",
      " - 32s - loss: 0.1207 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.1303 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10660\n",
      "Epoch 5/100\n",
      " - 32s - loss: 0.1281 - precision: 0.9453 - recall: 0.9453 - f1: 0.9453 - acc: 0.9453 - val_loss: 0.1304 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10660\n",
      "Epoch 6/100\n",
      " - 32s - loss: 0.1289 - precision: 0.9462 - recall: 0.9462 - f1: 0.9462 - acc: 0.9462 - val_loss: 0.2123 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10660\n",
      "Epoch 7/100\n",
      " - 32s - loss: 0.1631 - precision: 0.9288 - recall: 0.9288 - f1: 0.9288 - acc: 0.9288 - val_loss: 0.2313 - val_precision: 0.8785 - val_recall: 0.8785 - val_f1: 0.8785 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10660\n",
      "Epoch 8/100\n",
      " - 32s - loss: 0.1051 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 0.4366 - val_precision: 0.8819 - val_recall: 0.8819 - val_f1: 0.8819 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10660\n",
      "Epoch 9/100\n",
      " - 32s - loss: 0.1528 - precision: 0.9410 - recall: 0.9410 - f1: 0.9410 - acc: 0.9410 - val_loss: 0.1988 - val_precision: 0.8993 - val_recall: 0.8993 - val_f1: 0.8993 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10660\n",
      "Epoch 10/100\n",
      " - 32s - loss: 0.1289 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 0.1818 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10660\n",
      "Epoch 11/100\n",
      " - 32s - loss: 0.1572 - precision: 0.9262 - recall: 0.9262 - f1: 0.9262 - acc: 0.9262 - val_loss: 1.9240 - val_precision: 0.7049 - val_recall: 0.7049 - val_f1: 0.7049 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.10660\n",
      "Epoch 12/100\n",
      " - 32s - loss: 0.1359 - precision: 0.9444 - recall: 0.9444 - f1: 0.9444 - acc: 0.9444 - val_loss: 0.2396 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.10660\n",
      "Epoch 13/100\n",
      " - 32s - loss: 0.1089 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.0939 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.10660 to 0.09394, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 14/100\n",
      " - 32s - loss: 0.1258 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 0.3019 - val_precision: 0.8472 - val_recall: 0.8472 - val_f1: 0.8472 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09394\n",
      "Epoch 15/100\n",
      " - 32s - loss: 0.1472 - precision: 0.9427 - recall: 0.9427 - f1: 0.9427 - acc: 0.9427 - val_loss: 0.7186 - val_precision: 0.7118 - val_recall: 0.7118 - val_f1: 0.7118 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09394\n",
      "Epoch 16/100\n",
      " - 33s - loss: 0.1351 - precision: 0.9514 - recall: 0.9514 - f1: 0.9514 - acc: 0.9514 - val_loss: 0.4728 - val_precision: 0.8403 - val_recall: 0.8403 - val_f1: 0.8403 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09394\n",
      "Epoch 17/100\n",
      " - 32s - loss: 0.1141 - precision: 0.9540 - recall: 0.9540 - f1: 0.9540 - acc: 0.9540 - val_loss: 0.2026 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09394\n",
      "Epoch 18/100\n",
      " - 32s - loss: 0.1194 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 1.4079 - val_precision: 0.6736 - val_recall: 0.6736 - val_f1: 0.6736 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09394\n",
      "Epoch 19/100\n",
      " - 32s - loss: 0.1455 - precision: 0.9392 - recall: 0.9392 - f1: 0.9392 - acc: 0.9392 - val_loss: 1.4692 - val_precision: 0.5451 - val_recall: 0.5451 - val_f1: 0.5451 - val_acc: 0.5451\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09394\n",
      "Epoch 20/100\n",
      " - 33s - loss: 0.1108 - precision: 0.9549 - recall: 0.9549 - f1: 0.9549 - acc: 0.9549 - val_loss: 0.2018 - val_precision: 0.8819 - val_recall: 0.8819 - val_f1: 0.8819 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09394\n",
      "Epoch 21/100\n",
      " - 32s - loss: 0.1178 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.0916 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09394 to 0.09158, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 22/100\n",
      " - 32s - loss: 0.1283 - precision: 0.9479 - recall: 0.9479 - f1: 0.9479 - acc: 0.9479 - val_loss: 1.3930 - val_precision: 0.6910 - val_recall: 0.6910 - val_f1: 0.6910 - val_acc: 0.6910\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09158\n",
      "Epoch 23/100\n",
      " - 32s - loss: 0.1030 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 1.4158 - val_precision: 0.7326 - val_recall: 0.7326 - val_f1: 0.7326 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09158\n",
      "Epoch 24/100\n",
      " - 33s - loss: 0.1525 - precision: 0.9384 - recall: 0.9384 - f1: 0.9384 - acc: 0.9384 - val_loss: 0.4941 - val_precision: 0.8229 - val_recall: 0.8229 - val_f1: 0.8229 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09158\n",
      "Epoch 25/100\n",
      " - 32s - loss: 0.1110 - precision: 0.9540 - recall: 0.9540 - f1: 0.9540 - acc: 0.9540 - val_loss: 0.1936 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09158\n",
      "Epoch 26/100\n",
      " - 32s - loss: 0.0939 - precision: 0.9653 - recall: 0.9653 - f1: 0.9653 - acc: 0.9653 - val_loss: 0.3026 - val_precision: 0.9028 - val_recall: 0.9028 - val_f1: 0.9028 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09158\n",
      "Epoch 27/100\n",
      " - 32s - loss: 0.0923 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 0.5341 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09158\n",
      "Epoch 28/100\n",
      " - 32s - loss: 0.1163 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.6396 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09158\n",
      "Epoch 29/100\n",
      " - 32s - loss: 0.0901 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.1828 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09158\n",
      "Epoch 30/100\n",
      " - 32s - loss: 0.0777 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.0902 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09158 to 0.09021, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 31/100\n",
      " - 32s - loss: 0.0950 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.5131 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09021\n",
      "Epoch 32/100\n",
      " - 32s - loss: 0.0914 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 0.2458 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09021\n",
      "Epoch 33/100\n",
      " - 32s - loss: 0.0888 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.0943 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09021\n",
      "Epoch 34/100\n",
      " - 32s - loss: 0.0869 - precision: 0.9687 - recall: 0.9687 - f1: 0.9687 - acc: 0.9687 - val_loss: 0.0794 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09021 to 0.07943, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 35/100\n",
      " - 32s - loss: 0.0669 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.0558 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.07943 to 0.05578, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 36/100\n",
      " - 32s - loss: 0.0661 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0816 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05578\n",
      "Epoch 37/100\n",
      " - 32s - loss: 0.0780 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.7660 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05578\n",
      "Epoch 38/100\n",
      " - 32s - loss: 0.1158 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 1.8962 - val_precision: 0.5174 - val_recall: 0.5174 - val_f1: 0.5174 - val_acc: 0.5174\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05578\n",
      "Epoch 39/100\n",
      " - 32s - loss: 0.1258 - precision: 0.9497 - recall: 0.9497 - f1: 0.9497 - acc: 0.9497 - val_loss: 0.2481 - val_precision: 0.8854 - val_recall: 0.8854 - val_f1: 0.8854 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.05578\n",
      "Epoch 40/100\n",
      " - 32s - loss: 0.1098 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 1.2460 - val_precision: 0.7604 - val_recall: 0.7604 - val_f1: 0.7604 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05578\n",
      "Epoch 41/100\n",
      " - 32s - loss: 0.0708 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.3134 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.05578\n",
      "Epoch 42/100\n",
      " - 32s - loss: 0.0822 - precision: 0.9687 - recall: 0.9687 - f1: 0.9687 - acc: 0.9687 - val_loss: 0.1568 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.05578\n",
      "Epoch 43/100\n",
      " - 32s - loss: 0.0764 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.1277 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.05578\n",
      "Epoch 44/100\n",
      " - 32s - loss: 0.0548 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0685 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.05578\n",
      "Epoch 45/100\n",
      " - 32s - loss: 0.0837 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.6521 - val_precision: 0.8194 - val_recall: 0.8194 - val_f1: 0.8194 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.05578\n",
      "Epoch 46/100\n",
      " - 32s - loss: 0.1033 - precision: 0.9549 - recall: 0.9549 - f1: 0.9549 - acc: 0.9549 - val_loss: 0.5343 - val_precision: 0.8438 - val_recall: 0.8438 - val_f1: 0.8437 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.05578\n",
      "Epoch 47/100\n",
      " - 32s - loss: 0.0907 - precision: 0.9687 - recall: 0.9687 - f1: 0.9687 - acc: 0.9687 - val_loss: 0.1209 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.05578\n",
      "Epoch 48/100\n",
      " - 32s - loss: 0.1354 - precision: 0.9505 - recall: 0.9505 - f1: 0.9505 - acc: 0.9505 - val_loss: 0.4351 - val_precision: 0.8090 - val_recall: 0.8090 - val_f1: 0.8090 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.05578\n",
      "Epoch 49/100\n",
      " - 32s - loss: 0.1017 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.3961 - val_precision: 0.8576 - val_recall: 0.8576 - val_f1: 0.8576 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.05578\n",
      "Epoch 50/100\n",
      " - 32s - loss: 0.0830 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1297 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.05578\n",
      "Epoch 51/100\n",
      " - 32s - loss: 0.0831 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.0680 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.05578\n",
      "Epoch 52/100\n",
      " - 32s - loss: 0.0610 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.1107 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.05578\n",
      "Epoch 53/100\n",
      " - 32s - loss: 0.0748 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.4081 - val_precision: 0.8403 - val_recall: 0.8403 - val_f1: 0.8403 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.05578\n",
      "Epoch 54/100\n",
      " - 32s - loss: 0.1178 - precision: 0.9540 - recall: 0.9540 - f1: 0.9540 - acc: 0.9540 - val_loss: 0.2971 - val_precision: 0.8611 - val_recall: 0.8611 - val_f1: 0.8611 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.05578\n",
      "Epoch 55/100\n",
      " - 32s - loss: 0.0922 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 1.1741 - val_precision: 0.6528 - val_recall: 0.6528 - val_f1: 0.6528 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.05578\n",
      "Epoch 56/100\n",
      " - 32s - loss: 0.0946 - precision: 0.9661 - recall: 0.9661 - f1: 0.9661 - acc: 0.9661 - val_loss: 0.4645 - val_precision: 0.8715 - val_recall: 0.8715 - val_f1: 0.8715 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.05578\n",
      "Epoch 57/100\n",
      " - 32s - loss: 0.0946 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.3772 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.05578\n",
      "Epoch 58/100\n",
      " - 32s - loss: 0.0783 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1441 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.05578\n",
      "Epoch 59/100\n",
      " - 32s - loss: 0.0666 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.0891 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.05578\n",
      "Epoch 60/100\n",
      " - 32s - loss: 0.0713 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.1202 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.05578\n",
      "Epoch 61/100\n",
      " - 32s - loss: 0.0500 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.1005 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00061: val_loss did not improve from 0.05578\n",
      "Epoch 62/100\n",
      " - 32s - loss: 0.0786 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.5983 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.05578\n",
      "Epoch 63/100\n",
      " - 32s - loss: 0.0780 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 1.2695 - val_precision: 0.6007 - val_recall: 0.6007 - val_f1: 0.6007 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.05578\n",
      "Epoch 64/100\n",
      " - 32s - loss: 0.1332 - precision: 0.9549 - recall: 0.9549 - f1: 0.9549 - acc: 0.9549 - val_loss: 0.9697 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.05578\n",
      "Epoch 65/100\n",
      " - 32s - loss: 0.1085 - precision: 0.9540 - recall: 0.9540 - f1: 0.9540 - acc: 0.9540 - val_loss: 0.5745 - val_precision: 0.7465 - val_recall: 0.7465 - val_f1: 0.7465 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05578\n",
      "Epoch 66/100\n",
      " - 32s - loss: 0.0931 - precision: 0.9627 - recall: 0.9627 - f1: 0.9627 - acc: 0.9627 - val_loss: 0.2542 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.05578\n",
      "Epoch 67/100\n",
      " - 32s - loss: 0.0761 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.1569 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05578\n",
      "Epoch 68/100\n",
      " - 32s - loss: 0.0668 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.0677 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05578\n",
      "Epoch 69/100\n",
      " - 32s - loss: 0.0699 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.1003 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05578\n",
      "Epoch 70/100\n",
      " - 32s - loss: 0.0568 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.0942 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05578\n",
      "Epoch 71/100\n",
      " - 32s - loss: 0.1155 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.7077 - val_precision: 0.8715 - val_recall: 0.8715 - val_f1: 0.8715 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05578\n",
      "Epoch 72/100\n",
      " - 32s - loss: 0.0922 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 0.4893 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05578\n",
      "Epoch 73/100\n",
      " - 32s - loss: 0.1165 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 0.9022 - val_precision: 0.7743 - val_recall: 0.7743 - val_f1: 0.7743 - val_acc: 0.7743\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.05578\n",
      "Epoch 74/100\n",
      " - 32s - loss: 0.0969 - precision: 0.9609 - recall: 0.9609 - f1: 0.9609 - acc: 0.9609 - val_loss: 0.2033 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.05578\n",
      "Epoch 75/100\n",
      " - 32s - loss: 0.0621 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.3554 - val_precision: 0.9028 - val_recall: 0.9028 - val_f1: 0.9028 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05578\n",
      "Epoch 76/100\n",
      " - 32s - loss: 0.0577 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.2329 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.05578\n",
      "Epoch 77/100\n",
      " - 32s - loss: 0.0730 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1236 - val_precision: 0.9444 - val_recall: 0.9444 - val_f1: 0.9444 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.05578\n",
      "Epoch 78/100\n",
      " - 32s - loss: 0.0698 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.0481 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.05578 to 0.04813, saving model to BREAKHIST_FOLD_2.h5\n",
      "Epoch 79/100\n",
      " - 32s - loss: 0.0860 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.0693 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.04813\n",
      "Epoch 80/100\n",
      " - 32s - loss: 0.0522 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.0916 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.04813\n",
      "Epoch 81/100\n",
      " - 32s - loss: 0.0759 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.0961 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.04813\n",
      "Epoch 82/100\n",
      " - 32s - loss: 0.0512 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.0909 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.04813\n",
      "Epoch 83/100\n",
      " - 32s - loss: 0.0506 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.2626 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.04813\n",
      "Epoch 84/100\n",
      " - 32s - loss: 0.1075 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.1909 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.04813\n",
      "Epoch 85/100\n",
      " - 32s - loss: 0.1048 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.2382 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.04813\n",
      "Epoch 86/100\n",
      " - 32s - loss: 0.0750 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.6499 - val_precision: 0.8056 - val_recall: 0.8056 - val_f1: 0.8056 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.04813\n",
      "Epoch 87/100\n",
      " - 32s - loss: 0.0941 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.8796 - val_precision: 0.7118 - val_recall: 0.7118 - val_f1: 0.7118 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.04813\n",
      "Epoch 88/100\n",
      " - 32s - loss: 0.0882 - precision: 0.9635 - recall: 0.9635 - f1: 0.9635 - acc: 0.9635 - val_loss: 0.4472 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.04813\n",
      "Epoch 89/100\n",
      " - 32s - loss: 0.0856 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.7575 - val_precision: 0.7118 - val_recall: 0.7118 - val_f1: 0.7118 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.04813\n",
      "Epoch 90/100\n",
      " - 32s - loss: 0.0782 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.1452 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.04813\n",
      "Epoch 91/100\n",
      " - 32s - loss: 0.0756 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.1055 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.04813\n",
      "Epoch 92/100\n",
      " - 32s - loss: 0.0759 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.1193 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.04813\n",
      "Epoch 93/100\n",
      " - 32s - loss: 0.0588 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.0754 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.04813\n",
      "Epoch 94/100\n",
      " - 32s - loss: 0.0484 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0989 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00094: val_loss did not improve from 0.04813\n",
      "Epoch 95/100\n",
      " - 32s - loss: 0.0385 - precision: 0.9931 - recall: 0.9931 - f1: 0.9931 - acc: 0.9931 - val_loss: 0.0490 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.04813\n",
      "Epoch 96/100\n",
      " - 32s - loss: 0.0379 - precision: 0.9870 - recall: 0.9870 - f1: 0.9870 - acc: 0.9870 - val_loss: 0.0677 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.04813\n",
      "Epoch 97/100\n",
      " - 32s - loss: 0.0611 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0961 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.04813\n",
      "Epoch 98/100\n",
      " - 32s - loss: 0.0495 - precision: 0.9852 - recall: 0.9852 - f1: 0.9852 - acc: 0.9852 - val_loss: 0.1132 - val_precision: 0.9444 - val_recall: 0.9444 - val_f1: 0.9444 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.04813\n",
      "Epoch 99/100\n",
      " - 32s - loss: 0.0749 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.1670 - val_precision: 0.9028 - val_recall: 0.9028 - val_f1: 0.9028 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.04813\n",
      "Epoch 100/100\n",
      " - 32s - loss: 0.0580 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.1730 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04813\n",
      "Epoch 1/100\n",
      " - 34s - loss: 0.1221 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.0753 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07532, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 2/100\n",
      " - 32s - loss: 0.1137 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.1597 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07532\n",
      "Epoch 3/100\n",
      " - 33s - loss: 0.1074 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.2212 - val_precision: 0.8993 - val_recall: 0.8993 - val_f1: 0.8993 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07532\n",
      "Epoch 4/100\n",
      " - 33s - loss: 0.1307 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.2584 - val_precision: 0.8681 - val_recall: 0.8681 - val_f1: 0.8681 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07532\n",
      "Epoch 5/100\n",
      " - 32s - loss: 0.1329 - precision: 0.9462 - recall: 0.9462 - f1: 0.9462 - acc: 0.9462 - val_loss: 0.1444 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07532\n",
      "Epoch 6/100\n",
      " - 33s - loss: 0.0939 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.5963 - val_precision: 0.7778 - val_recall: 0.7778 - val_f1: 0.7778 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07532\n",
      "Epoch 7/100\n",
      " - 32s - loss: 0.1241 - precision: 0.9531 - recall: 0.9531 - f1: 0.9531 - acc: 0.9531 - val_loss: 1.9963 - val_precision: 0.6007 - val_recall: 0.6007 - val_f1: 0.6007 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07532\n",
      "Epoch 8/100\n",
      " - 33s - loss: 0.0910 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.0715 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07532 to 0.07150, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 9/100\n",
      " - 33s - loss: 0.1149 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.1547 - val_precision: 0.9444 - val_recall: 0.9444 - val_f1: 0.9444 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07150\n",
      "Epoch 10/100\n",
      " - 33s - loss: 0.1005 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.1621 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07150\n",
      "Epoch 11/100\n",
      " - 32s - loss: 0.0987 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.2071 - val_precision: 0.9063 - val_recall: 0.9063 - val_f1: 0.9062 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07150\n",
      "Epoch 12/100\n",
      " - 33s - loss: 0.0815 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.0668 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07150 to 0.06679, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 13/100\n",
      " - 32s - loss: 0.0871 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.0389 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06679 to 0.03887, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 14/100\n",
      " - 32s - loss: 0.0900 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.8401 - val_precision: 0.7292 - val_recall: 0.7292 - val_f1: 0.7292 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03887\n",
      "Epoch 15/100\n",
      " - 32s - loss: 0.1250 - precision: 0.9566 - recall: 0.9566 - f1: 0.9566 - acc: 0.9566 - val_loss: 1.4967 - val_precision: 0.6979 - val_recall: 0.6979 - val_f1: 0.6979 - val_acc: 0.6979\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03887\n",
      "Epoch 16/100\n",
      " - 32s - loss: 0.0875 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.2796 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03887\n",
      "Epoch 17/100\n",
      " - 33s - loss: 0.0799 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.0837 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03887\n",
      "Epoch 18/100\n",
      " - 33s - loss: 0.0810 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1339 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03887\n",
      "Epoch 19/100\n",
      " - 32s - loss: 0.1261 - precision: 0.9505 - recall: 0.9505 - f1: 0.9505 - acc: 0.9505 - val_loss: 2.6350 - val_precision: 0.5278 - val_recall: 0.5278 - val_f1: 0.5278 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03887\n",
      "Epoch 20/100\n",
      " - 32s - loss: 0.1340 - precision: 0.9479 - recall: 0.9479 - f1: 0.9479 - acc: 0.9479 - val_loss: 0.1208 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03887\n",
      "Epoch 21/100\n",
      " - 32s - loss: 0.0748 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.0517 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03887\n",
      "Epoch 22/100\n",
      " - 33s - loss: 0.0808 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.2327 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03887\n",
      "Epoch 23/100\n",
      " - 33s - loss: 0.1251 - precision: 0.9514 - recall: 0.9514 - f1: 0.9514 - acc: 0.9514 - val_loss: 1.7258 - val_precision: 0.5243 - val_recall: 0.5243 - val_f1: 0.5243 - val_acc: 0.5243\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03887\n",
      "Epoch 24/100\n",
      " - 32s - loss: 0.1188 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.2894 - val_precision: 0.8646 - val_recall: 0.8646 - val_f1: 0.8646 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03887\n",
      "Epoch 25/100\n",
      " - 32s - loss: 0.0984 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 0.1449 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03887\n",
      "Epoch 26/100\n",
      " - 32s - loss: 0.0899 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.1615 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03887\n",
      "Epoch 27/100\n",
      " - 32s - loss: 0.1151 - precision: 0.9557 - recall: 0.9557 - f1: 0.9557 - acc: 0.9557 - val_loss: 0.3175 - val_precision: 0.8194 - val_recall: 0.8194 - val_f1: 0.8194 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03887\n",
      "Epoch 28/100\n",
      " - 32s - loss: 0.1029 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.0655 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03887\n",
      "Epoch 29/100\n",
      " - 33s - loss: 0.0637 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.0519 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03887\n",
      "Epoch 30/100\n",
      " - 33s - loss: 0.0689 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0508 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03887\n",
      "Epoch 31/100\n",
      " - 32s - loss: 0.0602 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.1044 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03887\n",
      "Epoch 32/100\n",
      " - 32s - loss: 0.0847 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.2319 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03887\n",
      "Epoch 33/100\n",
      " - 32s - loss: 0.0587 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0671 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03887\n",
      "Epoch 34/100\n",
      " - 32s - loss: 0.0704 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.0380 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.03887 to 0.03803, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 35/100\n",
      " - 32s - loss: 0.0575 - precision: 0.9766 - recall: 0.9766 - f1: 0.9766 - acc: 0.9766 - val_loss: 0.0654 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03803\n",
      "Epoch 36/100\n",
      " - 32s - loss: 0.0625 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.0341 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.03803 to 0.03405, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 37/100\n",
      " - 32s - loss: 0.0616 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.3619 - val_precision: 0.8576 - val_recall: 0.8576 - val_f1: 0.8576 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03405\n",
      "Epoch 38/100\n",
      " - 32s - loss: 0.0560 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.1311 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03405\n",
      "Epoch 39/100\n",
      " - 32s - loss: 0.1410 - precision: 0.9462 - recall: 0.9462 - f1: 0.9462 - acc: 0.9462 - val_loss: 0.6687 - val_precision: 0.7813 - val_recall: 0.7813 - val_f1: 0.7812 - val_acc: 0.7813\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03405\n",
      "Epoch 40/100\n",
      " - 32s - loss: 0.1015 - precision: 0.9627 - recall: 0.9627 - f1: 0.9627 - acc: 0.9627 - val_loss: 1.8624 - val_precision: 0.6181 - val_recall: 0.6181 - val_f1: 0.6181 - val_acc: 0.6181\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03405\n",
      "Epoch 41/100\n",
      " - 32s - loss: 0.0711 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.2424 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03405\n",
      "Epoch 42/100\n",
      " - 32s - loss: 0.0781 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.0860 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03405\n",
      "Epoch 43/100\n",
      " - 33s - loss: 0.0661 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0444 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03405\n",
      "Epoch 44/100\n",
      " - 32s - loss: 0.0701 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.1634 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03405\n",
      "Epoch 45/100\n",
      " - 32s - loss: 0.0479 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.5594 - val_precision: 0.8368 - val_recall: 0.8368 - val_f1: 0.8368 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03405\n",
      "Epoch 46/100\n",
      " - 33s - loss: 0.0689 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.7835 - val_precision: 0.8021 - val_recall: 0.8021 - val_f1: 0.8021 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03405\n",
      "Epoch 47/100\n",
      " - 32s - loss: 0.1352 - precision: 0.9497 - recall: 0.9497 - f1: 0.9497 - acc: 0.9497 - val_loss: 1.2899 - val_precision: 0.6389 - val_recall: 0.6389 - val_f1: 0.6389 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03405\n",
      "Epoch 48/100\n",
      " - 32s - loss: 0.1882 - precision: 0.9306 - recall: 0.9306 - f1: 0.9306 - acc: 0.9306 - val_loss: 0.0958 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03405\n",
      "Epoch 49/100\n",
      " - 32s - loss: 0.1053 - precision: 0.9635 - recall: 0.9635 - f1: 0.9635 - acc: 0.9635 - val_loss: 0.0887 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03405\n",
      "Epoch 50/100\n",
      " - 32s - loss: 0.0594 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0758 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03405\n",
      "Epoch 51/100\n",
      " - 32s - loss: 0.0617 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0630 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03405\n",
      "Epoch 52/100\n",
      " - 32s - loss: 0.0691 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.0259 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03405 to 0.02590, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 53/100\n",
      " - 32s - loss: 0.0911 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.0477 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02590\n",
      "Epoch 54/100\n",
      " - 32s - loss: 0.0699 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0902 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02590\n",
      "Epoch 55/100\n",
      " - 32s - loss: 0.1077 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 0.5969 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02590\n",
      "Epoch 56/100\n",
      " - 32s - loss: 0.0667 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.0916 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02590\n",
      "Epoch 57/100\n",
      " - 32s - loss: 0.0749 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.4430 - val_precision: 0.8472 - val_recall: 0.8472 - val_f1: 0.8472 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02590\n",
      "Epoch 58/100\n",
      " - 33s - loss: 0.0988 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.0833 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02590\n",
      "Epoch 59/100\n",
      " - 32s - loss: 0.0560 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0308 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02590\n",
      "Epoch 60/100\n",
      " - 32s - loss: 0.0720 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.0304 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02590\n",
      "Epoch 61/100\n",
      " - 32s - loss: 0.0520 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0722 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02590\n",
      "Epoch 62/100\n",
      " - 32s - loss: 0.0674 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.1182 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02590\n",
      "Epoch 63/100\n",
      " - 32s - loss: 0.1005 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.4365 - val_precision: 0.8611 - val_recall: 0.8611 - val_f1: 0.8611 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02590\n",
      "Epoch 64/100\n",
      " - 32s - loss: 0.1020 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 1.6960 - val_precision: 0.6979 - val_recall: 0.6979 - val_f1: 0.6979 - val_acc: 0.6979\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02590\n",
      "Epoch 65/100\n",
      " - 32s - loss: 0.0772 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 1.3964 - val_precision: 0.5313 - val_recall: 0.5313 - val_f1: 0.5312 - val_acc: 0.5313\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02590\n",
      "Epoch 66/100\n",
      " - 32s - loss: 0.0610 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.0344 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02590\n",
      "Epoch 67/100\n",
      " - 32s - loss: 0.0807 - precision: 0.9653 - recall: 0.9653 - f1: 0.9653 - acc: 0.9653 - val_loss: 0.0681 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02590\n",
      "Epoch 68/100\n",
      " - 32s - loss: 0.0507 - precision: 0.9861 - recall: 0.9861 - f1: 0.9861 - acc: 0.9861 - val_loss: 0.0240 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.02590 to 0.02401, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 69/100\n",
      " - 32s - loss: 0.0508 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.0690 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02401\n",
      "Epoch 70/100\n",
      " - 32s - loss: 0.0422 - precision: 0.9852 - recall: 0.9852 - f1: 0.9852 - acc: 0.9852 - val_loss: 0.0942 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02401\n",
      "Epoch 71/100\n",
      " - 32s - loss: 0.0733 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.2359 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02401\n",
      "Epoch 72/100\n",
      " - 32s - loss: 0.0833 - precision: 0.9653 - recall: 0.9653 - f1: 0.9653 - acc: 0.9653 - val_loss: 0.2222 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02401\n",
      "Epoch 73/100\n",
      " - 32s - loss: 0.1058 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.2750 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02401\n",
      "Epoch 74/100\n",
      " - 32s - loss: 0.0840 - precision: 0.9635 - recall: 0.9635 - f1: 0.9635 - acc: 0.9635 - val_loss: 0.0361 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02401\n",
      "Epoch 75/100\n",
      " - 32s - loss: 0.0914 - precision: 0.9661 - recall: 0.9661 - f1: 0.9661 - acc: 0.9661 - val_loss: 0.2116 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02401\n",
      "Epoch 76/100\n",
      " - 32s - loss: 0.0529 - precision: 0.9826 - recall: 0.9826 - f1: 0.9826 - acc: 0.9826 - val_loss: 0.0626 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02401\n",
      "Epoch 77/100\n",
      " - 32s - loss: 0.0551 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0342 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02401\n",
      "Epoch 78/100\n",
      " - 32s - loss: 0.0552 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.0292 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02401\n",
      "Epoch 79/100\n",
      " - 32s - loss: 0.0516 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0224 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.02401 to 0.02241, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 80/100\n",
      " - 32s - loss: 0.0536 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0236 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02241\n",
      "Epoch 81/100\n",
      " - 32s - loss: 0.0597 - precision: 0.9766 - recall: 0.9766 - f1: 0.9766 - acc: 0.9766 - val_loss: 0.0221 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.02241 to 0.02209, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 82/100\n",
      " - 32s - loss: 0.0593 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.1935 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02209\n",
      "Epoch 83/100\n",
      " - 32s - loss: 0.0334 - precision: 0.9887 - recall: 0.9887 - f1: 0.9887 - acc: 0.9887 - val_loss: 0.0427 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02209\n",
      "Epoch 84/100\n",
      " - 32s - loss: 0.0628 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.1538 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02209\n",
      "Epoch 85/100\n",
      " - 32s - loss: 0.0671 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.3786 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02209\n",
      "Epoch 86/100\n",
      " - 32s - loss: 0.0894 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.5909 - val_precision: 0.7951 - val_recall: 0.7951 - val_f1: 0.7951 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02209\n",
      "Epoch 87/100\n",
      " - 32s - loss: 0.1090 - precision: 0.9618 - recall: 0.9618 - f1: 0.9618 - acc: 0.9618 - val_loss: 4.5202 - val_precision: 0.6563 - val_recall: 0.6563 - val_f1: 0.6562 - val_acc: 0.6563\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02209\n",
      "Epoch 88/100\n",
      " - 32s - loss: 0.0809 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.2526 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02209\n",
      "Epoch 89/100\n",
      " - 32s - loss: 0.0839 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.1736 - val_precision: 0.9444 - val_recall: 0.9444 - val_f1: 0.9444 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02209\n",
      "Epoch 90/100\n",
      " - 32s - loss: 0.0819 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.1193 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02209\n",
      "Epoch 91/100\n",
      " - 32s - loss: 0.0767 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.0717 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02209\n",
      "Epoch 92/100\n",
      " - 32s - loss: 0.0684 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.1606 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02209\n",
      "Epoch 93/100\n",
      " - 32s - loss: 0.0577 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.1176 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02209\n",
      "Epoch 94/100\n",
      " - 32s - loss: 0.0539 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.1315 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02209\n",
      "Epoch 95/100\n",
      " - 32s - loss: 0.0346 - precision: 0.9931 - recall: 0.9931 - f1: 0.9931 - acc: 0.9931 - val_loss: 0.0564 - val_precision: 0.9722 - val_recall: 0.9722 - val_f1: 0.9722 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02209\n",
      "Epoch 96/100\n",
      " - 32s - loss: 0.0479 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.0228 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02209\n",
      "Epoch 97/100\n",
      " - 32s - loss: 0.0398 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0220 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02209 to 0.02196, saving model to BREAKHIST_FOLD_3.h5\n",
      "Epoch 98/100\n",
      " - 32s - loss: 0.0454 - precision: 0.9861 - recall: 0.9861 - f1: 0.9861 - acc: 0.9861 - val_loss: 0.0278 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02196\n",
      "Epoch 99/100\n",
      " - 32s - loss: 0.0375 - precision: 0.9913 - recall: 0.9913 - f1: 0.9913 - acc: 0.9913 - val_loss: 0.0553 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02196\n",
      "Epoch 100/100\n",
      " - 32s - loss: 0.0549 - precision: 0.9826 - recall: 0.9826 - f1: 0.9826 - acc: 0.9826 - val_loss: 0.1226 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02196\n",
      "Epoch 1/100\n",
      " - 35s - loss: 0.1049 - precision: 0.9653 - recall: 0.9653 - f1: 0.9653 - acc: 0.9653 - val_loss: 0.2133 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21326, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 2/100\n",
      " - 32s - loss: 0.0648 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.3102 - val_precision: 0.8611 - val_recall: 0.8611 - val_f1: 0.8611 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21326\n",
      "Epoch 3/100\n",
      " - 32s - loss: 0.0872 - precision: 0.9687 - recall: 0.9687 - f1: 0.9687 - acc: 0.9687 - val_loss: 0.4882 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21326\n",
      "Epoch 4/100\n",
      " - 33s - loss: 0.0957 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.9123 - val_precision: 0.6215 - val_recall: 0.6215 - val_f1: 0.6215 - val_acc: 0.6215\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21326\n",
      "Epoch 5/100\n",
      " - 32s - loss: 0.0862 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.2746 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21326\n",
      "Epoch 6/100\n",
      " - 32s - loss: 0.0799 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 1.2488 - val_precision: 0.7049 - val_recall: 0.7049 - val_f1: 0.7049 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21326\n",
      "Epoch 7/100\n",
      " - 32s - loss: 0.1076 - precision: 0.9592 - recall: 0.9592 - f1: 0.9592 - acc: 0.9592 - val_loss: 0.0961 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.21326 to 0.09615, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 8/100\n",
      " - 32s - loss: 0.0811 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.1266 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09615\n",
      "Epoch 9/100\n",
      " - 32s - loss: 0.1265 - precision: 0.9575 - recall: 0.9575 - f1: 0.9575 - acc: 0.9575 - val_loss: 0.3125 - val_precision: 0.8924 - val_recall: 0.8924 - val_f1: 0.8924 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09615\n",
      "Epoch 10/100\n",
      " - 32s - loss: 0.0747 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.0543 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09615 to 0.05430, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 11/100\n",
      " - 32s - loss: 0.0879 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.2841 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05430\n",
      "Epoch 12/100\n",
      " - 33s - loss: 0.0935 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.0379 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05430 to 0.03792, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 13/100\n",
      " - 32s - loss: 0.0672 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.0205 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03792 to 0.02051, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 14/100\n",
      " - 33s - loss: 0.0850 - precision: 0.9687 - recall: 0.9687 - f1: 0.9687 - acc: 0.9687 - val_loss: 0.1541 - val_precision: 0.9653 - val_recall: 0.9653 - val_f1: 0.9653 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02051\n",
      "Epoch 15/100\n",
      " - 33s - loss: 0.0929 - precision: 0.9653 - recall: 0.9653 - f1: 0.9653 - acc: 0.9653 - val_loss: 0.1999 - val_precision: 0.9201 - val_recall: 0.9201 - val_f1: 0.9201 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02051\n",
      "Epoch 16/100\n",
      " - 32s - loss: 0.0914 - precision: 0.9661 - recall: 0.9661 - f1: 0.9661 - acc: 0.9661 - val_loss: 0.2305 - val_precision: 0.9097 - val_recall: 0.9097 - val_f1: 0.9097 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02051\n",
      "Epoch 17/100\n",
      " - 32s - loss: 0.0656 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.0364 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02051\n",
      "Epoch 18/100\n",
      " - 32s - loss: 0.0811 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.2097 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02051\n",
      "Epoch 19/100\n",
      " - 32s - loss: 0.0987 - precision: 0.9627 - recall: 0.9627 - f1: 0.9627 - acc: 0.9627 - val_loss: 0.4405 - val_precision: 0.8264 - val_recall: 0.8264 - val_f1: 0.8264 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02051\n",
      "Epoch 20/100\n",
      " - 32s - loss: 0.0841 - precision: 0.9687 - recall: 0.9687 - f1: 0.9687 - acc: 0.9687 - val_loss: 0.0794 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02051\n",
      "Epoch 21/100\n",
      " - 32s - loss: 0.0645 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.0402 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02051\n",
      "Epoch 22/100\n",
      " - 32s - loss: 0.0666 - precision: 0.9809 - recall: 0.9809 - f1: 0.9809 - acc: 0.9809 - val_loss: 0.1621 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02051\n",
      "Epoch 23/100\n",
      " - 32s - loss: 0.0953 - precision: 0.9644 - recall: 0.9644 - f1: 0.9644 - acc: 0.9644 - val_loss: 0.5832 - val_precision: 0.7986 - val_recall: 0.7986 - val_f1: 0.7986 - val_acc: 0.7986\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02051\n",
      "Epoch 24/100\n",
      " - 32s - loss: 0.0771 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 0.2992 - val_precision: 0.8993 - val_recall: 0.8993 - val_f1: 0.8993 - val_acc: 0.8993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02051\n",
      "Epoch 25/100\n",
      " - 33s - loss: 0.0696 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.0991 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02051\n",
      "Epoch 26/100\n",
      " - 32s - loss: 0.0761 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 1.4651 - val_precision: 0.7778 - val_recall: 0.7778 - val_f1: 0.7778 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02051\n",
      "Epoch 27/100\n",
      " - 32s - loss: 0.0792 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.1358 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02051\n",
      "Epoch 28/100\n",
      " - 32s - loss: 0.0741 - precision: 0.9661 - recall: 0.9661 - f1: 0.9661 - acc: 0.9661 - val_loss: 0.1013 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02051\n",
      "Epoch 29/100\n",
      " - 32s - loss: 0.0602 - precision: 0.9774 - recall: 0.9774 - f1: 0.9774 - acc: 0.9774 - val_loss: 0.0742 - val_precision: 0.9687 - val_recall: 0.9687 - val_f1: 0.9687 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02051\n",
      "Epoch 30/100\n",
      " - 32s - loss: 0.0726 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.4059 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02051\n",
      "Epoch 31/100\n",
      " - 32s - loss: 0.0759 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.7797 - val_precision: 0.7847 - val_recall: 0.7847 - val_f1: 0.7847 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02051\n",
      "Epoch 32/100\n",
      " - 32s - loss: 0.0737 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1710 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02051\n",
      "Epoch 33/100\n",
      " - 32s - loss: 0.0643 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.1530 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02051\n",
      "Epoch 34/100\n",
      " - 33s - loss: 0.0491 - precision: 0.9852 - recall: 0.9852 - f1: 0.9852 - acc: 0.9852 - val_loss: 0.0418 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02051\n",
      "Epoch 35/100\n",
      " - 32s - loss: 0.0491 - precision: 0.9818 - recall: 0.9818 - f1: 0.9818 - acc: 0.9818 - val_loss: 0.0322 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02051\n",
      "Epoch 36/100\n",
      " - 32s - loss: 0.0454 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0126 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.02051 to 0.01260, saving model to BREAKHIST_FOLD_4.h5\n",
      "Epoch 37/100\n",
      " - 32s - loss: 0.0514 - precision: 0.9870 - recall: 0.9870 - f1: 0.9870 - acc: 0.9870 - val_loss: 0.0905 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01260\n",
      "Epoch 38/100\n",
      " - 32s - loss: 0.0756 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.3209 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01260\n",
      "Epoch 39/100\n",
      " - 32s - loss: 0.0995 - precision: 0.9627 - recall: 0.9627 - f1: 0.9627 - acc: 0.9627 - val_loss: 0.4324 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01260\n",
      "Epoch 40/100\n",
      " - 32s - loss: 0.0806 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 1.0133 - val_precision: 0.7153 - val_recall: 0.7153 - val_f1: 0.7153 - val_acc: 0.7153\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01260\n",
      "Epoch 41/100\n",
      " - 32s - loss: 0.0660 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.3101 - val_precision: 0.8889 - val_recall: 0.8889 - val_f1: 0.8889 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01260\n",
      "Epoch 42/100\n",
      " - 32s - loss: 0.0620 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.1307 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01260\n",
      "Epoch 43/100\n",
      " - 32s - loss: 0.0427 - precision: 0.9861 - recall: 0.9861 - f1: 0.9861 - acc: 0.9861 - val_loss: 0.0715 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01260\n",
      "Epoch 44/100\n",
      " - 32s - loss: 0.0391 - precision: 0.9887 - recall: 0.9887 - f1: 0.9887 - acc: 0.9887 - val_loss: 0.0899 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01260\n",
      "Epoch 45/100\n",
      " - 32s - loss: 0.0456 - precision: 0.9852 - recall: 0.9852 - f1: 0.9852 - acc: 0.9852 - val_loss: 0.0637 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01260\n",
      "Epoch 46/100\n",
      " - 32s - loss: 0.0767 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.2882 - val_precision: 0.8785 - val_recall: 0.8785 - val_f1: 0.8785 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01260\n",
      "Epoch 47/100\n",
      " - 32s - loss: 0.0910 - precision: 0.9670 - recall: 0.9670 - f1: 0.9670 - acc: 0.9670 - val_loss: 0.7454 - val_precision: 0.8403 - val_recall: 0.8403 - val_f1: 0.8403 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01260\n",
      "Epoch 48/100\n",
      " - 32s - loss: 0.0898 - precision: 0.9679 - recall: 0.9679 - f1: 0.9679 - acc: 0.9679 - val_loss: 3.8864 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01260\n",
      "Epoch 49/100\n",
      " - 33s - loss: 0.0733 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.1210 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01260\n",
      "Epoch 50/100\n",
      " - 32s - loss: 0.0527 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.1335 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01260\n",
      "Epoch 51/100\n",
      " - 32s - loss: 0.0542 - precision: 0.9826 - recall: 0.9826 - f1: 0.9826 - acc: 0.9826 - val_loss: 0.0901 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01260\n",
      "Epoch 52/100\n",
      " - 33s - loss: 0.0369 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0268 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01260\n",
      "Epoch 53/100\n",
      " - 32s - loss: 0.0388 - precision: 0.9878 - recall: 0.9878 - f1: 0.9878 - acc: 0.9878 - val_loss: 0.1699 - val_precision: 0.9306 - val_recall: 0.9306 - val_f1: 0.9306 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01260\n",
      "Epoch 54/100\n",
      " - 32s - loss: 0.0751 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.1593 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01260\n",
      "Epoch 55/100\n",
      " - 32s - loss: 0.0984 - precision: 0.9635 - recall: 0.9635 - f1: 0.9635 - acc: 0.9635 - val_loss: 0.3578 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01260\n",
      "Epoch 56/100\n",
      " - 32s - loss: 0.1116 - precision: 0.9601 - recall: 0.9601 - f1: 0.9601 - acc: 0.9601 - val_loss: 0.1948 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01260\n",
      "Epoch 57/100\n",
      " - 32s - loss: 0.0787 - precision: 0.9722 - recall: 0.9722 - f1: 0.9722 - acc: 0.9722 - val_loss: 0.5396 - val_precision: 0.8507 - val_recall: 0.8507 - val_f1: 0.8507 - val_acc: 0.8507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01260\n",
      "Epoch 58/100\n",
      " - 32s - loss: 0.0729 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.1005 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01260\n",
      "Epoch 59/100\n",
      " - 32s - loss: 0.0382 - precision: 0.9887 - recall: 0.9887 - f1: 0.9887 - acc: 0.9887 - val_loss: 0.0455 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01260\n",
      "Epoch 60/100\n",
      " - 32s - loss: 0.0508 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.0276 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01260\n",
      "Epoch 61/100\n",
      " - 32s - loss: 0.0576 - precision: 0.9792 - recall: 0.9792 - f1: 0.9792 - acc: 0.9792 - val_loss: 0.0255 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01260\n",
      "Epoch 62/100\n",
      " - 32s - loss: 0.0489 - precision: 0.9826 - recall: 0.9826 - f1: 0.9826 - acc: 0.9826 - val_loss: 0.9943 - val_precision: 0.7708 - val_recall: 0.7708 - val_f1: 0.7708 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01260\n",
      "Epoch 63/100\n",
      " - 32s - loss: 0.0661 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.5117 - val_precision: 0.8056 - val_recall: 0.8056 - val_f1: 0.8056 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01260\n",
      "Epoch 64/100\n",
      " - 32s - loss: 0.0775 - precision: 0.9696 - recall: 0.9696 - f1: 0.9696 - acc: 0.9696 - val_loss: 0.6718 - val_precision: 0.7812 - val_recall: 0.7812 - val_f1: 0.7812 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01260\n",
      "Epoch 65/100\n",
      " - 32s - loss: 0.0600 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.1654 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01260\n",
      "Epoch 66/100\n",
      " - 32s - loss: 0.0610 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.2520 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01260\n",
      "Epoch 67/100\n",
      " - 32s - loss: 0.0446 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0477 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01260\n",
      "Epoch 68/100\n",
      " - 32s - loss: 0.0440 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0291 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01260\n",
      "Epoch 69/100\n",
      " - 32s - loss: 0.0454 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0370 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01260\n",
      "Epoch 70/100\n",
      " - 32s - loss: 0.0550 - precision: 0.9766 - recall: 0.9766 - f1: 0.9766 - acc: 0.9766 - val_loss: 0.5333 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01260\n",
      "Epoch 71/100\n",
      " - 32s - loss: 0.0676 - precision: 0.9740 - recall: 0.9740 - f1: 0.9740 - acc: 0.9740 - val_loss: 0.1052 - val_precision: 0.9514 - val_recall: 0.9514 - val_f1: 0.9514 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01260\n",
      "Epoch 72/100\n",
      " - 32s - loss: 0.0488 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0534 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01260\n",
      "Epoch 73/100\n",
      " - 32s - loss: 0.0740 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.1333 - val_precision: 0.9340 - val_recall: 0.9340 - val_f1: 0.9340 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01260\n",
      "Epoch 74/100\n",
      " - 33s - loss: 0.0431 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.5027 - val_precision: 0.8507 - val_recall: 0.8507 - val_f1: 0.8507 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01260\n",
      "Epoch 75/100\n",
      " - 32s - loss: 0.0481 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.1818 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01260\n",
      "Epoch 76/100\n",
      " - 32s - loss: 0.0424 - precision: 0.9852 - recall: 0.9852 - f1: 0.9852 - acc: 0.9852 - val_loss: 0.0340 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01260\n",
      "Epoch 77/100\n",
      " - 32s - loss: 0.0395 - precision: 0.9905 - recall: 0.9905 - f1: 0.9905 - acc: 0.9905 - val_loss: 0.0789 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01260\n",
      "Epoch 78/100\n",
      " - 32s - loss: 0.0362 - precision: 0.9887 - recall: 0.9887 - f1: 0.9887 - acc: 0.9887 - val_loss: 0.0476 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01260\n",
      "Epoch 79/100\n",
      " - 32s - loss: 0.0390 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0382 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01260\n",
      "Epoch 80/100\n",
      " - 33s - loss: 0.0405 - precision: 0.9870 - recall: 0.9870 - f1: 0.9870 - acc: 0.9870 - val_loss: 0.0463 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01260\n",
      "Epoch 81/100\n",
      " - 32s - loss: 0.0290 - precision: 0.9913 - recall: 0.9913 - f1: 0.9913 - acc: 0.9913 - val_loss: 0.0408 - val_precision: 0.9931 - val_recall: 0.9931 - val_f1: 0.9931 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01260\n",
      "Epoch 82/100\n",
      " - 32s - loss: 0.0302 - precision: 0.9878 - recall: 0.9878 - f1: 0.9878 - acc: 0.9878 - val_loss: 0.1201 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01260\n",
      "Epoch 83/100\n",
      " - 32s - loss: 0.0598 - precision: 0.9766 - recall: 0.9766 - f1: 0.9766 - acc: 0.9766 - val_loss: 0.2078 - val_precision: 0.9410 - val_recall: 0.9410 - val_f1: 0.9410 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01260\n",
      "Epoch 84/100\n",
      " - 32s - loss: 0.0706 - precision: 0.9757 - recall: 0.9757 - f1: 0.9757 - acc: 0.9757 - val_loss: 0.0446 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01260\n",
      "Epoch 85/100\n",
      " - 32s - loss: 0.0619 - precision: 0.9800 - recall: 0.9800 - f1: 0.9800 - acc: 0.9800 - val_loss: 0.1491 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01260\n",
      "Epoch 86/100\n",
      " - 32s - loss: 0.0814 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.2409 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01260\n",
      "Epoch 87/100\n",
      " - 32s - loss: 0.0557 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0438 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01260\n",
      "Epoch 88/100\n",
      " - 32s - loss: 0.0750 - precision: 0.9705 - recall: 0.9705 - f1: 0.9705 - acc: 0.9705 - val_loss: 0.3044 - val_precision: 0.9132 - val_recall: 0.9132 - val_f1: 0.9132 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01260\n",
      "Epoch 89/100\n",
      " - 32s - loss: 0.0669 - precision: 0.9783 - recall: 0.9783 - f1: 0.9783 - acc: 0.9783 - val_loss: 0.7278 - val_precision: 0.7882 - val_recall: 0.7882 - val_f1: 0.7882 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01260\n",
      "Epoch 90/100\n",
      " - 32s - loss: 0.0548 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.1585 - val_precision: 0.9549 - val_recall: 0.9549 - val_f1: 0.9549 - val_acc: 0.9549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01260\n",
      "Epoch 91/100\n",
      " - 32s - loss: 0.0640 - precision: 0.9714 - recall: 0.9714 - f1: 0.9714 - acc: 0.9714 - val_loss: 0.2422 - val_precision: 0.9236 - val_recall: 0.9236 - val_f1: 0.9236 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01260\n",
      "Epoch 92/100\n",
      " - 32s - loss: 0.0739 - precision: 0.9731 - recall: 0.9731 - f1: 0.9731 - acc: 0.9731 - val_loss: 0.1419 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01260\n",
      "Epoch 93/100\n",
      " - 32s - loss: 0.0449 - precision: 0.9835 - recall: 0.9835 - f1: 0.9835 - acc: 0.9835 - val_loss: 0.0470 - val_precision: 0.9826 - val_recall: 0.9826 - val_f1: 0.9826 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01260\n",
      "Epoch 94/100\n",
      " - 32s - loss: 0.0303 - precision: 0.9922 - recall: 0.9922 - f1: 0.9922 - acc: 0.9922 - val_loss: 0.0869 - val_precision: 0.9757 - val_recall: 0.9757 - val_f1: 0.9757 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01260\n",
      "Epoch 95/100\n",
      " - 32s - loss: 0.0400 - precision: 0.9878 - recall: 0.9878 - f1: 0.9878 - acc: 0.9878 - val_loss: 0.0840 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01260\n",
      "Epoch 96/100\n",
      " - 32s - loss: 0.0346 - precision: 0.9905 - recall: 0.9905 - f1: 0.9905 - acc: 0.9905 - val_loss: 0.0240 - val_precision: 0.9861 - val_recall: 0.9861 - val_f1: 0.9861 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01260\n",
      "Epoch 97/100\n",
      " - 32s - loss: 0.0399 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.0227 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01260\n",
      "Epoch 98/100\n",
      " - 32s - loss: 0.0326 - precision: 0.9878 - recall: 0.9878 - f1: 0.9878 - acc: 0.9878 - val_loss: 0.0310 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01260\n",
      "Epoch 99/100\n",
      " - 32s - loss: 0.0355 - precision: 0.9870 - recall: 0.9870 - f1: 0.9870 - acc: 0.9870 - val_loss: 0.0207 - val_precision: 0.9965 - val_recall: 0.9965 - val_f1: 0.9965 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01260\n",
      "Epoch 100/100\n",
      " - 32s - loss: 0.0331 - precision: 0.9852 - recall: 0.9852 - f1: 0.9852 - acc: 0.9852 - val_loss: 0.1768 - val_precision: 0.9618 - val_recall: 0.9618 - val_f1: 0.9618 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01260\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset.split_train_test(\"train\")[0])))):\n",
    "                                               \n",
    "    tg = BREAKHIST_DATASET(SHAPE, BATCH_SIZE, train_index, BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=True)\n",
    "    vg = BREAKHIST_DATASET(SHAPE, BATCH_SIZE, test_index , BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=False)\n",
    "        \n",
    "    schedule = SGDRScheduler(min_lr=1e-6,\n",
    "                             max_lr=1e-3,\n",
    "                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n",
    "                             lr_decay=0.9,\n",
    "                             cycle_length=10,\n",
    "                             mult_factor=2.)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[precision, recall, f1, 'acc'])\n",
    "\n",
    "    model_ckpt = \"BREAKHIST_FOLD_\"+str(ix)+\".h5\"\n",
    "    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n",
    "                 TensorBoard(log_dir='./log_'+str(ix), update_freq='batch'), \n",
    "                 schedule] \n",
    "                                               \n",
    "    model.fit_generator(tg.data_generator(),\n",
    "                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        validation_data=vg.data_generator(),\n",
    "                        validation_steps=len(test_index)//BATCH_SIZE,\n",
    "                        callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    gen = BREAKHIST_DATASET(SHAPE, BATCH_SIZE, range(1), BASE_DIR, DATASET_MODE, SEED, TRAIN_TEST_RATIO, augment=False).split_train_test(\"test\")\n",
    "                       \n",
    "    x = np.empty((len(gen[0]),)+SHAPE, dtype=np.float32)\n",
    "    y = np.empty((len(gen[1]), 2), dtype=np.float32)\n",
    "    \n",
    "    for ix, path in tqdm(enumerate(gen[0])):\n",
    "        img = np.array(Image.open(gen[0][ix]))\n",
    "        img = resize(img, SHAPE)\n",
    "\n",
    "        label = gen[1][ix]\n",
    "\n",
    "        x[ix] = img\n",
    "        y[ix] = label\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364it [00:16, 22.59it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions with THRESH_VAL\n",
    "def threshold_arr(array):\n",
    "    # Get all value from array\n",
    "    # Compare calue with THRESH_VAL \n",
    "    # IF value >= THRESH_VAL. round to 1\n",
    "    # ELSE. round to 0\n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val)), dtype=np.float32))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13333674847752183, 0.9478021978021978, 0.9478021978021978, 0.9478021506424789, 0.9478021978021978]\n",
      "[0.11773310712241865, 0.945054945054945, 0.945054945054945, 0.9450548978952261, 0.945054945054945]\n",
      "[0.10523121173565204, 0.9560439566989521, 0.9560439566989521, 0.9560439232941512, 0.9560439566989521]\n",
      "[0.09525326975099332, 0.9587912094462049, 0.9587912094462049, 0.9587911603214977, 0.9587912094462049]\n",
      "[0.10304161820274133, 0.9532967039516994, 0.9532967039516994, 0.9532966495870234, 0.9532967039516994]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    model = load_model(\"BREAKHIST_FOLD_{}.h5\".format(i), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
    "    print(model.evaluate(x, y, verbose=0))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig(\"400X - confusion matrix - 4. FOLD.jpg\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9587912087912088, F1_Score: 0.9526596436467681, Precision: 0.9536985539488321, Recall: 0.9516419253261359\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94       117\n",
      "           1       0.97      0.97      0.97       247\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       364\n",
      "   macro avg       0.95      0.95      0.95       364\n",
      "weighted avg       0.96      0.96      0.96       364\n",
      " samples avg       0.96      0.96      0.96       364\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds = threshold_arr(models[3].predict(x, verbose=0))\n",
    "\n",
    "results = precision_recall_fscore_support(y, y_preds ,average='macro')\n",
    "acc = accuracy_score(y, y_preds)\n",
    "\n",
    "print(\"Accuracy: {}, F1_Score: {}, Precision: {}, Recall: {}\".format(acc, results[2], results[0], results[1]))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y, y_preds))\n",
    "print(\"\\n\")\n",
    "cnf_matrix = confusion_matrix(y.argmax(axis=1), y_preds.argmax(axis=1))\n",
    "\n",
    "plot_confusion_matrix(cm           = cnf_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['BENIGN', 'MALIGNANT'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n",
      "115.41342735290527 ms\n",
      "***\n",
      "[[0. 1.]]\n",
      "8.04591178894043 ms\n",
      "***\n",
      "[[0. 1.]]\n",
      "8.021116256713867 ms\n",
      "***\n",
      "[[0. 1.]]\n",
      "9.024858474731445 ms\n",
      "***\n",
      "[[0. 1.]]\n",
      "7.992982864379883 ms\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(5): \n",
    "    img = np.array(Image.open(BASE_DIR+\"benign/SOB/adenosis/SOB_B_A_14-22549AB/40X/SOB_B_A-14-22549AB-40-009.png\"))\n",
    "    x = resize(img, SHAPE)\n",
    "    x = x.reshape((1,) + x.shape) \n",
    "    start = time.time()\n",
    "    prediction = models[3].predict(x, batch_size=1)\n",
    "    finish = time.time()\n",
    "    print(threshold_arr(prediction))\n",
    "    print((finish-start)*1000,\"ms\")\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BREAK_HIST_GANGSTERS_v1.0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
